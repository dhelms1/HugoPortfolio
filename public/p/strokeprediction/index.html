<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" href="/images/favicon.ico">
    
    <link rel="stylesheet" href="/scss/global.min.a88601b4c09a5d50233a8fb74cd2fec02c95c7ae004f6e6ba7b11d524a384d92.css">
    
    <link rel="stylesheet" href="/css/prism.css" />
    <link href="https://fonts.googleapis.com/css?family=Merriweather&display=swap" rel="stylesheet">
    

 






	




<title>Stroke Prediction: Battle of the Learning Methods | Derek Helms</title>
<meta name="description" content="Original GitHub Repository
Alright, I&rsquo;ll admit it&hellip; It sounded a lot cooler in my head. &ldquo;Battle of the Learning Methods: Statistical vs Machine Learning&rdquo;. Man vs. Machine (well not really, but you get the point). A test of speed, strength, learning, and maybe a little bit of overfitting. Maybe it&rsquo;s just me that&rsquo;s excited about the project, but I finally get to implement what I learn in school against what I do in my free time.">
<meta property="og:title" content="Stroke Prediction: Battle of the Learning Methods | Derek Helms">
<meta property="og:site_name" content="Derek Helms">
<meta property="og:description" content="Original GitHub Repository
Alright, I&rsquo;ll admit it&hellip; It sounded a lot cooler in my head. &ldquo;Battle of the Learning Methods: Statistical vs Machine Learning&rdquo;. Man vs. Machine (well not really, but you get the point). A test of speed, strength, learning, and maybe a little bit of overfitting. Maybe it&rsquo;s just me that&rsquo;s excited about the project, but I finally get to implement what I learn in school against what I do in my free time.">
<meta property="og:url" content="https://derekhelms.netlify.app/p/strokeprediction/">
<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:image" content='https://derekhelms.netlify.app/images/strokeprediction_img/header.jpg'><meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Stroke Prediction: Battle of the Learning Methods | Derek Helms">

	<link rel="canonical" href="https://derekhelms.netlify.app/p/strokeprediction/">


	<meta name="twitter:description" content="Original GitHub Repository
Alright, I&rsquo;ll admit it&hellip; It sounded a lot cooler in my head. &ldquo;Battle of the Learning Methods: Statistical vs Machine Learning&rdquo;. Man vs. Machine (well not really, but you get the point). A test of speed, strength, learning, and maybe a little bit of overfitting. Maybe it&rsquo;s just me that&rsquo;s excited about the project, but I finally get to implement what I learn in school against what I do in my free time.">
<meta name="twitter:image" content="https://derekhelms.netlify.app/images/strokeprediction_img/header.jpg">
<meta property="article:published_time" content="2021-03-21T00:00:00&#43;00:00">
	<meta property="article:updated_time" content="2021-03-21T00:00:00&#43;00:00">



    </head>


<body class="line-numbers">

    
    <script src="/js/initColors.js"></script>

    <div class="layout-styled">

        <Section class="section">
  <div class="nav-container">
    <a class="logo-link" href="/">
      <svg 
xmlns="http://www.w3.org/2000/svg" 
width="28" 
height="28" 
fill="none" 
class="bi bi-house-door" 
viewBox="0 0 15 15">
  <path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293L8.354 1.146zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4H2.5z"
  fill="#73737D"/>
</svg>
      <span class="header-hidden">Navigate back to the homepage</span>
    </a>
    <div class="nav-controls">
      
      <button style="margin:35px;">
        
            <a href="https://github.com/dhelms1" target="_blank" rel="noopener noreferrer"><svg
class="social-icon-image"
width="28"
height="28"
viewBox="0 0 14 14"
fill="none"
xmlns="http://www.w3.org/2000/svg"
>
<path
  fillRule="evenodd"
  clipRule="evenodd"
  d="M7 0C3.1325 0 0 3.21173 0 7.17706C0 10.3529 2.00375 13.0353 4.78625 13.9863C5.13625 14.0491 5.2675 13.8338 5.2675 13.6454C5.2675 13.4749 5.25875 12.9097 5.25875 12.3087C3.5 12.6406 3.045 11.8691 2.905 11.4653C2.82625 11.259 2.485 10.622 2.1875 10.4516C1.9425 10.317 1.5925 9.98508 2.17875 9.97611C2.73 9.96714 3.12375 10.4964 3.255 10.7118C3.885 11.7973 4.89125 11.4923 5.29375 11.3039C5.355 10.8374 5.53875 10.5234 5.74 10.3439C4.1825 10.1645 2.555 9.54549 2.555 6.80026C2.555 6.01976 2.82625 5.37382 3.2725 4.87143C3.2025 4.692 2.9575 3.95635 3.3425 2.96951C3.3425 2.96951 3.92875 2.78111 5.2675 3.70516C5.8275 3.54367 6.4225 3.46293 7.0175 3.46293C7.6125 3.46293 8.2075 3.54367 8.7675 3.70516C10.1063 2.77214 10.6925 2.96951 10.6925 2.96951C11.0775 3.95635 10.8325 4.692 10.7625 4.87143C11.2087 5.37382 11.48 6.01079 11.48 6.80026C11.48 9.55446 9.84375 10.1645 8.28625 10.3439C8.54 10.5682 8.75875 10.9988 8.75875 11.6717C8.75875 12.6316 8.75 13.4032 8.75 13.6454C8.75 13.8338 8.88125 14.0581 9.23125 13.9863C11.9963 13.0353 14 10.3439 14 7.17706C14 3.21173 10.8675 0 7 0Z"
  fill="#73737D"
/>
</svg></a>
      </button>
      
      <button>
        
            <a href="https://www.linkedin.com/in/derek-helms" target="_blank" rel="noopener noreferrer"><svg
class="social-icon-image"
width="28"
height="28"
viewBox="0 0 14 14"
fill="none"
xmlns="http://www.w3.org/2000/svg"
{...props}
>
<path
  fillRule="evenodd"
  clipRule="evenodd"
  d="M3.59615 13.125H0.871552V4.36523H3.59615V13.125ZM2.24847 3.16406C1.81878 3.16406 1.44769 3.00781 1.13519 2.69531C0.822692 2.38281 0.666443 2.01171 0.666443 1.58203C0.666443 1.15234 0.822692 0.781248 1.13519 0.468749C1.44769 0.156249 1.81878 0 2.24847 0C2.67816 0 3.04925 0.156249 3.36175 0.468749C3.67425 0.781248 3.8305 1.15234 3.8305 1.58203C3.8305 2.01171 3.67425 2.38281 3.36175 2.69531C3.04925 3.00781 2.67816 3.16406 2.24847 3.16406ZM13.7915 13.125H11.0669V8.84765C11.0669 8.14452 11.0083 7.63671 10.8911 7.32421C10.6763 6.79687 10.2563 6.5332 9.63134 6.5332C9.00634 6.5332 8.56689 6.76757 8.31298 7.23632C8.11767 7.58788 8.02001 8.10546 8.02001 8.78905V13.125H5.32471V4.36523H7.93212V5.5664H7.96142C8.15673 5.17578 8.46923 4.85351 8.89892 4.59961C9.36767 4.28711 9.91454 4.13086 10.5395 4.13086C11.8091 4.13086 12.6977 4.53125 13.2055 5.33203C13.5962 5.97656 13.7915 6.97265 13.7915 8.3203V13.125Z"
  fill="#73737D"
/>
</svg></a>
      </button>
      
      <button id="themeColorButton" class="icon-wrapper"> 
        <div id="sunRays" class="sun-rays"></div>
        <div id="moonOrSun" class="moon-or-sun"></div>
        <div id="moonMask" class="moon-mask"></div>
      </button>
      
    </div>
</div>
</Section>


<script src="/js/toggleLogos.js"></script>


<script src="/js/toggleColors.js"></script>


<script src="/js/copyUrl.js"></script>

        

<section class="section narrow">

    <section id="articleHero" class="section narrow">
    <div class="article-hero">
        <header class="article-header">
            <h1 class="article-hero-heading">Stroke Prediction: Battle of the Learning Methods</h1>
            <div class="article-hero-subtitle">
                <div class="article-meta">
                    


    
            <a href="/authors/hugo-authors/" class="article-author-link">
                
                    <div class="article-author-avatar">
                        <img src="/images/avatar.png" />
                    </div>
                
                
                <strong>Derek Helms</strong>
                
                <span class="hide-on-mobile">,&nbsp;</span>
            </a>
    



<script src="/js/collapseAuthors.js"></script>
                    March 21, 2021
                    â€¢ 18 min read
                </div>
            </div>
        </header>
        
        <div class="article-hero-image" id="ArticleImage__Hero">
            <img src="/images/strokeprediction_img/header.jpg">
        </div>
        
    </div>
</section>


    <aside id="progressBar" class="aside-container">
    <div class="aside-align">
      <div>
        <div class="overlap-container">
        </div>
      </div>
    </div>

    <div class="progress-container" tabIndex={-1}>
        <div class="track-line" aria-hidden="true">
            <div id="progressIndicator" class="progress-line"></div>
        </div>
    </div>
</aside>


    <article  id="articleContent" class="post-content" style="position:relative;">
        <p><a href="https://github.com/dhelms1/stroke_prediction"><strong>Original GitHub Repository</strong></a></p>
<h1 id="alright-ill-admit-it">Alright, I&rsquo;ll admit it&hellip;</h1>
<p>It sounded a lot cooler in my head. &ldquo;Battle of the Learning Methods: Statistical vs Machine Learning&rdquo;. Man vs. Machine (well not really, but you get the point). A test of speed, strength, learning, and maybe a little bit of overfitting. Maybe it&rsquo;s just me that&rsquo;s excited about the project, but I finally get to implement what I learn in school against what I do in my free time. This term I took both Sampling Methods and Applied Statistics II, and this brought me to think if I could create a project that implemented the material from those classes and my knowledge of Machine Learning? This project was the answer.</p>
<p>Stroke is the third leading cause of death in the United States, with over 140,000 people dying annually. Each year approximately 795,000 people suffer from a stroke with nearly 75% of these occurring in people over the age of 65. Using medical record information (age, gender, BMI, smoking, diseases, etc.) we will create models that can accurately predict whether or not a patient has had a stroke. The goal for this project will be to explore the data and find any correlations between features and the response variable stroke that will allow us to engineer new features for the data. After doing this, we will make a comparison between Statistical Modeling and Ensemble Modeling to see which we are able to achieve better results with. <em>Note: models will be evaluated by an F-Beta and Recall score since avoiding a missed diagnosis is the main focus.</em></p>
<h1 id="data-exploration--feature-engineering">Data Exploration &amp; Feature Engineering</h1>
<p>The data originated from the <a href="https://www.kaggle.com/fedesoriano/stroke-prediction-dataset">Kaggle</a> repository for stroke prediction. There are 11 features that were recorded for 5110 observations. Personally I like to perform feature engineering while doing my EDA so that I can create these new features while the ideas are fresh in my mind. From importing the data, our initial data set has the following features:</p>
<p><img src="/images/strokeprediction_img/head.jpg" alt=""></p>
<p>However, the <em>id</em> feature is just a unique identifier, so it can be dropped from the data set.</p>
<h2 id="bmi-body-mass-index">BMI (Body Mass Index)</h2>
<p>From the initial exploration, I found that the only feature that had missing values was <em>bmi</em> (with 201 na&rsquo;s). Before filling the missing values, we have the following distribution for <em>bmi</em>:
<img src="/images/strokeprediction_img/bmi_init.jpg" alt=""></p>
<p>Two important notes here: the data seems to be somewhat normally distributed and we seem to have extreme values for bmi. To handle the missing values, I decided to fill them with the median value +/- random noise between [1,4]. Do I have a logical explanation for why I did this? Of course not, but filling with the median only increased a single value being present, where adding the random noise allowed for a more &ldquo;even&rdquo; distribution around the median. On top of this, I filter the data to only include values up to the 99th quantile (approximately bmi = 53) to filter out the outliers. Doing this we used the following code and resulted in the following graph:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#75715e"># Fill missing with values with median +/- random noises between [1,4]</span>
error_term <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>round(np<span style="color:#f92672">.</span>sqrt(np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">16</span>, size<span style="color:#f92672">=</span>data<span style="color:#f92672">.</span>bmi<span style="color:#f92672">.</span>isna()<span style="color:#f92672">.</span>sum())),<span style="color:#ae81ff">2</span>)
bmi_fill <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>bmi<span style="color:#f92672">.</span>median() <span style="color:#f92672">+</span> error_term
data<span style="color:#f92672">.</span>loc[data<span style="color:#f92672">.</span>bmi<span style="color:#f92672">.</span>isnull(), <span style="color:#e6db74">&#39;bmi&#39;</span>] <span style="color:#f92672">=</span> bmi_fill

<span style="color:#75715e"># Remove any values above the 99th quantile (approx BMI = 53)</span>
data <span style="color:#f92672">=</span> data[data<span style="color:#f92672">.</span>bmi <span style="color:#f92672">&lt;</span> np<span style="color:#f92672">.</span>quantile(data<span style="color:#f92672">.</span>bmi, <span style="color:#ae81ff">0.99</span>)]
</code></pre></div><p><img src="/images/strokeprediction_img/bmi_final.jpg" alt=""></p>
<p>Okay so a bit more normal right? Maybe a little right skew in there too? I&rsquo;m happy with where that&rsquo;s at. Now that we&rsquo;ve handle the <em>bmi</em> feature it&rsquo;s time to create our first new feature: weight classes based on patient bmi.</p>
<h3 id="weight-class-feature-engineering">Weight Class (Feature Engineering)</h3>
<p>I decided to create weight classes based on the <a href="https://www.nhlbi.nih.gov/health/educational/lose_wt/BMI/bmicalc.htm">National Heart, Lung, and Blood Institute BMI Scale</a> which gives us the following categories:</p>
<ul>
<li>Underweight = Less than 18.5</li>
<li>Normal weight = 18.5 - 24.9</li>
<li>Overweight = 25 - 29.9</li>
<li>Obesity = Greater than 30</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weight</span>(row): <span style="color:#75715e"># see function.py file for function definition</span>
    <span style="color:#66d9ef">if</span> row[<span style="color:#e6db74">&#39;bmi&#39;</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">30</span>:
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;obese&#39;</span>
    <span style="color:#66d9ef">elif</span> ((row[<span style="color:#e6db74">&#39;bmi&#39;</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">25</span>) <span style="color:#f92672">&amp;</span> (row[<span style="color:#e6db74">&#39;bmi&#39;</span>] <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">30</span>)):
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;over weight&#39;</span>
    <span style="color:#66d9ef">elif</span> ((row[<span style="color:#e6db74">&#39;bmi&#39;</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">18.5</span>) <span style="color:#f92672">&amp;</span> (row[<span style="color:#e6db74">&#39;bmi&#39;</span>] <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">25</span>)):
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;normal weight&#39;</span>
    <span style="color:#66d9ef">else</span>:
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;under weight&#39;</span>
    <span style="color:#66d9ef">return</span> val
</code></pre></div><p><img src="/images/strokeprediction_img/stroke_weight_class.jpg" alt=""></p>
<p>So it seems that the majority of the data falls into the <em>obese</em> category, followed by <em>over weight</em>, <em>normal weight</em>, and finally <em>under weight</em>. The majority of strokes seem to occur in for patients that fall under either obese or over weight, some for normal weight, and only a single patient that was under weight. If we look further into this (not shown in this article, refer to notebook) we get the following mean ages for each weight group: normal - 33.76, obese - 49.96, over weight - 49.39, and under weight - 10.91. It seems that older patients, who are also more susceptible to strokes, fall into the obese and over weight classes.</p>
<h2 id="age">Age</h2>
<p><img src="/images/strokeprediction_img/age_part.jpg" alt=""></p>
<p>Age does not seem to have a certain type of distribution for all observations (closest to a uniform distribution). When looking strictly at stroke observations, we can see the majority of the data is above 35 and has an extreme left skew. The distribution for no stroke also seems to not have a certain distribution (but could be classified at closely uniform).</p>
<h3 id="age-class-feature-engineering">Age Class (Feature Engineering)</h3>
<p>Using the Age Categories from the Canadian Statistics website, I created features for the life cycle groupings defined on their website:</p>
<ul>
<li>Children (0-14)</li>
<li>Youth (15-24)</li>
<li>Adults (25-64)</li>
<li>Seniors (65+)</li>
</ul>
<p>I also created a generic <em>age_class</em> feature to be used for graphing. Think of the above individual features as a one-hot-encoding of the age_class feature.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">data[<span style="color:#e6db74">&#39;child&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(data<span style="color:#f92672">.</span>age <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
data[<span style="color:#e6db74">&#39;youth&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(((data<span style="color:#f92672">.</span>age <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">15</span>) <span style="color:#f92672">&amp;</span> (data<span style="color:#f92672">.</span>age <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">25</span>)), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
data[<span style="color:#e6db74">&#39;adult&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(((data<span style="color:#f92672">.</span>age <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">25</span>) <span style="color:#f92672">&amp;</span> (data<span style="color:#f92672">.</span>age <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">65</span>)), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
data[<span style="color:#e6db74">&#39;senior&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(data<span style="color:#f92672">.</span>age <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">65</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">age</span>(row): <span style="color:#75715e"># see function.py file for function definition</span>
    <span style="color:#66d9ef">if</span> row[<span style="color:#e6db74">&#39;child&#39;</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;child&#39;</span>
    <span style="color:#66d9ef">elif</span> row[<span style="color:#e6db74">&#39;youth&#39;</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;youth&#39;</span>
    <span style="color:#66d9ef">elif</span> row[<span style="color:#e6db74">&#39;adult&#39;</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;adult&#39;</span>
    <span style="color:#66d9ef">else</span>:
        val <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;senior&#39;</span>
    <span style="color:#66d9ef">return</span> val

data[<span style="color:#e6db74">&#39;age_class&#39;</span>] <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>apply(age, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</code></pre></div><p><img src="/images/strokeprediction_img/stroke_age_class.jpg" alt=""></p>
<p>It seems that the majority of the people observed in the data were young adults (25-64). However, people categorized as seniors had the highest amount of strokes. It also looks like both children and youth have very low stroke rates, and looking into this further we can see only two female children (age 1 and 14) had strokes. An important note is that the two children who had strokes were both categorized as obese with a BMI of approximately 30 for both.</p>
<h2 id="remaining-features-exploration">Remaining Features Exploration</h2>
<p>The remaining features didn&rsquo;t seem to have too many correlated findings or values that allowed for any feature engineering, so they will be generalized into this section. This will just be a brief overview of a few features, so refer the <em>stroke_prediction.ipynb</em> section <em>Age</em> &amp; <em>Other Numerical Features</em> for a more in-depth exploration.</p>
<p><img src="/images/strokeprediction_img/glucose_part.jpg" alt=""></p>
<p>We can see that <em>avg_glucose_level</em> doesn&rsquo;t have a certain distribution (could be considered bimodel), and for both stroke/no stroke we have the same findings (with stroke having a slightly higher second peak).</p>
<p><img src="/images/strokeprediction_img/stroke_hyper.jpg" alt=""></p>
<p>Looking at the above graph, we don&rsquo;t seem to have any clear indicator of <em>Hypertension</em> being a key feature for stroke detection. However, if we look at the ratio of stroke to no stroke within the Hypertension classes, we can find that people with Hypertension are about 4x more likely to have a stroke.</p>
<p><img src="/images/strokeprediction_img/stroke_heart.jpg" alt=""></p>
<p>Again looking at the above graph, we don&rsquo;t seem to have any clear indicator of <em>Heart Disease</em> being a key feature for stroke detection. However, if we look at the ratio of stroke to no stroke within the Heart Disease classes, we can find that people with Heart Disease are about 5x more likely to have a stroke.</p>
<p><img src="/images/strokeprediction_img/stroke_gender.jpg" alt=""></p>
<p>We seem to have around 800 more females than males in our dataset, but the amount of strokes for each gender seem to be about equivalent. Similarly, their ratio in respect to their group total are about equivalent. There is no clear evidence that one gender is more susceptible to stroke then the other.</p>
<h2 id="final-data-processing">Final Data Processing</h2>
<p>Now that we have finished exploring the data, we will want to create a final data set that can be used in the modeling section. We will drop all unused features and one-hot-encode the remaining categorical features.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#75715e"># Drop features that wont be used</span>
data<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#39;work_type&#39;</span>, <span style="color:#e6db74">&#39;Residence_type&#39;</span>, <span style="color:#e6db74">&#39;age_class&#39;</span>, <span style="color:#e6db74">&#39;gender&#39;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)

<span style="color:#75715e"># Encode and create any new necessary features</span>
data[<span style="color:#e6db74">&#39;ever_married&#39;</span>] <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;ever_married&#39;</span>]<span style="color:#f92672">.</span>replace([<span style="color:#e6db74">&#39;No&#39;</span>, <span style="color:#e6db74">&#39;Yes&#39;</span>], [<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>])
data[<span style="color:#e6db74">&#39;age_over_45&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where((data<span style="color:#f92672">.</span>age <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">45</span>), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
data[<span style="color:#e6db74">&#39;over_weight&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where((data<span style="color:#f92672">.</span>bmi <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">25</span>), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
data[<span style="color:#e6db74">&#39;smokes&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where((data<span style="color:#f92672">.</span>smoking_status <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;smokes&#39;</span>), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
data[<span style="color:#e6db74">&#39;never_smoked&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(((data<span style="color:#f92672">.</span>smoking_status <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;never smoked&#39;</span>) <span style="color:#f92672">|</span> 
                                 (data<span style="color:#f92672">.</span>smoking_status <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;Unkown&#39;</span>)), <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>)
data<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#39;smoking_status&#39;</span>, <span style="color:#e6db74">&#39;child&#39;</span>, <span style="color:#e6db74">&#39;youth&#39;</span>, <span style="color:#e6db74">&#39;adult&#39;</span>, <span style="color:#e6db74">&#39;senior&#39;</span>, <span style="color:#e6db74">&#39;weight_class&#39;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><p><img src="/images/strokeprediction_img/feat_corr.jpg" alt=""></p>
<p>Looking at the above matrix it seems that stroke has the highest correlation with the following features: age (0.25), age_over_45 (0.21), heart_disease (0.14), hypertension (0.13), and avg_glucose_level (0.13). There are other features that are correlated with stroke that we engineered, but are lower than the ones listed previously. Now that our data is processed and a subset of the features are kept, we can begin the modeling section.</p>
<h1 id="modeling">Modeling</h1>
<h3 id="brief-overview-of-data-formatting">Brief Overview of Data Formatting</h3>
<p>For the <strong>Statistical Modeling</strong> section, the data was reformatted in two ways to accommodate the large class imbalance (around 20x more observations of &ldquo;No Stroke&rdquo; compared to &ldquo;Stroke&rdquo;):</p>
<ul>
<li>
<p>Training data was balanced by using SMOTE (Synthetic Minority Oversampling Technique) to increased of minority &ldquo;stroke&rdquo; class to a 3:4 ratio with the majority class &ldquo;no stroke&rdquo;. This resulted in around 3400 majority observations (0) and 2500 minority observations (1).</p>
</li>
<li>
<p>Testing data was balanced using the NearMiss algorithm, which undersampled the majority class to a 4:3 ratio with the minority class. This resulted in around 120 majority and 90 minority observations to be used for evaluation. <em>Note: when evaluating based on oversampled data, I did not feel the results were as accurate since repeated observations were increasing the scores. I want the model to be prepared for real world data rather than higher metrics on repeated data.</em></p>
</li>
</ul>
<p>For the <strong>Ensemble Modeling</strong> section, the data was reformatted in the following ways to accommodate the class imbalance:</p>
<ul>
<li>
<p>Training data was left untouched since the ensemble algorithms we used are able to handle the imbalance within the model itself.</p>
</li>
<li>
<p>Testing data was resampled so that we would have a &ldquo;Stroke&rdquo; to &ldquo;No Stroke&rdquo; ratio of 2:3, resulting in around 50 minority and 75 majority observations (slightly smaller than the statistical modeling data).</p>
</li>
<li>
<p>An important note is that the extra observations from the majority class (after being undersampled) in the testing data were added back into the training data so that we had more data to train on. This was due to the algorithms being able to handle class imbalance (so more majority observations would not have a negative effect).</p>
</li>
</ul>
<h2 id="statistical-modeling">Statistical Modeling</h2>
<p>Using the <a href="https://www.statsmodels.org/v0.10.1/">StatsModels</a> module, I created a Generalized Linear Model based on the Logistic (Binomial) family. Before the actual model can be built, we first need to set up the data splits and sampling methods described above. The following code took care of this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)

over_sampler <span style="color:#f92672">=</span> over_sampling<span style="color:#f92672">.</span>SMOTE(sampling_strategy<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
under_sampler <span style="color:#f92672">=</span> under_sampling<span style="color:#f92672">.</span>NearMiss(sampling_strategy<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>, version<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
X_train, y_train <span style="color:#f92672">=</span> over_sampler<span style="color:#f92672">.</span>fit_resample(X_train, y_train)
X_test, y_test <span style="color:#f92672">=</span> under_sampler<span style="color:#f92672">.</span>fit_resample(X_test, y_test)

<span style="color:#75715e"># Create data frame for GLM model (X_train &amp; y_train in same data frame)</span>
data_glm <span style="color:#f92672">=</span> X_train<span style="color:#f92672">.</span>copy()
data_glm[<span style="color:#e6db74">&#39;stroke&#39;</span>] <span style="color:#f92672">=</span> y_train
</code></pre></div><p><img src="/images/strokeprediction_img/GLM_data.jpg" alt=""></p>
<p>As we can see, by oversampling we were able to bring up the minority class observations to approximately 2500, which gave us our 4:3 desired ratio (again, no real reason why I chose this but let&rsquo;s just go with it). Testing data has also been reduced down to the desired 3:4 ratio. Now that we have our data formatted correctly, we can begin to build our GLM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">response_var <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;stroke ~ &#39;</span> <span style="color:#75715e"># Fit with all variables</span>
explanatory_vars <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39; + &#39;</span><span style="color:#f92672">.</span>join(X_train<span style="color:#f92672">.</span>columns<span style="color:#f92672">.</span>values)
formula <span style="color:#f92672">=</span> response_var <span style="color:#f92672">+</span> explanatory_vars

model <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>GLM<span style="color:#f92672">.</span>from_formula(formula, family<span style="color:#f92672">=</span>sm<span style="color:#f92672">.</span>families<span style="color:#f92672">.</span>Binomial(), data<span style="color:#f92672">=</span>data_glm)
result <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit()
result<span style="color:#f92672">.</span>summary()
</code></pre></div><p><img src="/images/strokeprediction_img/GLM_init.jpg" alt=""></p>
<p>From our initial model, we can see that almost all features are statistically significant (except for <em>over_weight</em> which was a feature I had previously created from <em>bmi</em>). We can now use the above model to predict on the test data, and we get the following initial (base model) results:</p>
<p><img src="/images/strokeprediction_img/GLM_result_init.jpg" alt=""></p>
<p>Okay, so not bad. But I&rsquo;m pretty confident that we can improve these results. How you may ask? Unlike machine learning models, we can use Grid or Random Search with a bunch of hyperparameters. Statistical models require a little more finesse. And by finesse I mean stepwise selection to find the best combination of these features. I&rsquo;ll evaluate these models based on their BIC, Recall, and F-Beta score (beta=0.85 provided the best results).</p>
<p><strong>NOTE</strong>: All 3 stepwise methods resulted in the exact same model, with the same coefficient &amp; p-values. So forward selection will explain in a bit more detail, and then exhaustive/backwards will just be a quick overview of the code and idea behind them.</p>
<h3 id="foward-stepwise-feature-selection">Foward Stepwise Feature Selection</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fitModel</span>(feature_subset):
    response_var <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;stroke ~ &#39;</span>
    explanatory_vars <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39; + &#39;</span><span style="color:#f92672">.</span>join(X_train[list(feature_subset)]<span style="color:#f92672">.</span>columns<span style="color:#f92672">.</span>values)
    formula <span style="color:#f92672">=</span> response_var <span style="color:#f92672">+</span> explanatory_vars
    model <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>GLM<span style="color:#f92672">.</span>from_formula(formula, family<span style="color:#f92672">=</span>sm<span style="color:#f92672">.</span>families<span style="color:#f92672">.</span>Binomial(), data<span style="color:#f92672">=</span>data_glm)
    result <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit()
    y_preds <span style="color:#f92672">=</span> round(result<span style="color:#f92672">.</span>predict(X_test[list(feature_subset)]))
    model_recall <span style="color:#f92672">=</span> recall_score(y_test, y_preds)
    model_fbeta <span style="color:#f92672">=</span> fbeta_score(y_test, y_preds, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.85</span>)
    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#39;model&#39;</span>: result, <span style="color:#e6db74">&#39;bic&#39;</span>: result<span style="color:#f92672">.</span>bic, <span style="color:#e6db74">&#39;recall&#39;</span>: model_recall, 
            <span style="color:#e6db74">&#39;fbeta&#39;</span>: model_fbeta, 
            <span style="color:#e6db74">&#39;features&#39;</span> : X_train[list(feature_subset)]<span style="color:#f92672">.</span>columns<span style="color:#f92672">.</span>values}
            
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forwardSelection</span>(predictors):
    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
    remaining_predictors <span style="color:#f92672">=</span> [p <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> X<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> p <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> predictors]
    results <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> remaining_predictors:
        results<span style="color:#f92672">.</span>append(fitModel(predictors<span style="color:#f92672">+</span>[p]))
    
    models <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(results)
    best_model <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;fbeta&#39;</span>, ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">0</span>,:]

    <span style="color:#66d9ef">return</span> best_model<span style="color:#f92672">.</span>values
</code></pre></div><p>Okay so a good amount of code just thrown at you there. The general idea for <strong>forward selection</strong> is that we iterate over all of the features in the training data set with each iteration adding the coefficient that improves our model the best. For each iteration, we try all features but only return a single model that had the highest F-Beta score, and append this to our list of final models. In total, we will have 10 models (given that we have 10 features) which we then plot and choose a final model from.</p>
<p><img src="/images/strokeprediction_img/stepwise.jpg" alt=""></p>
<p>From the above graphs, we can see that the model using 4 features resulted in the highest Recall &amp; F-Beta score (BIC is in the middle, but this model gave the best results). To save space I&rsquo;ll put the results from the model summary and confusion matrix for the test set in the final model section since all 3 stepwise methods resulted in the same model/scores.</p>
<h3 id="exhaustive-stepwise-feature-selection">Exhaustive Stepwise Feature Selection</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">exhaustiveSearch</span>(k):
    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
    results <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> combo <span style="color:#f92672">in</span> itertools<span style="color:#f92672">.</span>combinations(X_train, k):
        results<span style="color:#f92672">.</span>append(fitModel(combo))
    
    models <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(results)
    best_model <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;fbeta&#39;</span>, ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">0</span>,:]

    <span style="color:#66d9ef">return</span> best_model<span style="color:#f92672">.</span>values
</code></pre></div><p>The general idea for <strong>exhaustive selection</strong> is that we try every combinations of predictors given a certain length (in this case, 1 through 10) and return the best model. For example, in step 2 we try every combination of 2 features and so on. <em>Note: forward search took around 1 second for all 10 steps, where exhaustive search took around 25 seconds - for a much larger model this would need to be considered before using.</em></p>
<h3 id="backwards-stepwise-feature-selection">Backwards Stepwise Feature Selection</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(predictors):
    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
    results <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> combo <span style="color:#f92672">in</span> itertools<span style="color:#f92672">.</span>combinations(predictors, len(predictors)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
        results<span style="color:#f92672">.</span>append(fitModel(combo))
    
    models <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(results)
    best_model <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>sort_values(by<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;fbeta&#39;</span>, ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">0</span>,:]

    <span style="color:#66d9ef">return</span> best_model<span style="color:#f92672">.</span>values
</code></pre></div><p>The final method is <strong>backwards selection</strong>, which will begin with all of our features in the data set, and each iteration it will drop the least significant coefficient. So we start with 10 features, and work our way down to 1 at the end.</p>
<h3 id="final-statistical-model">Final Statistical Model</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">models_bwd_flipped<span style="color:#f92672">.</span>loc[<span style="color:#ae81ff">4</span>][<span style="color:#e6db74">&#39;Model&#39;</span>]<span style="color:#f92672">.</span>summary() <span style="color:#75715e"># final model</span>
</code></pre></div><p><img src="/images/strokeprediction_img/GLM_final.jpg" alt=""></p>
<p>Our final model! Although I&rsquo;m showing the model from the Backward selection, both the forward/exhaustive models also had the same 4 features as their top performer (so just think of the above as the general winner for each method). All coefficients are now statistically significant, and to my surprise some of what I thought were key features have been dropped. <em>Hypertension, heart disease, smoking,</em> and <em>average glucose level</em> are key indicators for stroke but they were all dropped? Blows my mind, but our model seemed to perform much better without them and the results below show that:</p>
<p><img src="/images/strokeprediction_img/GLM_result_final.jpg" alt=""></p>
<p>Okay, so what improvements do we have over the base model:</p>
<ul>
<li>True Negatives increased by 2, while False Positives decreased by 2 (less people classified as stroke that did not have a stroke). :star:</li>
<li>False Negative decreased by 21, while True Positives increased by 21 (more people classified as stroke that actually had a stroke). :star:</li>
<li>Precision increased from 57% to 66%.</li>
<li>Recall increased from 58% to 82% (this was the most important evaluation metric to improve).</li>
<li>Accuracy increased from 63% to 74%.</li>
</ul>
<p><em>Note: the above came from the classification report, see either the README.md or stroke_prediction.ipynb</em>.</p>
<p>A huge improvement! The changes marked by the :star: were our main metrics to improve and we successfully did. There&rsquo;s still room to improve (False Positives could be lowered a bit but this was a trade off to increase Recall) but this is a strong model. However, will it be strong enough to stay champion? Let&rsquo;s move onto the ensemble modeling and see.</p>
<h2 id="ensemble-modeling">Ensemble Modeling</h2>
<p>For this section I initially planned on using the Scikit-Learn library, but after some research I found another one that seemed to work a bit better for my problem: <a href="https://imbalanced-learn.org/stable/">ImBalanced-Learn</a>. This library is build on Scikit-Learn put is designed mainly for classification with imbalanced classes. 3 initial models will be fit, cross-validated on all of the data and had the following scores:</p>
<p><img src="/images/strokeprediction_img/ML_init_models.jpg" alt=""></p>
<ul>
<li><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html">BalancedRandomForestClassifier</a> - A balanced random forest classifier that under-samples each bootstrap sample to balance our classes.</li>
<li><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.RUSBoostClassifier.html">RUSBoostClassifier</a> - An AdaBoost algorithm that performs random under-sampling at each iteration to balance classes.</li>
<li><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedBaggingClassifier.html">BalancedBaggingClassifier</a> - A bagging classifier that implements data balancing at fit time.</li>
</ul>
<p>This may be a surprise, but the model chosen to continue with for hyperparameter tuning is the <strong>BalancedRandomForestClassifier</strong>. Although it had the lowest accuracy, precision, and F-Beta score it also had the highest recall (which is what we want). I believe with changes to our data set and model parameters, we will be able to improve these results.</p>
<h3 id="data-set-up">Data Set Up</h3>
<p>Now that we have our initial model set up, we will want to create the training and testing splits. Unlike the previous section, we will want to keep the training set imbalanced but balance the testing set to have the same ratio used to evaluate the GLM, which is 4:3.</p>
<blockquote>
<p><em>NOTE: I now wonder if these results differed since we used different testing sets. The Ensemble Model testing set was smaller and had different observations, so this could be affecting the results.</em></p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sampled_data_split</span>(data):
    stroke_obs <span style="color:#f92672">=</span> data[data<span style="color:#f92672">.</span>stroke <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>]
    no_stroke_obs <span style="color:#f92672">=</span> data[data<span style="color:#f92672">.</span>stroke <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>]

    sample_size <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ceil(<span style="color:#ae81ff">0.2</span> <span style="color:#f92672">*</span> len(stroke_obs))
    stroke_sample <span style="color:#f92672">=</span> stroke_obs<span style="color:#f92672">.</span>sample(n<span style="color:#f92672">=</span>int(sample_size), random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    stroke_extra <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([stroke_obs, stroke_sample])<span style="color:#f92672">.</span>loc[stroke_obs<span style="color:#f92672">.</span>index<span style="color:#f92672">.</span>symmetric_difference(stroke_sample<span style="color:#f92672">.</span>index)]
    sample_size <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ceil(len(stroke_sample)<span style="color:#f92672">*</span><span style="color:#ae81ff">1.5</span>)
    no_stroke_sample <span style="color:#f92672">=</span> no_stroke_obs<span style="color:#f92672">.</span>sample(n<span style="color:#f92672">=</span>int(sample_size), random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    no_stroke_extra <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([no_stroke_obs,
                                 no_stroke_sample])<span style="color:#f92672">.</span>loc[no_stroke_obs<span style="color:#f92672">.</span>index<span style="color:#f92672">.</span>symmetric_difference(no_stroke_sample<span style="color:#f92672">.</span>index)]

    train_set <span style="color:#f92672">=</span> shuffle(pd<span style="color:#f92672">.</span>concat([stroke_extra, no_stroke_extra], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
    test_set <span style="color:#f92672">=</span> shuffle(pd<span style="color:#f92672">.</span>concat([stroke_sample, no_stroke_sample], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
    X_train, y_train <span style="color:#f92672">=</span> train_set<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#39;stroke&#39;</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>), train_set<span style="color:#f92672">.</span>stroke
    X_test, y_test <span style="color:#f92672">=</span> test_set<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#39;stroke&#39;</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>), test_set<span style="color:#f92672">.</span>stroke
    <span style="color:#66d9ef">return</span> X_train, X_test, y_train, y_test
</code></pre></div><p><img src="/images/strokeprediction_img/ML_data.jpg" alt=""></p>
<h3 id="base-model">Base Model</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">model <span style="color:#f92672">=</span> BalancedRandomForestClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>, class_weight<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;balanced&#39;</span>)
model <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(X_train, y_train)
y_preds <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</code></pre></div><p><img src="/images/strokeprediction_img/ML_results_init.jpg" alt=""></p>
<ul>
<li>Accuracy: 81%</li>
<li>Precision: 71%</li>
<li>Recall: 88%</li>
</ul>
<p>Okay, so already our model seems to be performing better than the GLM. This isn&rsquo;t a huge surprise, I expected this to happen for the most part. But now that we have our base model, let&rsquo;s see if we can increase these scores.</p>
<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<p><em>SPOILER</em>: The base model performed better than any of the tuned models below. Just in case you wanted to skip over the section (I don&rsquo;t blame you, I probably wouldn&rsquo;t make it through half this article with my attention span) the results seemed to slightly decrease with the tuned models. Two different methods were used in an attempt to improve the model: <em>GridSearchCV</em> and <em>Exhaustive Feature Selection</em>.</p>
<h4 id="gridsearchcv">GridSearchCV</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">parameters <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;n_estimators&#39;</span>: [<span style="color:#ae81ff">25</span>, <span style="color:#ae81ff">50</span>, <span style="color:#ae81ff">75</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">125</span>],
              <span style="color:#e6db74">&#39;criterion&#39;</span>: [<span style="color:#e6db74">&#39;gini&#39;</span>, <span style="color:#e6db74">&#39;entropy&#39;</span>],
              <span style="color:#e6db74">&#39;max_features&#39;</span>: [<span style="color:#e6db74">&#39;auto&#39;</span>, <span style="color:#e6db74">&#39;log2&#39;</span>, <span style="color:#66d9ef">None</span>],
              <span style="color:#e6db74">&#39;sampling_strategy&#39;</span>: [<span style="color:#e6db74">&#39;auto&#39;</span>, <span style="color:#e6db74">&#39;all&#39;</span>, <span style="color:#ae81ff">0.7</span>],
              <span style="color:#e6db74">&#39;class_weight&#39;</span>: [<span style="color:#e6db74">&#39;balanced&#39;</span>, <span style="color:#e6db74">&#39;balanced_subsample&#39;</span>]}

scoring_params <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;precision&#39;</span>: make_scorer(precision_score), 
                  <span style="color:#e6db74">&#39;recall&#39;</span>: make_scorer(recall_score), 
                  <span style="color:#e6db74">&#39;fbeta&#39;</span>: make_scorer(fbeta_score, <span style="color:#ae81ff">1.25</span>)}

hpt_model <span style="color:#f92672">=</span> BalancedRandomForestClassifier(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
rs_cv <span style="color:#f92672">=</span> GridSearchCV(hpt_model, parameters, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, scoring<span style="color:#f92672">=</span>scoring_params, refit<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;fbeta&#39;</span>, n_jobs<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
search <span style="color:#f92672">=</span> rs_cv<span style="color:#f92672">.</span>fit(X_train, y_train)
search<span style="color:#f92672">.</span>best_params_
<span style="color:#75715e"># {&#39;class_weight&#39;: &#39;balanced&#39;,  </span>
<span style="color:#75715e">#  &#39;criterion&#39;: &#39;gini&#39;,  </span>
<span style="color:#75715e">#  &#39;max_features&#39;: &#39;auto&#39;,  </span>
<span style="color:#75715e">#  &#39;n_estimators&#39;: 25,  </span>
<span style="color:#75715e">#  &#39;sampling_strategy&#39;: &#39;auto&#39;}</span>
</code></pre></div><p>Using the above parameters, a new model was fit and used to predict on the data. To save room I won&rsquo;t include the confusion matrix or classification report, but the main take away is that True Negatives decreased by 1 and False Positives increased by 1 (slightly worse than base model). However, the model increased it&rsquo;s prediction speed by 2x which could be considered if that was the goal for the product (slightly worse results for much faster performance).</p>
<h4 id="exhaustive-feature-selection">Exhaustive Feature Selection</h4>
<p><em>NOTE: Here we used the model from above to try and improve the results since it had similar results as the base model but faster performance).</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">efs <span style="color:#f92672">=</span> EFS(search<span style="color:#f92672">.</span>best_estimator_,
          min_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
          max_features<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>,
          scoring<span style="color:#f92672">=</span>make_scorer(fbeta_score, beta<span style="color:#f92672">=</span><span style="color:#ae81ff">1.25</span>),
          cv<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
          n_jobs<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)

efs <span style="color:#f92672">=</span> efs<span style="color:#f92672">.</span>fit(X_train, y_train)
list(X_train<span style="color:#f92672">.</span>iloc[:, list(efs<span style="color:#f92672">.</span>best_idx_)]<span style="color:#f92672">.</span>columns)
<span style="color:#75715e"># [&#39;age&#39;, &#39;hypertension&#39;, &#39;avg_glucose_level&#39;, &#39;bmi&#39;, &#39;smokes&#39;, &#39;never_smoked&#39;]</span>
</code></pre></div><p>Again, a new model was fit using only the above features but this resulted in even worse performance. Our True Negatives decreased by 1 and False Positives increased by 1, as well as False Negatives increasing by 2 and True Positives decreasing by 2. From this, our base model seems to have the best performance and will be our final model.</p>
<h3 id="final-model">Final Model</h3>
<p>Okay, so this was one of those cases where hyperparameter tuning didn&rsquo;t seem to benefit our model very much (actually it seemed to hurt our model more than anything). Our base model seemed to perform the best out of all 3 models, and the features had the following importance:</p>
<p><img src="/images/strokeprediction_img/ML_feat_import.jpg" alt=""></p>
<p>An important note is that the <em>age</em>, <em>bmi</em>, and <em>avg_glucose_levels</em> were not normalized and all other features had discrete values in the range [0,1], but normalizing the inputs resulted in the same feature importance but with worse performance.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Comparing the statistical and ensemble models, we can see that the ensemble model seems to be performing better (although not be a large margin). We have the following difference based on the final models for both types:</p>
<ul>
<li><strong>Accuracy</strong>: Ensemble model has a 7% advantage.</li>
<li><strong>Precision</strong>: Ensemble model has a 5% advantage.</li>
<li><strong>Recall</strong>: Ensemble model has a 6% advantage.</li>
</ul>
<p>An important note is that the two model types were trained and tested on different data. The statistical model had oversampling of the minority class for the training data in order to balance it since we could not handle the imbalance within the model itself (but the testing data was undersampled). The ensemble model had regular training data (not balanced) but the testing data was undersampled and slightly smaller (extra observations left over from the undersampling were also added back into training since imbalanced data was not a concern).</p>
<p>Overall, the ensemble model seems to be slightly stronger in all evaluation aspects. The computational speeds were similar for both models. Places to improve upon for our models would to be to try and get the data splits to be more similar in order to have more validity to our statement of the stronger model.</p>

    </article>


    





    
    
    




    
    
    

<section id="articleNext" class="section nartrow">
    <h3 class="footer-next-heading">More articles from Derek Helms</h3>
    <div class="footer-spacer"></div>
    <div class="next-articles-grid" numberOfArticles={numberOfArticles}>
        <div class="post-row">
            
                <a href="/p/prodvspop/" class="article-link"
                 id="article-link-bigger">
                    <div>
                        <div class="image-container">
                            <img src="/images/prodvspop_img/banner.png" class="article-image" />
                        </div>
                        <div>
                            <h2 class="article-title">
                                Global Food Production vs. Population: Predicting the Future
                            </h2>
                            <p class="article-excerpt">
                                In this project I create a Polynomial Regression Model to estimate the necessary global food production to support a given population size. The main goal is estimating the 2050 production necessary to support our population, as this is the year eastimated to approach 10 billion people on Earth.
                            </p>
                            <div class="article-metadata">
                                January 26, 2021 Â· 8 min read
                            </div>
                        </div>
                    </div>
                </a>
            
                <a href="/p/newsiteinfo/" class="article-link"
                >
                    <div>
                        <div class="image-container">
                            <img src="/images/newsite_banner.jpg" class="article-image" />
                        </div>
                        <div>
                            <h2 class="article-title">
                                Introduction to my new website [IN PROGRESS]
                            </h2>
                            <p class="article-excerpt">
                                A brief breakdown of my new website, what went into altering the layout, and credit to the original designer.
                            </p>
                            <div class="article-metadata">
                                January 19, 2022 Â· 1 min read
                            </div>
                        </div>
                    </div>
                </a>
            
        </div>
    </div>
</section>

</section>


 <script src="/js/progressBar.js"></script>

        
        <div class="footer-gradient"></div>
    <div class="section narrow">
      <div class="footer-hr"></div>
      <div class="footer-container">
        
    </div>
</div>

    </div>

    
    <script src="/js/prism.js"></script>
</body>

</html>