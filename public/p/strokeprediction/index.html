<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='In this project I compare the performance of Statistical and Machine Learning models for predicting whether or not a patient will have a stroke on a data set that has a large class imbalance.'><title>Stroke Prediction: Battle of the Learning Methods</title>

<link rel='canonical' href='/p/strokeprediction/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='Stroke Prediction: Battle of the Learning Methods'>
<meta property='og:description' content='In this project I compare the performance of Statistical and Machine Learning models for predicting whether or not a patient will have a stroke on a data set that has a large class imbalance.'>
<meta property='og:url' content='/p/strokeprediction/'>
<meta property='og:site_name' content='Derek Helms'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Python' /><meta property='article:tag' content='Statistics' /><meta property='article:tag' content='Machine Learning' /><meta property='article:published_time' content='2021-03-21T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2021-03-21T00:00:00&#43;00:00'/><meta property='og:image' content='/p/strokeprediction/img/header.jpg' />
<meta name="twitter:title" content="Stroke Prediction: Battle of the Learning Methods">
<meta name="twitter:description" content="In this project I compare the performance of Statistical and Machine Learning models for predicting whether or not a patient will have a stroke on a data set that has a large class imbalance."><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='/p/strokeprediction/img/header.jpg' /><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
    </head>
    <body class="">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "light");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.body.dataset.scheme = 'dark';
        } else {
            document.body.dataset.scheme = 'light';
        }
    })();
</script><div class="container main-container flex on-phone--column extended article-page with-toolbar">
            <aside class="sidebar left-sidebar sticky">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header class="site-info">
        
        <h2 class="site-description"></h2>
    </header>

    <ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/'>
                
                    <svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px"><path d="M 12 2.0996094 L 1 12 L 4 12 L 4 21 L 11 21 L 11 15 L 13 15 L 13 21 L 20 21 L 20 12 L 23 12 L 12 2.0996094 z M 12 4.7910156 L 18 10.191406 L 18 11 L 18 19 L 15 19 L 15 13 L 9 13 L 9 19 L 6 19 L 6 10.191406 L 12 4.7910156 z"/></svg>
                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about-myself/'>
                
                    <svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px"><path d="M 10 0 C 6.24 -0.02 2.97 3.1 3 7 L 3 8 L 3 11.748047 C 2.4701855 12.225307 2.0097656 13.000361 2.0097656 14.011719 C 2.0097656 15.27649 2.7280504 16.191398 3.4042969 16.585938 C 3.7657984 16.796847 3.8681582 16.757564 4.125 16.810547 C 5.6269646 19.927424 8.5402409 22.551214 11.867188 22.994141 L 12 23.011719 L 12.132812 22.994141 C 15.459499 22.550139 18.373013 19.926518 19.875 16.810547 C 20.131842 16.757567 20.234202 16.796847 20.595703 16.585938 C 21.27195 16.191398 21.990234 15.27649 21.990234 14.011719 C 21.990234 12.746948 21.27195 11.830087 20.595703 11.435547 C 20.297075 11.261319 20.232618 11.304211 20 11.248047 L 20 6 C 20 3.791 18.209 2 16 2 L 15 2 L 15 2.0097656 C 15 2.0097656 13.98 0 10 0 z M 12.445312 8 L 13 8 L 15 8 C 16.630274 8 17.949827 9.3018298 17.992188 10.921875 L 17.992188 13.013672 L 19.142578 13.013672 C 19.148378 13.014264 19.368026 13.035792 19.587891 13.164062 C 19.810644 13.294023 19.990234 13.37849 19.990234 14.011719 C 19.990234 14.644948 19.810644 14.727462 19.587891 14.857422 C 19.365137 14.987382 19.136719 15.009766 19.136719 15.009766 L 18.519531 15.044922 L 18.275391 15.611328 C 17.128688 18.256067 14.48056 20.569091 12 20.96875 C 9.5197032 20.569931 6.8713505 18.257043 5.7246094 15.611328 L 5.4804688 15.044922 L 4.8632812 15.009766 C 4.8632812 15.009766 4.6348629 14.987386 4.4121094 14.857422 C 4.1893558 14.727462 4.0097656 14.644948 4.0097656 14.011719 C 4.0097656 13.37849 4.1893558 13.294023 4.4121094 13.164062 C 4.6319738 13.035789 4.8516269 13.014264 4.8574219 13.013672 L 6.0078125 13.013672 L 6.0078125 10 L 9 10 C 10.477 10 11.752312 9.191 12.445312 8 z"/></svg>
                
                <span>About Myself</span>
            </a>
        </li>
        
        

        <li >
            <a href='https://www.linkedin.com/in/derek-helms'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
                
                <span>LinkedIn</span>
            </a>
        </li>
        
        

        <li >
            <a href='https://github.com/dhelms1'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                
                <span>GitHub</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives/'>
                
                    <?xml version="1.0"?><svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px">    <path d="M13.172,2H6C4.9,2,4,2.9,4,4v16c0,1.1,0.9,2,2,2h12c1.1,0,2-0.9,2-2V8.828c0-0.53-0.211-1.039-0.586-1.414l-4.828-4.828 C14.211,2.211,13.702,2,13.172,2z M18.5,9H13V3.5L18.5,9z"/></svg>
                
                <span>Archive</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search/'>
                
                    <?xml version="1.0"?><svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px">    <path d="M 9 2 C 5.1458514 2 2 5.1458514 2 9 C 2 12.854149 5.1458514 16 9 16 C 10.747998 16 12.345009 15.348024 13.574219 14.28125 L 14 14.707031 L 14 16 L 20 22 L 22 20 L 16 14 L 14.707031 14 L 14.28125 13.574219 C 15.348024 12.345009 16 10.747998 16 9 C 16 5.1458514 12.854149 2 9 2 z M 9 4 C 11.773268 4 14 6.2267316 14 9 C 14 11.773268 11.773268 14 9 14 C 6.2267316 14 4 11.773268 4 9 C 4 6.2267316 6.2267316 4 9 4 z"/></svg>
                
                <span>Search</span>
            </a>
        </li>
        

        
            <li id="dark-mode-toggle">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                <span>Dark Mode</span>
            </li>
        
    </ol>
</aside>

            <main class="main full-width">
    <div id="article-toolbar">
        <a href="/" class="back-home">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



            <span>Back</span>
        </a>
    </div>

    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/strokeprediction/">
                <img src="/p/strokeprediction/img/header_hucdbe1c766fbb076b014b35afd7dcfcdc_900878_800x0_resize_q75_box.jpg"
                        srcset="/p/strokeprediction/img/header_hucdbe1c766fbb076b014b35afd7dcfcdc_900878_800x0_resize_q75_box.jpg 800w, /p/strokeprediction/img/header_hucdbe1c766fbb076b014b35afd7dcfcdc_900878_1600x0_resize_q75_box.jpg 1600w"
                        width="800" 
                        height="408" 
                        loading="lazy"
                        alt="Featured image of post Stroke Prediction: Battle of the Learning Methods" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/python/" >
                Python
            </a>
        
            <a href="/categories/projects/" >
                Projects
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/p/strokeprediction/">Stroke Prediction: Battle of the Learning Methods</a>
    </h2>

    
    <h3 class="article-subtitle">
        In this project I compare the performance of Statistical and Machine Learning models for predicting whether or not a patient will have a stroke on a data set that has a large class imbalance.
    </h3>
    <footer class="article-time">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <time class="article-time--published">Mar 21, 2021</time>
    </footer></div>
</header>

    <section class="article-content">
    <blockquote>
<p><a class="link" href="https://github.com/dhelms1/stroke_prediction"  target="_blank" rel="noopener"
    ><strong>Original GitHub Repository</strong></a></p>
</blockquote>
<h1 id="alright-ill-admit-it">Alright, I&rsquo;ll admit it&hellip;</h1>
<p>It sounded a lot cooler in my head. &ldquo;Battle of the Learning Methods: Statistical vs Machine Learning&rdquo;. Man vs. Machine (well not really, but you get the point). A test of speed, strength, learning, and maybe a little bit of overfitting. Maybe it&rsquo;s just me that&rsquo;s excited about the project, but I finally get to implement what I learn in school against what I do in my free time. This term I took both Sampling Methods and Applied Statistics II, and this brought me to think if I could create a project that implemented the material from those classes and my knowledge of Machine Learning? This project was the answer.</p>
<p>Stroke is the third leading cause of death in the United States, with over 140,000 people dying annually. Each year approximately 795,000 people suffer from a stroke with nearly 75% of these occurring in people over the age of 65. Using medical record information (age, gender, BMI, smoking, diseases, etc.) we will create models that can accurately predict whether or not a patient has had a stroke. The goal for this project will be to explore the data and find any correlations between features and the response variable stroke that will allow us to engineer new features for the data. After doing this, we will make a comparison between Statistical Modeling and Ensemble Modeling to see which we are able to achieve better results with. <em>Note: models will be evaluated by an F-Beta and Recall score since avoiding a missed diagnosis is the main focus.</em></p>
<h1 id="data-exploration--feature-engineering">Data Exploration &amp; Feature Engineering</h1>
<p>The data originated from the <a class="link" href="https://www.kaggle.com/fedesoriano/stroke-prediction-dataset"  target="_blank" rel="noopener"
    >Kaggle</a> repository for stroke prediction. There are 11 features that were recorded for 5110 observations. Personally I like to perform feature engineering while doing my EDA so that I can create these new features while the ideas are fresh in my mind. From importing the data, our initial data set has the following features:</p>
<p><figure style="flex-grow: 526; flex-basis: 1263px">
		<a href="/p/strokeprediction/img/head.jpg" data-size="795x151"><img src="/p/strokeprediction/img/head.jpg"
				
				width="795"
				height="151"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>However, the <em>id</em> feature is just a unique identifier, so it can be dropped from the data set.</p>
<h2 id="bmi-body-mass-index">BMI (Body Mass Index)</h2>
<p>From the initial exploration, I found that the only feature that had missing values was <em>bmi</em> (with 201 na&rsquo;s). Before filling the missing values, we have the following distribution for <em>bmi</em>:
<figure style="flex-grow: 194; flex-basis: 466px">
		<a href="/p/strokeprediction/img/bmi_init.jpg" data-size="523x269"><img src="/p/strokeprediction/img/bmi_init.jpg"
				
				width="523"
				height="269"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Two important notes here: the data seems to be somewhat normally distributed and we seem to have extreme values for bmi. To handle the missing values, I decided to fill them with the median value +/- random noise between [1,4]. Do I have a logical explanation for why I did this? Of course not, but filling with the median only increased a single value being present, where adding the random noise allowed for a more &ldquo;even&rdquo; distribution around the median. On top of this, I filter the data to only include values up to the 99th quantile (approximately bmi = 53) to filter out the outliers. Doing this we used the following code and resulted in the following graph:</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="c1"># Fill missing with values with median +/- random noises between [1,4]</span>
<span class="n">error_term</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">bmi</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())),</span><span class="mi">2</span><span class="p">)</span>
<span class="n">bmi_fill</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">bmi</span><span class="o">.</span><span class="n">median</span><span class="p">()</span> <span class="o">+</span> <span class="n">error_term</span>
<span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">bmi</span><span class="o">.</span><span class="n">isnull</span><span class="p">(),</span> <span class="s1">&#39;bmi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bmi_fill</span>

<span class="c1"># Remove any values above the 99th quantile (approx BMI = 53)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">bmi</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">bmi</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)]</span>
</code></pre></div><p><figure style="flex-grow: 181; flex-basis: 435px">
		<a href="/p/strokeprediction/img/bmi_final.jpg" data-size="505x278"><img src="/p/strokeprediction/img/bmi_final.jpg"
				
				width="505"
				height="278"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Okay so a bit more normal right? Maybe a little right skew in there too? I&rsquo;m happy with where that&rsquo;s at. Now that we&rsquo;ve handle the <em>bmi</em> feature it&rsquo;s time to create our first new feature: weight classes based on patient bmi.</p>
<h3 id="weight-class-feature-engineering">Weight Class (Feature Engineering)</h3>
<p>I decided to create weight classes based on the <a class="link" href="https://www.nhlbi.nih.gov/health/educational/lose_wt/BMI/bmicalc.htm"  target="_blank" rel="noopener"
    >National Heart, Lung, and Blood Institute BMI Scale</a> which gives us the following categories:</p>
<ul>
<li>Underweight = Less than 18.5</li>
<li>Normal weight = 18.5 - 24.9</li>
<li>Overweight = 25 - 29.9</li>
<li>Obesity = Greater than 30</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="n">row</span><span class="p">):</span> <span class="c1"># see function.py file for function definition</span>
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">30</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;obese&#39;</span>
    <span class="k">elif</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">25</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">30</span><span class="p">)):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;over weight&#39;</span>
    <span class="k">elif</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">18.5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">25</span><span class="p">)):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;normal weight&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;under weight&#39;</span>
    <span class="k">return</span> <span class="n">val</span>
</code></pre></div><p><figure style="flex-grow: 116; flex-basis: 279px">
		<a href="/p/strokeprediction/img/stroke_weight_class.jpg" data-size="468x402"><img src="/p/strokeprediction/img/stroke_weight_class.jpg"
				
				width="468"
				height="402"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>So it seems that the majority of the data falls into the <em>obese</em> category, followed by <em>over weight</em>, <em>normal weight</em>, and finally <em>under weight</em>. The majority of strokes seem to occur in for patients that fall under either obese or over weight, some for normal weight, and only a single patient that was under weight. If we look further into this (not shown in this article, refer to notebook) we get the following mean ages for each weight group: normal - 33.76, obese - 49.96, over weight - 49.39, and under weight - 10.91. It seems that older patients, who are also more susceptible to strokes, fall into the obese and over weight classes.</p>
<h2 id="age">Age</h2>
<p><figure style="flex-grow: 339; flex-basis: 815px">
		<a href="/p/strokeprediction/img/age_part.jpg" data-size="795x234"><img src="/p/strokeprediction/img/age_part.jpg"
				
				width="795"
				height="234"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Age does not seem to have a certain type of distribution for all observations (closest to a uniform distribution). When looking strictly at stroke observations, we can see the majority of the data is above 35 and has an extreme left skew. The distribution for no stroke also seems to not have a certain distribution (but could be classified at closely uniform).</p>
<h3 id="age-class-feature-engineering">Age Class (Feature Engineering)</h3>
<p>Using the Age Categories from the Canadian Statistics website, I created features for the life cycle groupings defined on their website:</p>
<ul>
<li>Children (0-14)</li>
<li>Youth (15-24)</li>
<li>Adults (25-64)</li>
<li>Seniors (65+)</li>
</ul>
<p>I also created a generic <em>age_class</em> feature to be used for graphing. Think of the above individual features as a one-hot-encoding of the age_class feature.</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">data</span><span class="p">[</span><span class="s1">&#39;child&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;youth&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(((</span><span class="n">data</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;=</span> <span class="mi">15</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">25</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;adult&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(((</span><span class="n">data</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;=</span> <span class="mi">25</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">65</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;senior&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">age</span><span class="p">(</span><span class="n">row</span><span class="p">):</span> <span class="c1"># see function.py file for function definition</span>
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;child&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;child&#39;</span>
    <span class="k">elif</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;youth&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;youth&#39;</span>
    <span class="k">elif</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;adult&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;adult&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="s1">&#39;senior&#39;</span>
    <span class="k">return</span> <span class="n">val</span>

<span class="n">data</span><span class="p">[</span><span class="s1">&#39;age_class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">age</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><figure style="flex-grow: 129; flex-basis: 310px">
		<a href="/p/strokeprediction/img/stroke_age_class.jpg" data-size="460x356"><img src="/p/strokeprediction/img/stroke_age_class.jpg"
				
				width="460"
				height="356"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>It seems that the majority of the people observed in the data were young adults (25-64). However, people categorized as seniors had the highest amount of strokes. It also looks like both children and youth have very low stroke rates, and looking into this further we can see only two female children (age 1 and 14) had strokes. An important note is that the two children who had strokes were both categorized as obese with a BMI of approximately 30 for both.</p>
<h2 id="remaining-features-exploration">Remaining Features Exploration</h2>
<p>The remaining features didn&rsquo;t seem to have too many correlated findings or values that allowed for any feature engineering, so they will be generalized into this section. This will just be a brief overview of a few features, so refer the <em>stroke_prediction.ipynb</em> section <em>Age</em> &amp; <em>Other Numerical Features</em> for a more in-depth exploration.</p>
<p><figure style="flex-grow: 351; flex-basis: 844px">
		<a href="/p/strokeprediction/img/glucose_part.jpg" data-size="795x226"><img src="/p/strokeprediction/img/glucose_part.jpg"
				
				width="795"
				height="226"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>We can see that <em>avg_glucose_level</em> doesn&rsquo;t have a certain distribution (could be considered bimodel), and for both stroke/no stroke we have the same findings (with stroke having a slightly higher second peak).</p>
<p><figure style="flex-grow: 143; flex-basis: 343px">
		<a href="/p/strokeprediction/img/stroke_hyper.jpg" data-size="470x328"><img src="/p/strokeprediction/img/stroke_hyper.jpg"
				
				width="470"
				height="328"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Looking at the above graph, we don&rsquo;t seem to have any clear indicator of <em>Hypertension</em> being a key feature for stroke detection. However, if we look at the ratio of stroke to no stroke within the Hypertension classes, we can find that people with Hypertension are about 4x more likely to have a stroke.</p>
<p><figure style="flex-grow: 141; flex-basis: 339px">
		<a href="/p/strokeprediction/img/stroke_heart.jpg" data-size="464x328"><img src="/p/strokeprediction/img/stroke_heart.jpg"
				
				width="464"
				height="328"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Again looking at the above graph, we don&rsquo;t seem to have any clear indicator of <em>Heart Disease</em> being a key feature for stroke detection. However, if we look at the ratio of stroke to no stroke within the Heart Disease classes, we can find that people with Heart Disease are about 5x more likely to have a stroke.</p>
<p><figure style="flex-grow: 126; flex-basis: 303px">
		<a href="/p/strokeprediction/img/stroke_gender.jpg" data-size="463x366"><img src="/p/strokeprediction/img/stroke_gender.jpg"
				
				width="463"
				height="366"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>We seem to have around 800 more females than males in our dataset, but the amount of strokes for each gender seem to be about equivalent. Similarly, their ratio in respect to their group total are about equivalent. There is no clear evidence that one gender is more susceptible to stroke then the other.</p>
<h2 id="final-data-processing">Final Data Processing</h2>
<p>Now that we have finished exploring the data, we will want to create a final data set that can be used in the modeling section. We will drop all unused features and one-hot-encode the remaining categorical features.</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="c1"># Drop features that wont be used</span>
<span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;work_type&#39;</span><span class="p">,</span> <span class="s1">&#39;Residence_type&#39;</span><span class="p">,</span> <span class="s1">&#39;age_class&#39;</span><span class="p">,</span> <span class="s1">&#39;gender&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Encode and create any new necessary features</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;ever_married&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;ever_married&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;No&#39;</span><span class="p">,</span> <span class="s1">&#39;Yes&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;age_over_45&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">data</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;=</span> <span class="mi">45</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;over_weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">data</span><span class="o">.</span><span class="n">bmi</span> <span class="o">&gt;=</span> <span class="mi">25</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;smokes&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">data</span><span class="o">.</span><span class="n">smoking_status</span> <span class="o">==</span> <span class="s1">&#39;smokes&#39;</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;never_smoked&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(((</span><span class="n">data</span><span class="o">.</span><span class="n">smoking_status</span> <span class="o">==</span> <span class="s1">&#39;never smoked&#39;</span><span class="p">)</span> <span class="o">|</span> 
                                 <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">smoking_status</span> <span class="o">==</span> <span class="s1">&#39;Unkown&#39;</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;smoking_status&#39;</span><span class="p">,</span> <span class="s1">&#39;child&#39;</span><span class="p">,</span> <span class="s1">&#39;youth&#39;</span><span class="p">,</span> <span class="s1">&#39;adult&#39;</span><span class="p">,</span> <span class="s1">&#39;senior&#39;</span><span class="p">,</span> <span class="s1">&#39;weight_class&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div><p><figure style="flex-grow: 126; flex-basis: 302px">
		<a href="/p/strokeprediction/img/feat_corr.jpg" data-size="795x630"><img src="/p/strokeprediction/img/feat_corr.jpg"
				
				width="795"
				height="630"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Looking at the above matrix it seems that stroke has the highest correlation with the following features: age (0.25), age_over_45 (0.21), heart_disease (0.14), hypertension (0.13), and avg_glucose_level (0.13). There are other features that are correlated with stroke that we engineered, but are lower than the ones listed previously. Now that our data is processed and a subset of the features are kept, we can begin the modeling section.</p>
<h1 id="modeling">Modeling</h1>
<h3 id="brief-overview-of-data-formatting">Brief Overview of Data Formatting</h3>
<p>For the <strong>Statistical Modeling</strong> section, the data was reformatted in two ways to accommodate the large class imbalance (around 20x more observations of &ldquo;No Stroke&rdquo; compared to &ldquo;Stroke&rdquo;):</p>
<ul>
<li>
<p>Training data was balanced by using SMOTE (Synthetic Minority Oversampling Technique) to increased of minority &ldquo;stroke&rdquo; class to a 3:4 ratio with the majority class &ldquo;no stroke&rdquo;. This resulted in around 3400 majority observations (0) and 2500 minority observations (1).</p>
</li>
<li>
<p>Testing data was balanced using the NearMiss algorithm, which undersampled the majority class to a 4:3 ratio with the minority class. This resulted in around 120 majority and 90 minority observations to be used for evaluation. <em>Note: when evaluating based on oversampled data, I did not feel the results were as accurate since repeated observations were increasing the scores. I want the model to be prepared for real world data rather than higher metrics on repeated data.</em></p>
</li>
</ul>
<p>For the <strong>Ensemble Modeling</strong> section, the data was reformatted in the following ways to accommodate the class imbalance:</p>
<ul>
<li>
<p>Training data was left untouched since the ensemble algorithms we used are able to handle the imbalance within the model itself.</p>
</li>
<li>
<p>Testing data was resampled so that we would have a &ldquo;Stroke&rdquo; to &ldquo;No Stroke&rdquo; ratio of 2:3, resulting in around 50 minority and 75 majority observations (slightly smaller than the statistical modeling data).</p>
</li>
<li>
<p>An important note is that the extra observations from the majority class (after being undersampled) in the testing data were added back into the training data so that we had more data to train on. This was due to the algorithms being able to handle class imbalance (so more majority observations would not have a negative effect).</p>
</li>
</ul>
<h2 id="statistical-modeling">Statistical Modeling</h2>
<p>Using the <a class="link" href="https://www.statsmodels.org/v0.10.1/"  target="_blank" rel="noopener"
    >StatsModels</a> module, I created a Generalized Linear Model based on the Logistic (Binomial) family. Before the actual model can be built, we first need to set up the data splits and sampling methods described above. The following code took care of this:</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">over_sampler</span> <span class="o">=</span> <span class="n">over_sampling</span><span class="o">.</span><span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">under_sampler</span> <span class="o">=</span> <span class="n">under_sampling</span><span class="o">.</span><span class="n">NearMiss</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">over_sampler</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">under_sampler</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># Create data frame for GLM model (X_train &amp; y_train in same data frame)</span>
<span class="n">data_glm</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">data_glm</span><span class="p">[</span><span class="s1">&#39;stroke&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train</span>
</code></pre></div><p><figure style="flex-grow: 256; flex-basis: 616px">
		<a href="/p/strokeprediction/img/GLM_data.jpg" data-size="717x279"><img src="/p/strokeprediction/img/GLM_data.jpg"
				
				width="717"
				height="279"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>As we can see, by oversampling we were able to bring up the minority class observations to approximately 2500, which gave us our 4:3 desired ratio (again, no real reason why I chose this but let&rsquo;s just go with it). Testing data has also been reduced down to the desired 3:4 ratio. Now that we have our data formatted correctly, we can begin to build our GLM:</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">response_var</span> <span class="o">=</span> <span class="s1">&#39;stroke ~ &#39;</span> <span class="c1"># Fit with all variables</span>
<span class="n">explanatory_vars</span> <span class="o">=</span> <span class="s1">&#39; + &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">formula</span> <span class="o">=</span> <span class="n">response_var</span> <span class="o">+</span> <span class="n">explanatory_vars</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="o">.</span><span class="n">from_formula</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(),</span> <span class="n">data</span><span class="o">=</span><span class="n">data_glm</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div><p><figure style="flex-grow: 72; flex-basis: 173px">
		<a href="/p/strokeprediction/img/GLM_init.jpg" data-size="429x593"><img src="/p/strokeprediction/img/GLM_init.jpg"
				
				width="429"
				height="593"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>From our initial model, we can see that almost all features are statistically significant (except for <em>over_weight</em> which was a feature I had previously created from <em>bmi</em>). We can now use the above model to predict on the test data, and we get the following initial (base model) results:</p>
<p><figure style="flex-grow: 111; flex-basis: 268px">
		<a href="/p/strokeprediction/img/GLM_result_init.jpg" data-size="327x292"><img src="/p/strokeprediction/img/GLM_result_init.jpg"
				
				width="327"
				height="292"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Okay, so not bad. But I&rsquo;m pretty confident that we can improve these results. How you may ask? Unlike machine learning models, we can use Grid or Random Search with a bunch of hyperparameters. Statistical models require a little more finesse. And by finesse I mean stepwise selection to find the best combination of these features. I&rsquo;ll evaluate these models based on their BIC, Recall, and F-Beta score (beta=0.85 provided the best results).</p>
<blockquote>
<p><strong>NOTE</strong>: All 3 stepwise methods resulted in the exact same model, with the same coefficient &amp; p-values. So forward selection will explain in a bit more detail, and then exhaustive/backwards will just be a quick overview of the code and idea behind them.</p>
</blockquote>
<h3 id="foward-stepwise-feature-selection">Foward Stepwise Feature Selection</h3>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="k">def</span> <span class="nf">fitModel</span><span class="p">(</span><span class="n">feature_subset</span><span class="p">):</span>
    <span class="n">response_var</span> <span class="o">=</span> <span class="s1">&#39;stroke ~ &#39;</span>
    <span class="n">explanatory_vars</span> <span class="o">=</span> <span class="s1">&#39; + &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">feature_subset</span><span class="p">)]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">formula</span> <span class="o">=</span> <span class="n">response_var</span> <span class="o">+</span> <span class="n">explanatory_vars</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="o">.</span><span class="n">from_formula</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(),</span> <span class="n">data</span><span class="o">=</span><span class="n">data_glm</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">y_preds</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">feature_subset</span><span class="p">)]))</span>
    <span class="n">model_recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">)</span>
    <span class="n">model_fbeta</span> <span class="o">=</span> <span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.85</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">result</span><span class="p">,</span> <span class="s1">&#39;bic&#39;</span><span class="p">:</span> <span class="n">result</span><span class="o">.</span><span class="n">bic</span><span class="p">,</span> <span class="s1">&#39;recall&#39;</span><span class="p">:</span> <span class="n">model_recall</span><span class="p">,</span> 
            <span class="s1">&#39;fbeta&#39;</span><span class="p">:</span> <span class="n">model_fbeta</span><span class="p">,</span> 
            <span class="s1">&#39;features&#39;</span> <span class="p">:</span> <span class="n">X_train</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">feature_subset</span><span class="p">)]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">}</span>
            
<span class="k">def</span> <span class="nf">forwardSelection</span><span class="p">(</span><span class="n">predictors</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">remaining_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">predictors</span><span class="p">]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">remaining_predictors</span><span class="p">:</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fitModel</span><span class="p">(</span><span class="n">predictors</span><span class="o">+</span><span class="p">[</span><span class="n">p</span><span class="p">]))</span>
    
    <span class="n">models</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="n">best_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;fbeta&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>

    <span class="k">return</span> <span class="n">best_model</span><span class="o">.</span><span class="n">values</span>
</code></pre></div><p>Okay so a good amount of code just thrown at you there. The general idea for <strong>forward selection</strong> is that we iterate over all of the features in the training data set with each iteration adding the coefficient that improves our model the best. For each iteration, we try all features but only return a single model that had the highest F-Beta score, and append this to our list of final models. In total, we will have 10 models (given that we have 10 features) which we then plot and choose a final model from.</p>
<p><figure style="flex-grow: 349; flex-basis: 838px">
		<a href="/p/strokeprediction/img/stepwise.jpg" data-size="790x226"><img src="/p/strokeprediction/img/stepwise.jpg"
				
				width="790"
				height="226"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>From the above graphs, we can see that the model using 4 features resulted in the highest Recall &amp; F-Beta score (BIC is in the middle, but this model gave the best results). To save space I&rsquo;ll put the results from the model summary and confusion matrix for the test set in the final model section since all 3 stepwise methods resulted in the same model/scores.</p>
<h3 id="exhaustive-stepwise-feature-selection">Exhaustive Stepwise Feature Selection</h3>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="k">def</span> <span class="nf">exhaustiveSearch</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">combo</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fitModel</span><span class="p">(</span><span class="n">combo</span><span class="p">))</span>
    
    <span class="n">models</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="n">best_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;fbeta&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>

    <span class="k">return</span> <span class="n">best_model</span><span class="o">.</span><span class="n">values</span>
</code></pre></div><p>The general idea for <strong>exhaustive selection</strong> is that we try every combinations of predictors given a certain length (in this case, 1 through 10) and return the best model. For example, in step 2 we try every combination of 2 features and so on. <em>Note: forward search took around 1 second for all 10 steps, where exhaustive search took around 25 seconds - for a much larger model this would need to be considered before using.</em></p>
<h3 id="backwards-stepwise-feature-selection">Backwards Stepwise Feature Selection</h3>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">predictors</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">combo</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictors</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fitModel</span><span class="p">(</span><span class="n">combo</span><span class="p">))</span>
    
    <span class="n">models</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="n">best_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;fbeta&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>

    <span class="k">return</span> <span class="n">best_model</span><span class="o">.</span><span class="n">values</span>
</code></pre></div><p>The final method is <strong>backwards selection</strong>, which will begin with all of our features in the data set, and each iteration it will drop the least significant coefficient. So we start with 10 features, and work our way down to 1 at the end.</p>
<h3 id="final-statistical-model">Final Statistical Model</h3>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">models_bwd_flipped</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="s1">&#39;Model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span> <span class="c1"># final model</span>
</code></pre></div><p><figure style="flex-grow: 93; flex-basis: 223px">
		<a href="/p/strokeprediction/img/GLM_final.jpg" data-size="413x444"><img src="/p/strokeprediction/img/GLM_final.jpg"
				
				width="413"
				height="444"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Our final model! Although I&rsquo;m showing the model from the Backward selection, both the forward/exhaustive models also had the same 4 features as their top performer (so just think of the above as the general winner for each method). All coefficients are now statistically significant, and to my surprise some of what I thought were key features have been dropped. <em>Hypertension, heart disease, smoking,</em> and <em>average glucose level</em> are key indicators for stroke but they were all dropped? Blows my mind, but our model seemed to perform much better without them and the results below show that:</p>
<p><figure style="flex-grow: 109; flex-basis: 261px">
		<a href="/p/strokeprediction/img/GLM_result_final.jpg" data-size="312x286"><img src="/p/strokeprediction/img/GLM_result_final.jpg"
				
				width="312"
				height="286"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Okay, so what improvements do we have over the base model:</p>
<ul>
<li>True Negatives increased by 2, while False Positives decreased by 2 (less people classified as stroke that did not have a stroke). </li>
<li>False Negative decreased by 21, while True Positives increased by 21 (more people classified as stroke that actually had a stroke). </li>
<li>Precision increased from 57% to 66%.</li>
<li>Recall increased from 58% to 82% (this was the most important evaluation metric to improve).</li>
<li>Accuracy increased from 63% to 74%.</li>
</ul>
<p><em>Note: the above came from the classification report, see either the README.md or stroke_prediction.ipynb</em>.</p>
<p>A huge improvement! The changes marked by the  were our main metrics to improve and we successfully did. There&rsquo;s still room to improve (False Positives could be lowered a bit but this was a trade off to increase Recall) but this is a strong model. However, will it be strong enough to stay champion? Let&rsquo;s move onto the ensemble modeling and see.</p>
<h2 id="ensemble-modeling">Ensemble Modeling</h2>
<p>For this section I initially planned on using the Scikit-Learn library, but after some research I found another one that seemed to work a bit better for my problem: <a class="link" href="https://imbalanced-learn.org/stable/"  target="_blank" rel="noopener"
    >ImBalanced-Learn</a>. This library is build on Scikit-Learn put is designed mainly for classification with imbalanced classes. 3 initial models will be fit, cross-validated on all of the data and had the following scores:</p>
<p><figure style="flex-grow: 104; flex-basis: 250px">
		<a href="/p/strokeprediction/img/ML_init_models.jpg" data-size="790x758"><img src="/p/strokeprediction/img/ML_init_models.jpg"
				
				width="790"
				height="758"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<ul>
<li><a class="link" href="https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html"  target="_blank" rel="noopener"
    >BalancedRandomForestClassifier</a> - A balanced random forest classifier that under-samples each bootstrap sample to balance our classes.</li>
<li><a class="link" href="https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.RUSBoostClassifier.html"  target="_blank" rel="noopener"
    >RUSBoostClassifier</a> - An AdaBoost algorithm that performs random under-sampling at each iteration to balance classes.</li>
<li><a class="link" href="https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedBaggingClassifier.html"  target="_blank" rel="noopener"
    >BalancedBaggingClassifier</a> - A bagging classifier that implements data balancing at fit time.</li>
</ul>
<p>This may be a surprise, but the model chosen to continue with for hyperparameter tuning is the <strong>BalancedRandomForestClassifier</strong>. Although it had the lowest accuracy, precision, and F-Beta score it also had the highest recall (which is what we want). I believe with changes to our data set and model parameters, we will be able to improve these results.</p>
<h3 id="data-set-up">Data Set Up</h3>
<p>Now that we have our initial model set up, we will want to create the training and testing splits. Unlike the previous section, we will want to keep the training set imbalanced but balance the testing set to have the same ratio used to evaluate the GLM, which is 4:3.</p>
<blockquote>
<p><em>NOTE: I now wonder if these results differed since we used different testing sets. The Ensemble Model testing set was smaller and had different observations, so this could be affecting the results.</em></p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="k">def</span> <span class="nf">sampled_data_split</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">stroke_obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">stroke</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">no_stroke_obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">stroke</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>

    <span class="n">sample_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">stroke_obs</span><span class="p">))</span>
    <span class="n">stroke_sample</span> <span class="o">=</span> <span class="n">stroke_obs</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">sample_size</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">stroke_extra</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">stroke_obs</span><span class="p">,</span> <span class="n">stroke_sample</span><span class="p">])</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">stroke_obs</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">symmetric_difference</span><span class="p">(</span><span class="n">stroke_sample</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span>
    <span class="n">sample_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stroke_sample</span><span class="p">)</span><span class="o">*</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="n">no_stroke_sample</span> <span class="o">=</span> <span class="n">no_stroke_obs</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">sample_size</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">no_stroke_extra</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">no_stroke_obs</span><span class="p">,</span>
                                 <span class="n">no_stroke_sample</span><span class="p">])</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">no_stroke_obs</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">symmetric_difference</span><span class="p">(</span><span class="n">no_stroke_sample</span><span class="o">.</span><span class="n">index</span><span class="p">)]</span>

    <span class="n">train_set</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">stroke_extra</span><span class="p">,</span> <span class="n">no_stroke_extra</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">test_set</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">stroke_sample</span><span class="p">,</span> <span class="n">no_stroke_sample</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_set</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;stroke&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">train_set</span><span class="o">.</span><span class="n">stroke</span>
    <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_set</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;stroke&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_set</span><span class="o">.</span><span class="n">stroke</span>
    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span>
</code></pre></div><p><figure style="flex-grow: 262; flex-basis: 630px">
		<a href="/p/strokeprediction/img/ML_data.jpg" data-size="725x276"><img src="/p/strokeprediction/img/ML_data.jpg"
				
				width="725"
				height="276"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<h3 id="base-model">Base Model</h3>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">model</span> <span class="o">=</span> <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><p><figure style="flex-grow: 115; flex-basis: 276px">
		<a href="/p/strokeprediction/img/ML_results_init.jpg" data-size="320x278"><img src="/p/strokeprediction/img/ML_results_init.jpg"
				
				width="320"
				height="278"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<ul>
<li>Accuracy: 81%</li>
<li>Precision: 71%</li>
<li>Recall: 88%</li>
</ul>
<p>Okay, so already our model seems to be performing better than the GLM. This isn&rsquo;t a huge surprise, I expected this to happen for the most part. But now that we have our base model, let&rsquo;s see if we can increase these scores.</p>
<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<p><em>SPOILER</em>: The base model performed better than any of the tuned models below. Just in case you wanted to skip over the section (I don&rsquo;t blame you, I probably wouldn&rsquo;t make it through half this article with my attention span) the results seemed to slightly decrease with the tuned models. Two different methods were used in an attempt to improve the model: <em>GridSearchCV</em> and <em>Exhaustive Feature Selection</em>.</p>
<h4 id="gridsearchcv">GridSearchCV</h4>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">125</span><span class="p">],</span>
              <span class="s1">&#39;criterion&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">],</span>
              <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;log2&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
              <span class="s1">&#39;sampling_strategy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
              <span class="s1">&#39;class_weight&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span> <span class="s1">&#39;balanced_subsample&#39;</span><span class="p">]}</span>

<span class="n">scoring_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">precision_score</span><span class="p">),</span> 
                  <span class="s1">&#39;recall&#39;</span><span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">recall_score</span><span class="p">),</span> 
                  <span class="s1">&#39;fbeta&#39;</span><span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)}</span>

<span class="n">hpt_model</span> <span class="o">=</span> <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rs_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">hpt_model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring_params</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="s1">&#39;fbeta&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">rs_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="c1"># {&#39;class_weight&#39;: &#39;balanced&#39;,  </span>
<span class="c1">#  &#39;criterion&#39;: &#39;gini&#39;,  </span>
<span class="c1">#  &#39;max_features&#39;: &#39;auto&#39;,  </span>
<span class="c1">#  &#39;n_estimators&#39;: 25,  </span>
<span class="c1">#  &#39;sampling_strategy&#39;: &#39;auto&#39;}</span>
</code></pre></div><p>Using the above parameters, a new model was fit and used to predict on the data. To save room I won&rsquo;t include the confusion matrix or classification report, but the main take away is that True Negatives decreased by 1 and False Positives increased by 1 (slightly worse than base model). However, the model increased it&rsquo;s prediction speed by 2x which could be considered if that was the goal for the product (slightly worse results for much faster performance).</p>
<h4 id="exhaustive-feature-selection">Exhaustive Feature Selection</h4>
<p><em>NOTE: Here we used the model from above to try and improve the results since it had similar results as the base model but faster performance).</em></p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">efs</span> <span class="o">=</span> <span class="n">EFS</span><span class="p">(</span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span>
          <span class="n">min_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">max_features</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
          <span class="n">scoring</span><span class="o">=</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.25</span><span class="p">),</span>
          <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
          <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">efs</span> <span class="o">=</span> <span class="n">efs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">list</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="nb">list</span><span class="p">(</span><span class="n">efs</span><span class="o">.</span><span class="n">best_idx_</span><span class="p">)]</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="c1"># [&#39;age&#39;, &#39;hypertension&#39;, &#39;avg_glucose_level&#39;, &#39;bmi&#39;, &#39;smokes&#39;, &#39;never_smoked&#39;]</span>
</code></pre></div><p>Again, a new model was fit using only the above features but this resulted in even worse performance. Our True Negatives decreased by 1 and False Positives increased by 1, as well as False Negatives increasing by 2 and True Positives decreasing by 2. From this, our base model seems to have the best performance and will be our final model.</p>
<h3 id="final-model">Final Model</h3>
<p>Okay, so this was one of those cases where hyperparameter tuning didn&rsquo;t seem to benefit our model very much (actually it seemed to hurt our model more than anything). Our base model seemed to perform the best out of all 3 models, and the features had the following importance:</p>
<p><figure style="flex-grow: 107; flex-basis: 258px">
		<a href="/p/strokeprediction/img/ML_feat_import.jpg" data-size="457x424"><img src="/p/strokeprediction/img/ML_feat_import.jpg"
				
				width="457"
				height="424"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>An important note is that the <em>age</em>, <em>bmi</em>, and <em>avg_glucose_levels</em> were not normalized and all other features had discrete values in the range [0,1], but normalizing the inputs resulted in the same feature importance but with worse performance.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Comparing the statistical and ensemble models, we can see that the ensemble model seems to be performing better (although not be a large margin). We have the following difference based on the final models for both types:</p>
<ul>
<li><strong>Accuracy</strong>: Ensemble model has a 7% advantage.</li>
<li><strong>Precision</strong>: Ensemble model has a 5% advantage.</li>
<li><strong>Recall</strong>: Ensemble model has a 6% advantage.</li>
</ul>
<p>An important note is that the two model types were trained and tested on different data. The statistical model had oversampling of the minority class for the training data in order to balance it since we could not handle the imbalance within the model itself (but the testing data was undersampled). The ensemble model had regular training data (not balanced) but the testing data was undersampled and slightly smaller (extra observations left over from the undersampling were also added back into training since imbalanced data was not a concern).</p>
<p>Overall, the ensemble model seems to be slightly stronger in all evaluation aspects. The computational speeds were similar for both models. Places to improve upon for our models would to be to try and get the data splits to be more similar in order to have more validity to our statement of the stronger model.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/python/">Python</a>
        
            <a href="/tags/statistics/">Statistics</a>
        
            <a href="/tags/machine-learning/">Machine Learning</a>
        
    </section>


    </footer>

    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">Related contents</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="has-image">
    <a href="/p/prodvspop/">
        
        
            <div class="article-image">
                <img src="/p/prodvspop/img/banner.44fe4c762e3c9a637a486dde25ff5ab3_hu2fba2ac787967c0d994b01819822a76b_1268585_250x150_fill_box_smart1_2.png" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="prodvspop" 
                        data-hash="md5-RP5Mdi48mmN6SG3eJf9asw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Global Food Production vs. Population: Predicting the Future</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/braintumor/">
        
        
            <div class="article-image">
                <img src="/p/braintumor/images/banner.53ef3a151262bedf49f76482506b45e4_hu9eb6ca4c1784edf324c9c2921bdfab7c_1375483_250x150_fill_q75_box_smart1.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="braintumor" 
                        data-hash="md5-U&#43;86FRJivt9J92SCUGtF5A==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Brain Tumor Detection using SageMaker &amp; TensorFlow</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/gpuscraping/">
        
        
            <div class="article-image">
                <img src="/p/gpuscraping/img/banner.f92c3686a581e36a20d48dbd8254f38f_huda43a43d3a89ed86863926de204a0975_2317027_250x150_fill_box_smart1_2.png" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="gpuscraping" 
                        data-hash="md5-&#43;Sw2hqWB42og1I29glTzjw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Web Scraping GPU Information with Rvest</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>


    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2021 Derek Helms
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="2.3.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >
            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
