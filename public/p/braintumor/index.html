<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" href="/images/favicon.ico">
    
    <link rel="stylesheet" href="/scss/global.min.a88601b4c09a5d50233a8fb74cd2fec02c95c7ae004f6e6ba7b11d524a384d92.css">
    
    <link rel="stylesheet" href="/css/prism.css" />
    <link href="https://fonts.googleapis.com/css?family=Merriweather&display=swap" rel="stylesheet">
    

 






	




<title>Brain Tumor Detection using SageMaker &amp; TensorFlow | Derek Helms</title>
<meta name="description" content="Original GitHub Repository
Introduction MRI scans are one of the main tools used for analyzing tumors in the human brain. Huge amounts of image data are generated through these scans, which need to be examined by a radiologist, and can be susceptible to diagnosis error due to complex MRI scans. This is where the application of neural networks come in. Through Convolutional Neural Networks, we are able to process these scans in order to extract low level features that can help us correctly classifying and diagnose brain tumors.">
<meta property="og:title" content="Brain Tumor Detection using SageMaker &amp; TensorFlow | Derek Helms">
<meta property="og:site_name" content="Derek Helms">
<meta property="og:description" content="Original GitHub Repository
Introduction MRI scans are one of the main tools used for analyzing tumors in the human brain. Huge amounts of image data are generated through these scans, which need to be examined by a radiologist, and can be susceptible to diagnosis error due to complex MRI scans. This is where the application of neural networks come in. Through Convolutional Neural Networks, we are able to process these scans in order to extract low level features that can help us correctly classifying and diagnose brain tumors.">
<meta property="og:url" content="https://derekhelms.netlify.app/p/braintumor/">
<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:image" content='https://derekhelms.netlify.app/images/braintumor_img/header.jpg'><meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Brain Tumor Detection using SageMaker &amp; TensorFlow | Derek Helms">

	<link rel="canonical" href="https://derekhelms.netlify.app/p/braintumor/">


	<meta name="twitter:description" content="Original GitHub Repository
Introduction MRI scans are one of the main tools used for analyzing tumors in the human brain. Huge amounts of image data are generated through these scans, which need to be examined by a radiologist, and can be susceptible to diagnosis error due to complex MRI scans. This is where the application of neural networks come in. Through Convolutional Neural Networks, we are able to process these scans in order to extract low level features that can help us correctly classifying and diagnose brain tumors.">
<meta name="twitter:image" content="https://derekhelms.netlify.app/images/braintumor_img/header.jpg">
<meta property="article:published_time" content="2021-05-10T00:00:00&#43;00:00">
	<meta property="article:updated_time" content="2021-05-10T00:00:00&#43;00:00">



    </head>


<body class="line-numbers">

    
    <script src="/js/initColors.js"></script>

    <div class="layout-styled">

        <Section class="section">
  <div class="nav-container">
    <a class="logo-link" href="/">
      <svg 
xmlns="http://www.w3.org/2000/svg" 
width="28" 
height="28" 
fill="none" 
class="bi bi-house-door" 
viewBox="0 0 15 15">
  <path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293L8.354 1.146zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4H2.5z"
  fill="#73737D"/>
</svg>
      <span class="header-hidden">Navigate back to the homepage</span>
    </a>
    <div class="nav-controls">
      
      <button style="margin:35px;">
        
            <a href="https://github.com/dhelms1" target="_blank" rel="noopener noreferrer"><svg
class="social-icon-image"
width="28"
height="28"
viewBox="0 0 14 14"
fill="none"
xmlns="http://www.w3.org/2000/svg"
>
<path
  fillRule="evenodd"
  clipRule="evenodd"
  d="M7 0C3.1325 0 0 3.21173 0 7.17706C0 10.3529 2.00375 13.0353 4.78625 13.9863C5.13625 14.0491 5.2675 13.8338 5.2675 13.6454C5.2675 13.4749 5.25875 12.9097 5.25875 12.3087C3.5 12.6406 3.045 11.8691 2.905 11.4653C2.82625 11.259 2.485 10.622 2.1875 10.4516C1.9425 10.317 1.5925 9.98508 2.17875 9.97611C2.73 9.96714 3.12375 10.4964 3.255 10.7118C3.885 11.7973 4.89125 11.4923 5.29375 11.3039C5.355 10.8374 5.53875 10.5234 5.74 10.3439C4.1825 10.1645 2.555 9.54549 2.555 6.80026C2.555 6.01976 2.82625 5.37382 3.2725 4.87143C3.2025 4.692 2.9575 3.95635 3.3425 2.96951C3.3425 2.96951 3.92875 2.78111 5.2675 3.70516C5.8275 3.54367 6.4225 3.46293 7.0175 3.46293C7.6125 3.46293 8.2075 3.54367 8.7675 3.70516C10.1063 2.77214 10.6925 2.96951 10.6925 2.96951C11.0775 3.95635 10.8325 4.692 10.7625 4.87143C11.2087 5.37382 11.48 6.01079 11.48 6.80026C11.48 9.55446 9.84375 10.1645 8.28625 10.3439C8.54 10.5682 8.75875 10.9988 8.75875 11.6717C8.75875 12.6316 8.75 13.4032 8.75 13.6454C8.75 13.8338 8.88125 14.0581 9.23125 13.9863C11.9963 13.0353 14 10.3439 14 7.17706C14 3.21173 10.8675 0 7 0Z"
  fill="#73737D"
/>
</svg></a>
      </button>
      
      <button>
        
            <a href="https://www.linkedin.com/in/derek-helms" target="_blank" rel="noopener noreferrer"><svg
class="social-icon-image"
width="28"
height="28"
viewBox="0 0 14 14"
fill="none"
xmlns="http://www.w3.org/2000/svg"
{...props}
>
<path
  fillRule="evenodd"
  clipRule="evenodd"
  d="M3.59615 13.125H0.871552V4.36523H3.59615V13.125ZM2.24847 3.16406C1.81878 3.16406 1.44769 3.00781 1.13519 2.69531C0.822692 2.38281 0.666443 2.01171 0.666443 1.58203C0.666443 1.15234 0.822692 0.781248 1.13519 0.468749C1.44769 0.156249 1.81878 0 2.24847 0C2.67816 0 3.04925 0.156249 3.36175 0.468749C3.67425 0.781248 3.8305 1.15234 3.8305 1.58203C3.8305 2.01171 3.67425 2.38281 3.36175 2.69531C3.04925 3.00781 2.67816 3.16406 2.24847 3.16406ZM13.7915 13.125H11.0669V8.84765C11.0669 8.14452 11.0083 7.63671 10.8911 7.32421C10.6763 6.79687 10.2563 6.5332 9.63134 6.5332C9.00634 6.5332 8.56689 6.76757 8.31298 7.23632C8.11767 7.58788 8.02001 8.10546 8.02001 8.78905V13.125H5.32471V4.36523H7.93212V5.5664H7.96142C8.15673 5.17578 8.46923 4.85351 8.89892 4.59961C9.36767 4.28711 9.91454 4.13086 10.5395 4.13086C11.8091 4.13086 12.6977 4.53125 13.2055 5.33203C13.5962 5.97656 13.7915 6.97265 13.7915 8.3203V13.125Z"
  fill="#73737D"
/>
</svg></a>
      </button>
      
      <button id="themeColorButton" class="icon-wrapper"> 
        <div id="sunRays" class="sun-rays"></div>
        <div id="moonOrSun" class="moon-or-sun"></div>
        <div id="moonMask" class="moon-mask"></div>
      </button>
      
    </div>
</div>
</Section>


<script src="/js/toggleLogos.js"></script>


<script src="/js/toggleColors.js"></script>


<script src="/js/copyUrl.js"></script>

        

<section class="section narrow">

    <section id="articleHero" class="section narrow">
    <div class="article-hero">
        <header class="article-header">
            <h1 class="article-hero-heading">Brain Tumor Detection using SageMaker &amp; TensorFlow</h1>
            <div class="article-hero-subtitle">
                <div class="article-meta">
                    


    
            <a href="/authors/hugo-authors/" class="article-author-link">
                
                    <div class="article-author-avatar">
                        <img src="/images/avatar.png" />
                    </div>
                
                
                <strong>Derek Helms</strong>
                
                <span class="hide-on-mobile">,&nbsp;</span>
            </a>
    



<script src="/js/collapseAuthors.js"></script>
                    May 10, 2021
                    â€¢ 8 min read
                </div>
            </div>
        </header>
        
        <div class="article-hero-image" id="ArticleImage__Hero">
            <img src="/images/braintumor_img/header.jpg">
        </div>
        
    </div>
</section>


    <aside id="progressBar" class="aside-container">
    <div class="aside-align">
      <div>
        <div class="overlap-container">
        </div>
      </div>
    </div>

    <div class="progress-container" tabIndex={-1}>
        <div class="track-line" aria-hidden="true">
            <div id="progressIndicator" class="progress-line"></div>
        </div>
    </div>
</aside>


    <article  id="articleContent" class="post-content" style="position:relative;">
        <p><a href="https://github.com/dhelms1/brain_tumor"><strong>Original GitHub Repository</strong></a></p>
<h1 id="introduction">Introduction</h1>
<p>MRI scans are one of the main tools used for analyzing tumors in the human brain. Huge amounts of image data are generated through these scans, which need to be examined by a radiologist, and can be susceptible to diagnosis error due to complex MRI scans. This is where the application of neural networks come in. Through Convolutional Neural Networks, we are able to process these scans in order to extract low level features that can help us correctly classifying and diagnose brain tumors. The purpose of this project is to deploy a deep learning model using Amazon SageMaker that can accurately classify MRI scans of brain tumors into four different categories:</p>
<ul>
<li><strong>Glioma</strong> - a tumor made of astrocytes that occurs in the brain and spinal cord.</li>
<li><strong>Meningioma</strong> - a usually noncancerous tumor that arises from membranes surrounding the brain &amp; spinal cord.</li>
<li><strong>None</strong> - no tumor present in brain.</li>
<li><strong>Pituitary</strong> - a tumor that forms in the pituitary gland near the brain that can change hormone levels.</li>
</ul>
<h1 id="acquiring--processing-the-data">Acquiring &amp; Processing the data</h1>
<p>The first step in this project was acquiring the images necessary for training and testing, which originally came from <a href="https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri"><strong>Kaggle</strong></a> but was cloned from the original <a href="https://github.com/sartajbhuvaji/brain-tumor-classification-dataset"><strong>GitHub Repository</strong></a> hosted by <em>Sartaj Bhuvaji</em>. Initially, 2870 training images and 394 testing images were present in the data set. The training images were further split into training/validation sets, with an 80/20 split resulting in 2296 training and 574 validation images.</p>
<p><em>NOTE: I now question whether randomly flipping up/down was appropriate for medical imaging (since most will be inputted the correct way), but it did not seem to negatively effect the results so I left it in. Going forward, possibly avoiding augmenting the images in a way that is very uncommon could be more beneficial.</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">train_dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((X_train, y_train))
val_dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices((X_val, y_val))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cast</span>(image, label):
    image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>convert_to_tensor(image, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>uint8) <span style="color:#75715e"># convert to tensor</span>
    <span style="color:#66d9ef">return</span> image, label

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_augment</span>(image, label):
    seed <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">150</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)[<span style="color:#ae81ff">0</span>]
    image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>convert_to_tensor(image, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>uint8) <span style="color:#75715e"># convert to tensor</span>
    image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>random_flip_up_down(image, seed<span style="color:#f92672">=</span>seed) <span style="color:#75715e"># random flip</span>
    image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>random_flip_left_right(image, seed<span style="color:#f92672">=</span>seed) <span style="color:#75715e"># random flip</span>
    <span style="color:#66d9ef">return</span> image, label
    
train_dataset <span style="color:#f92672">=</span> (
    train_dataset
    <span style="color:#f92672">.</span>map(train_augment, num_parallel_calls<span style="color:#f92672">=</span>AUTOTUNE)
    <span style="color:#f92672">.</span>prefetch(AUTOTUNE)
    <span style="color:#f92672">.</span>shuffle(buffer_size<span style="color:#f92672">=</span>len(X_train))
)

val_dataset <span style="color:#f92672">=</span> (
    val_dataset
    <span style="color:#f92672">.</span>map(cast, num_parallel_calls<span style="color:#f92672">=</span>AUTOTUNE)
    <span style="color:#f92672">.</span>prefetch(AUTOTUNE)
)
</code></pre></div><p><img src="/images/braintumor_img/train_img.jpg" alt=""></p>
<p>Although these data sets came separated into subdirectories, which could have been uploaded directly to S3 and read using the ImageDataGenerator function in TensorFlow, I decided to convert them into TensorFlow Datasets and then into TFRecord Files. The training images were also augmented using rotations and initially, normalization. Little did I know how much of an issue this one step would become during training&hellip;</p>
<h3 id="the-struggle-of-never-reading">The Struggle of Never Reading</h3>
<p>Side note, if there&rsquo;s one thing this project taught me it was to read the documentation. I had chosen the EfficientNet architecture for the model, which I now know handles normalization within the architecture, but I had skipped reading this part. 2 days and countless failed training runs later, and I still couldn&rsquo;t figure out why my images (which were currently being normalized) were resulting in 99% train accuracy and 10% validation accuracy. That is, until I opened the <a href="https://keras.io/api/applications/efficientnet/"><strong>EfficientNet</strong></a> documentation and saw the fourth line:</p>
<p>&ldquo;EfficientNet models expect their inputs to be float tensors of pixels with values in the [0-255] range.&rdquo;&quot;</p>
<h1 id="heading"></h1>
<p><img src="/images/braintumor_img/fine_meme.jpg" alt=""></p>
<p>I don&rsquo;t know if anything else could describe the feeling when I saw that line. Hours of downloading and importing the project into Google Colab to step through the training process&hellip; Hours of reformatting my TFRecord files thinking I was mislabeling my data&hellip; Hours of reading Medium articles on SageMaker and TensorFlow training setups&hellip; But it&rsquo;s totally fine. At least my file size was reduced a lot from removing the normalization so hey, one win for me I guess.</p>
<h1 id="modeling">Modeling</h1>
<p>Alright, enough with my rant. Now we can finally get to the good part, creating the TensorFlow model. As stated previously, the EfficientNet architecture was chosen for the model, more specifically the B0 architecture. Looking into the <em>scripts/model.py</em> directory we can see that the output was replaced with a new Dense layer to handle our 4 classes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#f92672">from</span> tensorflow.keras.applications <span style="color:#f92672">import</span> EfficientNetB0
<span style="color:#f92672">from</span> tensorflow.keras <span style="color:#f92672">import</span> layers, models
<span style="color:#f92672">from</span> tensorflow.keras.optimizers <span style="color:#f92672">import</span> Adam
IMG_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">224</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">EfficientNetClassifier</span>():
    effnetb0 <span style="color:#f92672">=</span> EfficientNetB0(weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;imagenet&#39;</span>, include_top<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, input_shape<span style="color:#f92672">=</span>(IMG_SIZE, IMG_SIZE, <span style="color:#ae81ff">3</span>))
    model <span style="color:#f92672">=</span> layers<span style="color:#f92672">.</span>GlobalAveragePooling2D()(effnetb0<span style="color:#f92672">.</span>output)
    model <span style="color:#f92672">=</span> layers<span style="color:#f92672">.</span>Dropout(<span style="color:#ae81ff">0.3</span>)(model)
    model <span style="color:#f92672">=</span> layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">4</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(model)
    model <span style="color:#f92672">=</span> models<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>effnetb0<span style="color:#f92672">.</span>input, outputs<span style="color:#f92672">=</span>model)
    model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
    <span style="color:#66d9ef">return</span> model
</code></pre></div><p>Class weights were also computed and uploaded to the S3 bucket since there is a slight difference in the no_tumor class compared to the other three and we need the model to learn equally from each class. They are as follows:</p>
<table>
<thead>
<tr>
<th>Class</th>
<th>Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>Glioma</td>
<td>0.86445783</td>
</tr>
<tr>
<td>Meningioma</td>
<td>0.85928144</td>
</tr>
<tr>
<td>No Tumor</td>
<td>1.81072555</td>
</tr>
<tr>
<td>Pituitary</td>
<td>0.88717156</td>
</tr>
</tbody>
</table>
<p>Both Early Stopping and Learning Rate Reduction are implemented for the model, with learning rate being reduced 3 times and early stopping occurring at epoch 16 when validation loss plateaued.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">early_stop <span style="color:#f92672">=</span> EarlyStopping(monitor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;val_loss&#39;</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;min&#39;</span>, patience<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
lr_reduction <span style="color:#f92672">=</span> ReduceLROnPlateau(monitor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;val_loss&#39;</span>, patience <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, factor<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, min_lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.000001</span>)
</code></pre></div><h2 id="training">Training</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python">model<span style="color:#f92672">.</span>fit(x<span style="color:#f92672">=</span>train_dataset,
          epochs<span style="color:#f92672">=</span>epochs, 
          validation_data<span style="color:#f92672">=</span>val_dataset,
          verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
          class_weight<span style="color:#f92672">=</span>class_weights,
          callbacks<span style="color:#f92672">=</span>[early_stop, lr_reduction])
</code></pre></div><p>On epoch 1, the initial training accuracy was 85.1% with a validation accuracy of 62.37%. After epoch 5, 13, and 15 the learning rate was reduced from an initial value of 0.001 to a final value of 0.000008. Early stopping ended our models training after epoch 16, where the validation loss plateaued around 0.044. The final results from training are:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>0.0036</td>
<td>99.91%</td>
</tr>
<tr>
<td>Validation</td>
<td>0.0431</td>
<td>98.61%</td>
</tr>
</tbody>
</table>
<p>With such a high training accuracy, I would be skeptical that the model is overfitting. But since our validation accuracy is within 1.5% of the training accuracy, it leads me to think that the model is performing well. This will be either confirmed or denied in the testing results section depending on the accuracy of the model of predicting with new data. <em>Note: The final model is saved to the default S3 bucket, which will be loaded back into the main notebook and used for predicting.</em></p>
<h2 id="testing">Testing</h2>
<p>Testing images/labels (394 total) were loaded and saved into numpy arrays, which were then flattened and sent to the endpoint for predicting. The maximum probability from the predicted array was then taken and converted back into a string label corresponding to the true label.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_image</span>(image):
    image <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>expand_dims(image, <span style="color:#ae81ff">0</span>)
    input_data <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;instances&#39;</span>: np<span style="color:#f92672">.</span>asarray(image)}
    <span style="color:#66d9ef">return</span> input_data
  
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_test_data</span>(test_img):
    preds <span style="color:#f92672">=</span> []
    probs <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> img <span style="color:#f92672">in</span> tqdm<span style="color:#f92672">.</span>tqdm(test_img):
        input_data <span style="color:#f92672">=</span> process_image(img)
        y_pred <span style="color:#f92672">=</span> predictor<span style="color:#f92672">.</span>predict(input_data)
        probs<span style="color:#f92672">.</span>append(max(y_pred[<span style="color:#e6db74">&#39;predictions&#39;</span>][<span style="color:#ae81ff">0</span>]))
        preds<span style="color:#f92672">.</span>append(string_labels[np<span style="color:#f92672">.</span>argmax(y_pred[<span style="color:#e6db74">&#39;predictions&#39;</span>][<span style="color:#ae81ff">0</span>])])
    
    <span style="color:#66d9ef">return</span> preds, probs
</code></pre></div><p>With such a high training/validation accuracy, I expected the testing accuracy to have a similar result. (NOTE: refer to <em>test_results</em> directory to see the dataframe containing each test images predicted probability/label, true label, and if it was correct). However, the final results were:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Testing</td>
<td>73.86%</td>
</tr>
</tbody>
</table>
<p>With these results, I&rsquo;m skeptical that the model is indeed overfitting. With ~25% difference in training and testing accuracy, we may need to re-evaluate how we are augmenting images and ensure that there are no duplicates in the train/validation sets. However, further exploration and looking at other people&rsquo;s projects, we can see the test accuracy we achieved is in on the mid-high end (which is somewhat reassuring I think?).</p>
<h1 id="final-results">Final Results</h1>
<p>Due to the large difference in train/test accuracy, further exploration was needed on these incorrectly predicted observations. Looking at the testing accuracy for each class, as well as the probabilities for the incorrect predictions, we get the following:</p>
<p><img src="/images/braintumor_img/per_class.jpg" alt=""> <img src="/images/braintumor_img/wrong_probs.jpg" alt=""></p>
<p>Looking at the accuracy per class, we can see that <em>Meningioma</em> and <em>No Tumor</em> are performing close to 100% accuracy. However, <em>Glioma</em> and <em>Pituitary</em> are not performing very well at all, with around 30% and 65% accuracy (respectively). If we looked the the probabilities associated with these incorrect predictions, we can see that most are extremely confident in their predictions. This leads me to wonder if something within the dataset is causing the issues, especially with everyone else having the same issues from what I can tell.</p>
<p>Finally, looking at a confusion matrix, we can see where these mislabeled predictions are:</p>
<p><img src="/images/braintumor_img/conf_mat.jpg" alt=""></p>
<ul>
<li><strong>Glioma</strong> - it seems the majority of Glioma tumors are being classified as Meningioma (56), followed by the true label Glioma (26), followed by No Tumor (17), and finally Pituitary (1).</li>
<li><strong>Pituitary</strong> - It seems the majority of Pituitary tumors are being classified as the true label Pituitary (48), followed by No Tumor (14), and finally Meningioma (12).</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>From start to finish, this project was a learning experience that taught me more than I expected. As my first real project using TensorFlow, I feel there are areas that I could improve on (maybe going straight to TFRecord instead of the intermediate step of a TFDataset) but overall I&rsquo;m happy with my results. Getting a training/validation accuracy of over 99% was very nice, and although testing accuracy was much lower I have suspicions that it may be an issues with the labeling/images rather than overfitting (given other people&rsquo;s similar results). Overall, this project taught a lot of crucial skills that I will carry onto my future projects. But none more important than READ THE DOCUMENTATION.</p>
<h2 id="resources">Resources</h2>
<p>Below is a list of resources used throughout this project, including both documentation and articles that, help with ideas/formatting of my TensorFlow code and made the project possible:</p>
<ul>
<li><a href="https://towardsdatascience.com/train-a-tensorflow-model-in-amazon-sagemaker-e2df9b036a8">Train a TensorFlow Model in Amazon SageMaker</a> - Jun M.</li>
<li><a href="https://www.tensorflow.org/tutorials/load_data/tfrecord#walkthrough_reading_and_writing_image_data">TFRecord and Image Data Example</a> - TensorFlow Documentation</li>
<li><a href="https://towardsdatascience.com/how-to-train-an-image-classifier-on-tfrecord-files-97a98a6a1d7a">How to train an Image Classifier on TFRecord files</a> - Karan Sindwani</li>
<li><a href="https://towardsdatascience.com/working-with-tfrecords-and-tf-train-example-36d111b3ff4d">Working with TFRecords and tf.train.Example</a> - Cihan Soylu</li>
<li><a href="https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/">Image classification via fine-tuning with EfficientNet</a> - Keras API</li>
</ul>

    </article>


    





    
    
    
        
    




<section id="articleNext" class="section nartrow">
    <h3 class="footer-next-heading">More articles from Derek Helms</h3>
    <div class="footer-spacer"></div>
    <div class="next-articles-grid" numberOfArticles={numberOfArticles}>
        <div class="post-row">
            
                <a href="/p/strokeprediction/" class="article-link"
                 id="article-link-bigger">
                    <div>
                        <div class="image-container">
                            <img src="/images/strokeprediction_img/header.jpg" class="article-image" />
                        </div>
                        <div>
                            <h2 class="article-title">
                                Stroke Prediction: Battle of the Learning Methods
                            </h2>
                            <p class="article-excerpt">
                                In this project I compare the performance of Statistical and Machine Learning models for predicting whether or not a patient will have a stroke on a data set that has a large class imbalance.
                            </p>
                            <div class="article-metadata">
                                March 21, 2021 Â· 18 min read
                            </div>
                        </div>
                    </div>
                </a>
            
                <a href="/p/prodvspop/" class="article-link"
                >
                    <div>
                        <div class="image-container">
                            <img src="/images/prodvspop_img/banner.png" class="article-image" />
                        </div>
                        <div>
                            <h2 class="article-title">
                                Global Food Production vs. Population: Predicting the Future
                            </h2>
                            <p class="article-excerpt">
                                In this project I create a Polynomial Regression Model to estimate the necessary global food production to support a given population size. The main goal is estimating the 2050 production necessary to support our population, as this is the year eastimated to approach 10 billion people on Earth.
                            </p>
                            <div class="article-metadata">
                                January 26, 2021 Â· 8 min read
                            </div>
                        </div>
                    </div>
                </a>
            
        </div>
    </div>
</section>

</section>


 <script src="/js/progressBar.js"></script>

        
        <div class="footer-gradient"></div>
    <div class="section narrow">
      <div class="footer-hr"></div>
      <div class="footer-container">
        
    </div>
</div>

    </div>

    
    <script src="/js/prism.js"></script>
</body>

</html>