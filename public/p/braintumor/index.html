<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='In this project I use TensorFlow and Amazon SageMaker to build, train, and deploy a deep learning model that can accurately classify MRI scans of 4 different types of brain tumors.'><title>Brain Tumor Detection using SageMaker &amp; TensorFlow</title>

<link rel='canonical' href='/p/braintumor/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='Brain Tumor Detection using SageMaker &amp; TensorFlow'>
<meta property='og:description' content='In this project I use TensorFlow and Amazon SageMaker to build, train, and deploy a deep learning model that can accurately classify MRI scans of 4 different types of brain tumors.'>
<meta property='og:url' content='/p/braintumor/'>
<meta property='og:site_name' content='Derek Helms'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Python' /><meta property='article:tag' content='Deep Learning' /><meta property='article:tag' content='TensorFlow' /><meta property='article:published_time' content='2021-05-10T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2021-05-10T00:00:00&#43;00:00'/><meta property='og:image' content='/p/braintumor/images/banner.jpg' />
<meta name="twitter:title" content="Brain Tumor Detection using SageMaker &amp; TensorFlow">
<meta name="twitter:description" content="In this project I use TensorFlow and Amazon SageMaker to build, train, and deploy a deep learning model that can accurately classify MRI scans of 4 different types of brain tumors."><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='/p/braintumor/images/banner.jpg' /><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
    </head>
    <body class="">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "light");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.body.dataset.scheme = 'dark';
        } else {
            document.body.dataset.scheme = 'light';
        }
    })();
</script><div class="container main-container flex on-phone--column extended article-page with-toolbar">
            <aside class="sidebar left-sidebar sticky">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header class="site-info">
        
        <h2 class="site-description"></h2>
    </header>

    <ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/'>
                
                    <svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px"><path d="M 12 2.0996094 L 1 12 L 4 12 L 4 21 L 11 21 L 11 15 L 13 15 L 13 21 L 20 21 L 20 12 L 23 12 L 12 2.0996094 z M 12 4.7910156 L 18 10.191406 L 18 11 L 18 19 L 15 19 L 15 13 L 9 13 L 9 19 L 6 19 L 6 10.191406 L 12 4.7910156 z"/></svg>
                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about-myself/'>
                
                    <svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px"><path d="M 10 0 C 6.24 -0.02 2.97 3.1 3 7 L 3 8 L 3 11.748047 C 2.4701855 12.225307 2.0097656 13.000361 2.0097656 14.011719 C 2.0097656 15.27649 2.7280504 16.191398 3.4042969 16.585938 C 3.7657984 16.796847 3.8681582 16.757564 4.125 16.810547 C 5.6269646 19.927424 8.5402409 22.551214 11.867188 22.994141 L 12 23.011719 L 12.132812 22.994141 C 15.459499 22.550139 18.373013 19.926518 19.875 16.810547 C 20.131842 16.757567 20.234202 16.796847 20.595703 16.585938 C 21.27195 16.191398 21.990234 15.27649 21.990234 14.011719 C 21.990234 12.746948 21.27195 11.830087 20.595703 11.435547 C 20.297075 11.261319 20.232618 11.304211 20 11.248047 L 20 6 C 20 3.791 18.209 2 16 2 L 15 2 L 15 2.0097656 C 15 2.0097656 13.98 0 10 0 z M 12.445312 8 L 13 8 L 15 8 C 16.630274 8 17.949827 9.3018298 17.992188 10.921875 L 17.992188 13.013672 L 19.142578 13.013672 C 19.148378 13.014264 19.368026 13.035792 19.587891 13.164062 C 19.810644 13.294023 19.990234 13.37849 19.990234 14.011719 C 19.990234 14.644948 19.810644 14.727462 19.587891 14.857422 C 19.365137 14.987382 19.136719 15.009766 19.136719 15.009766 L 18.519531 15.044922 L 18.275391 15.611328 C 17.128688 18.256067 14.48056 20.569091 12 20.96875 C 9.5197032 20.569931 6.8713505 18.257043 5.7246094 15.611328 L 5.4804688 15.044922 L 4.8632812 15.009766 C 4.8632812 15.009766 4.6348629 14.987386 4.4121094 14.857422 C 4.1893558 14.727462 4.0097656 14.644948 4.0097656 14.011719 C 4.0097656 13.37849 4.1893558 13.294023 4.4121094 13.164062 C 4.6319738 13.035789 4.8516269 13.014264 4.8574219 13.013672 L 6.0078125 13.013672 L 6.0078125 10 L 9 10 C 10.477 10 11.752312 9.191 12.445312 8 z"/></svg>
                
                <span>About Myself</span>
            </a>
        </li>
        
        

        <li >
            <a href='https://www.linkedin.com/in/derek-helms'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
                
                <span>LinkedIn</span>
            </a>
        </li>
        
        

        <li >
            <a href='https://github.com/dhelms1'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                
                <span>GitHub</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives/'>
                
                    <?xml version="1.0"?><svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px">    <path d="M13.172,2H6C4.9,2,4,2.9,4,4v16c0,1.1,0.9,2,2,2h12c1.1,0,2-0.9,2-2V8.828c0-0.53-0.211-1.039-0.586-1.414l-4.828-4.828 C14.211,2.211,13.702,2,13.172,2z M18.5,9H13V3.5L18.5,9z"/></svg>
                
                <span>Archive</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search/'>
                
                    <?xml version="1.0"?><svg fill="#000000" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 24 24" width="24px" height="24px">    <path d="M 9 2 C 5.1458514 2 2 5.1458514 2 9 C 2 12.854149 5.1458514 16 9 16 C 10.747998 16 12.345009 15.348024 13.574219 14.28125 L 14 14.707031 L 14 16 L 20 22 L 22 20 L 16 14 L 14.707031 14 L 14.28125 13.574219 C 15.348024 12.345009 16 10.747998 16 9 C 16 5.1458514 12.854149 2 9 2 z M 9 4 C 11.773268 4 14 6.2267316 14 9 C 14 11.773268 11.773268 14 9 14 C 6.2267316 14 4 11.773268 4 9 C 4 6.2267316 6.2267316 4 9 4 z"/></svg>
                
                <span>Search</span>
            </a>
        </li>
        

        
            <li id="dark-mode-toggle">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                <span>Dark Mode</span>
            </li>
        
    </ol>
</aside>

            <main class="main full-width">
    <div id="article-toolbar">
        <a href="/" class="back-home">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



            <span>Back</span>
        </a>
    </div>

    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/braintumor/">
                <img src="/p/braintumor/images/banner_hu9eb6ca4c1784edf324c9c2921bdfab7c_1375483_800x0_resize_q75_box.jpg"
                        srcset="/p/braintumor/images/banner_hu9eb6ca4c1784edf324c9c2921bdfab7c_1375483_800x0_resize_q75_box.jpg 800w, /p/braintumor/images/banner_hu9eb6ca4c1784edf324c9c2921bdfab7c_1375483_1600x0_resize_q75_box.jpg 1600w"
                        width="800" 
                        height="608" 
                        loading="lazy"
                        alt="Featured image of post Brain Tumor Detection using SageMaker &amp; TensorFlow" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/projects/" >
                Projects
            </a>
        
            <a href="/categories/python/" >
                Python
            </a>
        
            <a href="/categories/tensorflow/" >
                TensorFlow
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/p/braintumor/">Brain Tumor Detection using SageMaker &amp; TensorFlow</a>
    </h2>

    
    <h3 class="article-subtitle">
        In this project I use TensorFlow and Amazon SageMaker to build, train, and deploy a deep learning model that can accurately classify MRI scans of 4 different types of brain tumors.
    </h3>
    <footer class="article-time">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <time class="article-time--published">May 10, 2021</time>
    </footer></div>
</header>

    <section class="article-content">
    <blockquote>
<p><a class="link" href="https://github.com/dhelms1/brain_tumor"  target="_blank" rel="noopener"
    ><strong>Original GitHub Repository</strong></a></p>
</blockquote>
<h1 id="introduction">Introduction</h1>
<p>MRI scans are one of the main tools used for analyzing tumors in the human brain. Huge amounts of image data are generated through these scans, which need to be examined by a radiologist, and can be susceptible to diagnosis error due to complex MRI scans. This is where the application of neural networks come in. Through Convolutional Neural Networks, we are able to process these scans in order to extract low level features that can help us correctly classifying and diagnose brain tumors. The purpose of this project is to deploy a deep learning model using Amazon SageMaker that can accurately classify MRI scans of brain tumors into four different categories:</p>
<ul>
<li><strong>Glioma</strong> - a tumor made of astrocytes that occurs in the brain and spinal cord.</li>
<li><strong>Meningioma</strong> - a usually noncancerous tumor that arises from membranes surrounding the brain &amp; spinal cord.</li>
<li><strong>None</strong> - no tumor present in brain.</li>
<li><strong>Pituitary</strong> - a tumor that forms in the pituitary gland near the brain that can change hormone levels.</li>
</ul>
<h1 id="acquiring--processing-the-data">Acquiring &amp; Processing the data</h1>
<p>The first step in this project was acquiring the images necessary for training and testing, which originally came from <a class="link" href="https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri"  target="_blank" rel="noopener"
    ><strong>Kaggle</strong></a> but was cloned from the original <a class="link" href="https://github.com/sartajbhuvaji/brain-tumor-classification-dataset"  target="_blank" rel="noopener"
    ><strong>GitHub Repository</strong></a> hosted by <em>Sartaj Bhuvaji</em>. Initially, 2870 training images and 394 testing images were present in the data set. The training images were further split into training/validation sets, with an 80/20 split resulting in 2296 training and 574 validation images.</p>
<p><em>NOTE: I now question whether randomly flipping up/down was appropriate for medical imaging (since most will be inputted the correct way), but it did not seem to negatively effect the results so I left it in. Going forward, possibly avoiding augmenting the images in a way that is very uncommon could be more beneficial.</em></p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">cast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span> <span class="c1"># convert to tensor</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>

<span class="k">def</span> <span class="nf">train_augment</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span> <span class="c1"># convert to tensor</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_flip_up_down</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># random flip</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_flip_left_right</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># random flip</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
    
<span class="n">train_dataset</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">train_dataset</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">train_augment</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
    <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">AUTOTUNE</span><span class="p">)</span>
    <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="p">)</span>

<span class="n">val_dataset</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">val_dataset</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">cast</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">AUTOTUNE</span><span class="p">)</span>
    <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p><figure style="flex-grow: 144; flex-basis: 347px">
		<a href="/p/braintumor/images/train_img.jpg" data-size="850x587"><img src="/p/braintumor/images/train_img.jpg"
				
				width="850"
				height="587"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Although these data sets came separated into subdirectories, which could have been uploaded directly to S3 and read using the ImageDataGenerator function in TensorFlow, I decided to convert them into TensorFlow Datasets and then into TFRecord Files. The training images were also augmented using rotations and initially, normalization. Little did I know how much of an issue this one step would become during training&hellip;</p>
<h3 id="the-struggle-of-never-reading">The Struggle of Never Reading</h3>
<p>Side note, if there&rsquo;s one thing this project taught me it was to read the documentation. I had chosen the EfficientNet architecture for the model, which I now know handles normalization within the architecture, but I had skipped reading this part. 2 days and countless failed training runs later, and I still couldn&rsquo;t figure out why my images (which were currently being normalized) were resulting in 99% train accuracy and 10% validation accuracy. That is, until I opened the <a class="link" href="https://keras.io/api/applications/efficientnet/"  target="_blank" rel="noopener"
    ><strong>EfficientNet</strong></a> documentation and saw the fourth line:</p>
<blockquote>
<p>EfficientNet models expect their inputs to be float tensors of pixels with values in the [0-255] range.</p>
</blockquote>
<h1 id="heading"></h1>
<p><figure style="flex-grow: 210; flex-basis: 506px">
		<a href="/p/braintumor/images/fine_meme.jpg" data-size="979x464"><img src="/p/braintumor/images/fine_meme.jpg"
				
				width="979"
				height="464"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>I don&rsquo;t know if anything else could describe the feeling when I saw that line. Hours of downloading and importing the project into Google Colab to step through the training process&hellip; Hours of reformatting my TFRecord files thinking I was mislabeling my data&hellip; Hours of reading Medium articles on SageMaker and TensorFlow training setups&hellip; But it&rsquo;s totally fine. At least my file size was reduced a lot from removing the normalization so hey, one win for me I guess.</p>
<h1 id="modeling">Modeling</h1>
<p>Alright, enough with my rant. Now we can finally get to the good part, creating the TensorFlow model. As stated previously, the EfficientNet architecture was chosen for the model, more specifically the B0 architecture. Looking into the <em>scripts/model.py</em> directory we can see that the output was replaced with a new Dense layer to handle our 4 classes:</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="kn">from</span> <span class="nn">tensorflow.keras.applications</span> <span class="kn">import</span> <span class="n">EfficientNetB0</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="n">IMG_SIZE</span> <span class="o">=</span> <span class="mi">224</span>

<span class="k">def</span> <span class="nf">EfficientNetClassifier</span><span class="p">():</span>
    <span class="n">effnetb0</span> <span class="o">=</span> <span class="n">EfficientNetB0</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">IMG_SIZE</span><span class="p">,</span> <span class="n">IMG_SIZE</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">effnetb0</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">effnetb0</span><span class="o">.</span><span class="n">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><p>Class weights were also computed and uploaded to the S3 bucket since there is a slight difference in the no_tumor class compared to the other three and we need the model to learn equally from each class. They are as follows:</p>
<table>
<thead>
<tr>
<th>Class</th>
<th>Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>Glioma</td>
<td>0.86445783</td>
</tr>
<tr>
<td>Meningioma</td>
<td>0.85928144</td>
</tr>
<tr>
<td>No Tumor</td>
<td>1.81072555</td>
</tr>
<tr>
<td>Pituitary</td>
<td>0.88717156</td>
</tr>
</tbody>
</table>
<p>Both Early Stopping and Learning Rate Reduction are implemented for the model, with learning rate being reduced 3 times and early stopping occurring at epoch 16 when validation loss plateaued.</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">lr_reduction</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">)</span>
</code></pre></div><h2 id="training">Training</h2>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
          <span class="n">validation_data</span><span class="o">=</span><span class="n">val_dataset</span><span class="p">,</span>
          <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
          <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">,</span>
          <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">lr_reduction</span><span class="p">])</span>
</code></pre></div><p>On epoch 1, the initial training accuracy was 85.1% with a validation accuracy of 62.37%. After epoch 5, 13, and 15 the learning rate was reduced from an initial value of 0.001 to a final value of 0.000008. Early stopping ended our models training after epoch 16, where the validation loss plateaued around 0.044. The final results from training are:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>0.0036</td>
<td>99.91%</td>
</tr>
<tr>
<td>Validation</td>
<td>0.0431</td>
<td>98.61%</td>
</tr>
</tbody>
</table>
<p>With such a high training accuracy, I would be skeptical that the model is overfitting. But since our validation accuracy is within 1.5% of the training accuracy, it leads me to think that the model is performing well. This will be either confirmed or denied in the testing results section depending on the accuracy of the model of predicting with new data. <em>Note: The final model is saved to the default S3 bucket, which will be loaded back into the main notebook and used for predicting.</em></p>
<h2 id="testing">Testing</h2>
<p>Testing images/labels (394 total) were loaded and saved into numpy arrays, which were then flattened and sent to the endpoint for predicting. The maximum probability from the predicted array was then taken and converted back into a string label corresponding to the true label.</p>
<div class="highlight"><pre class="chroma"><code class="language-Python" data-lang="Python"><span class="k">def</span> <span class="nf">process_image</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;instances&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">image</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">input_data</span>
  
<span class="k">def</span> <span class="nf">predict_test_data</span><span class="p">(</span><span class="n">test_img</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">test_img</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">process_image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">string_labels</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="s1">&#39;predictions&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])])</span>
    
    <span class="k">return</span> <span class="n">preds</span><span class="p">,</span> <span class="n">probs</span>
</code></pre></div><p>With such a high training/validation accuracy, I expected the testing accuracy to have a similar result. (NOTE: refer to <em>test_results</em> directory to see the dataframe containing each test images predicted probability/label, true label, and if it was correct). However, the final results were:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Testing</td>
<td>73.86%</td>
</tr>
</tbody>
</table>
<p>With these results, I&rsquo;m skeptical that the model is indeed overfitting. With ~25% difference in training and testing accuracy, we may need to re-evaluate how we are augmenting images and ensure that there are no duplicates in the train/validation sets. However, further exploration and looking at other people&rsquo;s projects, we can see the test accuracy we achieved is in on the mid-high end (which is somewhat reassuring I think?).</p>
<h1 id="final-results">Final Results</h1>
<p>Due to the large difference in train/test accuracy, further exploration was needed on these incorrectly predicted observations. Looking at the testing accuracy for each class, as well as the probabilities for the incorrect predictions, we get the following:</p>
<p><figure style="flex-grow: 182; flex-basis: 437px">
		<a href="/p/braintumor/images/per_class.jpg" data-size="521x286"><img src="/p/braintumor/images/per_class.jpg"
				
				width="521"
				height="286"
				loading="lazy"
				>
		</a>
		
	</figure> <figure style="flex-grow: 176; flex-basis: 423px">
		<a href="/p/braintumor/images/wrong_probs.jpg" data-size="505x286"><img src="/p/braintumor/images/wrong_probs.jpg"
				
				width="505"
				height="286"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<p>Looking at the accuracy per class, we can see that <em>Meningioma</em> and <em>No Tumor</em> are performing close to 100% accuracy. However, <em>Glioma</em> and <em>Pituitary</em> are not performing very well at all, with around 30% and 65% accuracy (respectively). If we looked the the probabilities associated with these incorrect predictions, we can see that most are extremely confident in their predictions. This leads me to wonder if something within the dataset is causing the issues, especially with everyone else having the same issues from what I can tell.</p>
<p>Finally, looking at a confusion matrix, we can see where these mislabeled predictions are:</p>
<p><figure style="flex-grow: 182; flex-basis: 438px">
		<a href="/p/braintumor/images/conf_mat.jpg" data-size="703x385"><img src="/p/braintumor/images/conf_mat.jpg"
				
				width="703"
				height="385"
				loading="lazy"
				>
		</a>
		
	</figure></p>
<ul>
<li><strong>Glioma</strong> - it seems the majority of Glioma tumors are being classified as Meningioma (56), followed by the true label Glioma (26), followed by No Tumor (17), and finally Pituitary (1).</li>
<li><strong>Pituitary</strong> - It seems the majority of Pituitary tumors are being classified as the true label Pituitary (48), followed by No Tumor (14), and finally Meningioma (12).</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>From start to finish, this project was a learning experience that taught me more than I expected. As my first real project using TensorFlow, I feel there are areas that I could improve on (maybe going straight to TFRecord instead of the intermediate step of a TFDataset) but overall I&rsquo;m happy with my results. Getting a training/validation accuracy of over 99% was very nice, and although testing accuracy was much lower I have suspicions that it may be an issues with the labeling/images rather than overfitting (given other people&rsquo;s similar results). Overall, this project taught a lot of crucial skills that I will carry onto my future projects. But none more important than READ THE DOCUMENTATION.</p>
<h2 id="resources">Resources</h2>
<p>Below is a list of resources used throughout this project, including both documentation and articles that, help with ideas/formatting of my TensorFlow code and made the project possible:</p>
<ul>
<li><a class="link" href="https://towardsdatascience.com/train-a-tensorflow-model-in-amazon-sagemaker-e2df9b036a8"  target="_blank" rel="noopener"
    >Train a TensorFlow Model in Amazon SageMaker</a> - Jun M.</li>
<li><a class="link" href="https://www.tensorflow.org/tutorials/load_data/tfrecord#walkthrough_reading_and_writing_image_data"  target="_blank" rel="noopener"
    >TFRecord and Image Data Example</a> - TensorFlow Documentation</li>
<li><a class="link" href="https://towardsdatascience.com/how-to-train-an-image-classifier-on-tfrecord-files-97a98a6a1d7a"  target="_blank" rel="noopener"
    >How to train an Image Classifier on TFRecord files</a> - Karan Sindwani</li>
<li><a class="link" href="https://towardsdatascience.com/working-with-tfrecords-and-tf-train-example-36d111b3ff4d"  target="_blank" rel="noopener"
    >Working with TFRecords and tf.train.Example</a> - Cihan Soylu</li>
<li><a class="link" href="https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"  target="_blank" rel="noopener"
    >Image classification via fine-tuning with EfficientNet</a> - Keras API</li>
</ul>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/python/">Python</a>
        
            <a href="/tags/deep-learning/">Deep Learning</a>
        
            <a href="/tags/tensorflow/">TensorFlow</a>
        
    </section>


    </footer>

    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">Related contents</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="has-image">
    <a href="/p/strokeprediction/">
        
        
            <div class="article-image">
                <img src="/p/strokeprediction/img/header.8d5fc9f40277fc2e6277efd5be0a402e_hucdbe1c766fbb076b014b35afd7dcfcdc_900878_250x150_fill_q75_box_smart1.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="strokeprediction" 
                        data-hash="md5-jV/J9AJ3/C5id&#43;/VvgpALg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Stroke Prediction: Battle of the Learning Methods</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/prodvspop/">
        
        
            <div class="article-image">
                <img src="/p/prodvspop/img/banner.44fe4c762e3c9a637a486dde25ff5ab3_hu2fba2ac787967c0d994b01819822a76b_1268585_250x150_fill_box_smart1_2.png" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="prodvspop" 
                        data-hash="md5-RP5Mdi48mmN6SG3eJf9asw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Global Food Production vs. Population: Predicting the Future</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/gpuscraping/">
        
        
            <div class="article-image">
                <img src="/p/gpuscraping/img/banner.f92c3686a581e36a20d48dbd8254f38f_huda43a43d3a89ed86863926de204a0975_2317027_250x150_fill_box_smart1_2.png" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="gpuscraping" 
                        data-hash="md5-&#43;Sw2hqWB42og1I29glTzjw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Web Scraping GPU Information with Rvest</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>


    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2021 Derek Helms
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="2.3.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >
            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
