[{"content":" Original GitHub Repository\n So Close\u0026hellip; Yet So Far If you\u0026rsquo;re reading this, then most likely you too have some type of interest in machine learning or data science. And you know that GPUs are amazing for speeding up the processes you do in this fields, especially for Deep Learning. However if you\u0026rsquo;re currently in the market for one of these GPUs, especially the new NVIDIA RTX Series which would be PERFECT for Deep Learning projects, you know how impossible it is to find one of them (special thanks to all the crypto miners and resellers out there). So I figured if I can\u0026rsquo;t have one, might as well scrape Newegg for information on a bunch of different graphics cards and do some Data Analysis right? Not my first though, but it did sound like an interesting project. And although I initially wanted to implement it in Python, I figured this would be a great learning opportunity for R especially given my current internship. And so here we are, scraping Newegg for information on GPUs while I patiently wait for them to become available.\nIn my many hours of searching the internet about the rules of web scraping and if certain website allow it or not, I find myself asking \u0026ldquo;Am I really suppose to be doing this?\u0026rdquo;. Some people say no, some say it\u0026rsquo;s fine, others just avoid the question and continue their work. I still don\u0026rsquo;t have the answer to this question, but in my attempt to collect the data as ethically as possible I\u0026rsquo;ve taken the following steps:\n Only creating requests ~2 times per day, spaced out by a few hours. Keeping the data private on my computer and not uploading for public use. No private information is being collected, only product details.  Each page on Newegg has around 36 results, and even though it has hundreds of pages I only scraped 10 of them over the span of 5 days. I didn\u0026rsquo;t want to create too many requests and overload their site, so I tried to minimize the amount of time I spent sending requests. Hopefully this is ethical, and if not then this all was just hypothetical.\nThe Scraping Script So basically there are three main categories for which data was scraped: Model Specs (Brand, Series, Memory, etc.), Ratings (Average \u0026amp; Total Number), and Pricing Information (Price, Sale, \u0026amp; Shipping). I\u0026rsquo;ll break down each of these, and the exact code used to collect the data.\nJust a quick note, a lifesaver for this project was the SelectorGadget from Rvest and part of the tidyverse. Basically instead of having to inspect elements using html code, this allowed me to find css selectors in the page that I needed and extract them below (these are the string passed to the html_nodes() functions).\nGPU Specs get_model_info \u0026lt;- function(gpu_link) { cols \u0026lt;- c(\u0026#39;Brand\u0026#39;, \u0026#39;Model\u0026#39;, \u0026#39;Chipset Manufacturer\u0026#39;, \u0026#39;GPU Series\u0026#39;, \u0026#39;GPU\u0026#39;, \u0026#39;Core Clock\u0026#39;, \u0026#39;Boost Clock\u0026#39;, \u0026#39;Memory Size\u0026#39;, \u0026#39;Memory Interface\u0026#39;, \u0026#39;Memory Type\u0026#39;, \u0026#39;Date First Available\u0026#39;) model_info \u0026lt;- read_html(gpu_link) %\u0026gt;% html_nodes(\u0026#34;table , #product-mini-feature .tab-nav:nth-child(2)\u0026#34;) %\u0026gt;% html_table(fill=TRUE) info_df \u0026lt;- data.frame(tmp=t(do.call(rbind, model_info))) %\u0026gt;% row_to_names(row_number = 1) for (cname in cols) { if (cname %notin% colnames(info_df)) { info_df[cname] \u0026lt;- rep(NA, nrow(info_df)) } } info_df[cols] } So the first piece of information I wanted to pull were specs for each GPU (described in the cols variable above). This wasn\u0026rsquo;t too hard to complete, the string in html_nodes() navigates to the specs tab and pulls the entire table from the website. I then collapsed this into a data frame, and if the specs tab was missing one of the desired pieces of information (from the cols variable), then I would fill the column with NA. The function returned a data frame with the select columns, which was then added to a larger data frame which I will describe later in the Data Collection section.\nGPU Ratings get_gpu_ratings \u0026lt;- function(gpu_link) { data.frame(\u0026#39;AvgRating\u0026#39; = substring(unlist(strsplit(as.character(read_html(gpu_link) %\u0026gt;% html_nodes(\u0026#34;.product-rating\u0026#34;)), \u0026#34;rating rating\u0026#34;))[2], 10, 30), \u0026#39;NumberOfRatings\u0026#39; = read_html(gpu_link) %\u0026gt;% html_nodes(\u0026#34;.product-rating .item-rating-num\u0026#34;) %\u0026gt;% html_text() %\u0026gt;% .[1]) } Now this\u0026hellip; This was true pain to get working. If only Newegg had a normal rating system like every other website out there. But of course they have to put their ratings as \u0026ldquo;eggs\u0026rdquo;, which is creative I will say, but also made pulling this information ridiculously long. I don\u0026rsquo;t expect the above code to make much sense (it doesn\u0026rsquo;t really to me and I\u0026rsquo;m the one who wrote it), but basically I had to read through the raw data without converting it to readable text using html_text(). I couldn\u0026rsquo;t just index it either because each rating appeared at a different point for each gpu. However, the work around this was to split the raw data at the rating rating tag and take the second object, which I then subset ranging from index 10 to 30 to produce this: =\u0026quot;5 out of 5 eggs\u0026quot;\u0026gt;\u0026lt;/. This is an example of rating for a \u0026ldquo;5 star\u0026rdquo; product, which I will now need to clean but that will occur later during the Data Processing section (I probably could have decreased the index range, but I wanted to keep a buffer just in case). Getting the Number of Ratings was much easier, just converting to readable text and taking the first item. I\u0026rsquo;m just glad this part is over, and now we can move onto the Pricing script code.\nGPU Pricing get_gpu_pricing \u0026lt;- function(page_link) { page \u0026lt;- read_html(page_link) tmp_df \u0026lt;- data.frame(\u0026#39;Price\u0026#39; = page %\u0026gt;% html_nodes(\u0026#39;.price-current\u0026#39;) %\u0026gt;% html_text()) tmp_df$CurrentSavings \u0026lt;- page %\u0026gt;% html_nodes(\u0026#39;.price-save\u0026#39;) %\u0026gt;% html_text() tmp_df$Shipping \u0026lt;- page %\u0026gt;% html_nodes(\u0026#39;.price-ship\u0026#39;) %\u0026gt;% html_text() tmp_df } We go from the hardest to now the easiest piece of code in this entire project. I wish everything was this straight forward, just a simple css selector and converting to it text and we have all the variables we need. I pulled the current price, the savings tag (saying how much % off an item was), and the shipping cost (either \u0026ldquo;Free\u0026rdquo; or Numeric value). Originally I had tried to also get the original price before it was put on sale and create a new column, where I\u0026rsquo;d fill it with the current price if not on sale and the sale price if on sale but this gave me a lot of issues. The original prices was a different size vector each time and trying to input them was taking more time than I\u0026rsquo;d like to admit, so I skipped this and just stuck with the current price.\nData Collection So as I had previously stated earlier, the data was gathered over a number of days since I only wanted to send a few requests per day. Around 2 pages were scraped per day for a total of 356 observations over the collection period. To save some room I won\u0026rsquo;t include all the code used to gather and combine the data into a single data set, but the general idea was that I created an empty data frame for each script above. I then bind the returned values by rows into a data frame, which then contained 36 observations and the corresponding number of features from above. Below is the code used to do this:\nI would then read in the previously saved raw data, create a new data frame that matched the current layout of the raw data, and then would row bind the two data frames together into a new raw data set. This would be saved, and the process would be repeated for each page (since they were spaced out). At the end of the process, I ended with 356 total observations with 18 features. However, this data was not at all ready to be used for Exploration or Modeling, and that brings us to the cleaning script.\nData Cleaning ❤️ RegExp Okay so maybe not, but there was a good amount of regular expression\u0026rsquo;s used to clean the data. I guess it could be seen as a good thing, but everything read in from the scraping script was put into the data frame as a character. Lot\u0026rsquo;s of which held the numeric values somewhere in a string, such as:\n\u0026quot;1241 MHz (OC Mode)1190 MHz (Gaming Mode)1127 MHz (Silent Mode)\u0026quot;\nAll I wanted was the core clock speed, but this is what I got instead. And yes I thought maybe split it and index, but of course Newegg likes to change the order in which they list the modes so I couldn\u0026rsquo;t do that. But that\u0026rsquo;s besides the current point. I had all the data ready and it\u0026rsquo;s time to get it in the correct format and create any new features that would be necessary. My goal is to maintain around 300 observations (hence why I scraped a little extra) after cleaning up the data. Just for ease of reading and my own sanity, I\u0026rsquo;ll split up the cleaning based on the scripts above.\nGPU Spec Cleaning Luckily, a few features of this data could be left as characters and didn\u0026rsquo;t need much cleaning (Brand, Model, GPU Series, GPU, Memory Interface, \u0026amp; Memory Type). Other features like Chipset Manufacturer and Memory Size only had slight cleaning done to them (Chipset had a single value filtered out and Memory Size was filtered to only keep memory measured in GB, which was all but 4).\nClock Speeds raw.data \u0026lt;- raw.data %\u0026gt;% mutate(tmp=regmatches(Core.Clock, gregexpr(\u0026#34;[[:digit:]]+\u0026#34;, Core.Clock)), Core.Clock=as.numeric(lapply(tmp, min)), OC.Core.Clock=as.numeric(lapply(tmp, function(x) ifelse(length(x) \u0026gt; 1, max(x), NA)))) %\u0026gt;% select(-tmp) %\u0026gt;% select(Page:Core.Clock,OC.Core.Clock,Boost.Clock:NumberOfRatings) raw.data \u0026lt;- raw.data %\u0026gt;% mutate(tmp=regmatches(Boost.Clock, gregexpr(\u0026#34;[[:digit:]]+\u0026#34;, Boost.Clock)), Boost.Clock=as.numeric(lapply(tmp, min)), OC.Boost.Clock=as.numeric(lapply(tmp, function(x) ifelse(length(x) \u0026gt; 1, max(x), NA)))) %\u0026gt;% select(-tmp) %\u0026gt;% select(Page:Boost.Clock,OC.Boost.Clock,Memory.Size:NumberOfRatings) So now as I had pointed out earlier, the clock speeds (both core and boost) had been read in as strings. But of course there were multiple values that had to be parsed, which were then extracted into lists of numbers. I decided on the following methods for both the core and boost clock speed features:\n Base: taking the minimum gave the the lowest measurement within each row, so I just assumed this as the \u0026ldquo;base\u0026rdquo; value. OC: the overclock value was a little more tricky. I couldn\u0026rsquo;t just take the max because then GPUs that couldn\u0026rsquo;t be overclocked had their base value in the OC feature. The solution to this was to ensure there was more than one value in the feature, and if true then we took the max (otherwise we filled in with NA).  Month/Year raw.data \u0026lt;- raw.data %\u0026gt;% mutate(Date.First.Available=as.Date(raw.data$Date.First.Available, format = \u0026#34;%B %d, %Y\u0026#34;), Month=month(Date.First.Available), Year=year(Date.First.Available)) %\u0026gt;% select(-Date.First.Available) %\u0026gt;% select(Page:Memory.Type, Month, Year, Price:NumberOfRatings) Not too much cleaning here either. The original format for the date feature was \u0026ldquo;Month Day, Year\u0026rdquo; which needed to be converted to \u0026ldquo;YYYY-MM-DD\u0026rdquo; before using the lubridate functions. After this, I created features for the Month and Year it was first available (both numeric) then dropped the original Date feature.\nGPU Rating Cleaning Average Ratings raw.data \u0026lt;- raw.data %\u0026gt;% mutate(AvgRating=as.numeric(lapply(AvgRating, function(x) unlist(regmatches(x, gregexpr(\u0026#34;[0-9]\u0026#34;, x)))[1]))) As I had said earlier, the Average Ratings were in a bit of a strange format. Each review had the form =“[ ] out of 5 eggs”\u0026gt;\u0026lt;/ where [ ] refers to the Average Rating. To get this value, I used a regular expression to get the first numeric value appearing in the string, unlisted it, and converted to numeric. Voila, we now have our rating in the correct format.\nNumber of Ratings raw.data \u0026lt;- raw.data %\u0026gt;% mutate(NumberOfRatings=as.numeric(gsub(\u0026#34;\\\\(|\\\\)\u0026#34;, \u0026#34;\u0026#34;, NumberOfRatings))) Another simple cleaning, the number of ratings had parentheses around them and so I substituted either \u0026ldquo;(\u0026rdquo; or \u0026ldquo;)\u0026rdquo; with \u0026quot;\u0026quot; to remove them and convert to numeric.\nGPU Pricing Cleaning Price raw.data \u0026lt;- raw.data %\u0026gt;% mutate(Price=as.numeric(lapply(Price, function(x) gsub(\u0026#34;\\\\$\u0026#34;, \u0026#34;\u0026#34;, unlist(str_split(x, \u0026#39;\\\\s+\u0026#39;))[1])))) This one was a little tricky, but the Price feature had the following initial format $1,061.99 (11 Offers)–. The way I handled this was to split on the space and take the first element from this vector, which I then substituted the \u0026ldquo;$\u0026rdquo; for empty space and finally converted the values to numeric. Initially I had wanted to extract the numeric values similar to ratings above, but this also split on the period and I was losing the cent values.\nCurrent Savings raw.data \u0026lt;- raw.data %\u0026gt;% mutate(tmp=gsub(\u0026#34;%\u0026#34;, \u0026#34;\u0026#34;, regmatches(CurrentSavings, gregexpr(\u0026#34;[0-9]*%\u0026#34;, CurrentSavings))), CurrentSavings=as.numeric(lapply(tmp, function(x) ifelse(rlang::is_empty(x), NA, x)))) %\u0026gt;% select(-tmp) At first this one was also a little tricky, as there were two values formats this feature had: Save: 14% and Sale Ends in 7 Hours - Save: 8%. So again I couldn\u0026rsquo;t just take the first number, and the solution for this was to look for patterns that were numeric and followed by a % sign. I then removed the % sign, filling non-sale items with NA, and finally converted to numeric.\nShipping Fees raw.data \u0026lt;- raw.data %\u0026gt;% mutate(Shipping=recode(Shipping, \u0026#34;Free Shipping\u0026#34; = \u0026#34;0\u0026#34;), tmp=ifelse(Shipping != \u0026#34;0\u0026#34;, regmatches(Shipping, gregexpr(\u0026#34;\\\\$[0-9]*.[0-9]*\u0026#34;, Shipping)), Shipping), Shipping=as.numeric(gsub(\u0026#34;\\\\$\u0026#34;, \u0026#34;\u0026#34;, tmp))) %\u0026gt;% select(-tmp) This one wasn\u0026rsquo;t too bad either, a lot of the GPUs had free shipping (which I replaced with 0 since there was no cost). The general format for this feature was $39.99 Shipping, so I used a regular expression to find a pattern of \\\\$[0-9]*.[0-9]*. I then stripped the $ off of the string and converted to numeric.\nFinal Cleaning (NA Filtering) raw.data \u0026lt;- raw.data %\u0026gt;% mutate(na.percent=round(rowSums(is.na(.))/ncol(.), 4)*100) %\u0026gt;% filter(na.percent \u0026lt; 30) %\u0026gt;% select(-na.percent) The final step. Although I don\u0026rsquo;t have a solid reason as to why I chose to keep all values with less than 30% of their data missing, that\u0026rsquo;s what I chose to do. Doing this took away the most observations of all the data cleaning, pushing us from 351 to 288 final observations.\nSo a bit under my initial goal, but close enough. We now have 288 observations with 21 features that are ready to be explored.\nData Exploration Okay so I know I said that I wasn\u0026rsquo;t going to upload the data for privacy reasons, but I figured showing a few observations would be fine (just to get a feel for the general layout of the data).\n\r\nThis is the final cleaned data, and it\u0026rsquo;s ready to explore. I\u0026rsquo;ll go over a few of the main features in the data set and some aggregated graphs/tables. The main purpose of this project was web scraping and data cleaning, but it\u0026rsquo;s also important to actually use the data in exploration.\nBrands \r \r\nAs we can see, mainly 4 brands are responsible for the majority of the GPUs within our data. MSI, ASUS, GIGABYTE, and EVGA have a clear lead on the number of GPUs present on NewEgg. If we were to look at the average price over the past few years for each of these brands, there is no clear pattern but it does seem that in 2021 and 2021 the average prices seem to be stabilizing and within approximately $100 of each other. One more aspect I wanted to look at is the average review for each brand:\n   Brand Rating     EVGA 4.52   ASUS 4.35   MSI 4.32   GIGABYTE 4.20    Although there is now huge difference in rating either, EVGA does seem to be the leader by a bit. ASUS and MSI and nearly tied, and GIGABYTE trails behind them by a small amount.\nGPUs \r \r\nIf we look at the most common GPUs in our data set we can see the majority of them are GeForce, with only the Radeon RX 6700 XT being in the top 11. However this does not carry over to the most common GPU Series, which seems to be split 6 to 5 for NVIDIA and AMD (respectively). The GTX 10 Series is the most common, which makes since given how long they have been on the market. I also wanted to look into the highest rated GPUs:\n\r\nThe top 15 rated GPUs (for the most part) all fall under the GeForce Series, except the Radeon HD 5870. Which is an interesting find because it only has 1 GB of memory size, cost $325, and does not have good performance (from what I saw on YouTube it was averaging below 30 FPS on many modern games). Yet there are 239 reviews with an average of 5 stars\u0026hellip; Maybe there\u0026rsquo;s something I\u0026rsquo;m missing here but that blows my mind. Besides that, many of these remaining GPUs are still popular to this day being in the GTX 10, 16, and 20 Series. I also filtered these results to have at least 30 ratings since GPUs, with let\u0026rsquo;s say 3 total ratings of 5 stars, aren\u0026rsquo;t as reliable to be \u0026ldquo;top rated\u0026rdquo;.\n\r\nI also pulled an average overview of the specs for the 6 most common GPU series in the data. Although the NVIDIA cards seem to be more expensive than AMD, they also have a higher average rating. If memory size was essential, the AMD RX 6000 Series average 16 GB for only $360 which seems like a really good deal (but AMD also isn\u0026rsquo;t compatible with CUDA so keep that in mind).\nClock Speed \r \r\n\r \r\nThere really isn\u0026rsquo;t too much to say about this information. The core clock speed is less than the boost clock speed on average (expected), and the overclocked core clock speed is less than the overclocked boost clock speed on average (expected). I did however have to look into the boost clock value of 5000 MHz, which belongs to the AMD FirePro V7900 and was not an error (which I thought it was). It goes from a core clock of 1250 MHz to a boost clock of 5000 MHz, which is also interesting given it only has 2GB of memory size.\nPrice \r\nFor the most part, the majority of GPUs in the data are below $1000 and seem to cost an average of roughly $500. However, we have a single observation for a GPU that costs over $3000. This had me wondering, what on Earth could cost that much money? Which brought me to examine the 15 most expensive GPUs in the data:\n\r\nI also explored the most expensive GPUs, which is completely dominated by the TITAN V. 12 GB of memory size, 1460 MHz boost clock speed, and uses NVIDIA chipset\u0026hellip; A bit pricey but understandable. Besides that outlier, the most expensive GPUs in our data set (on average) seem to fall into th $620 to $870 range and again, the majority of them are GeForce cards.\nConclusion Overall this project had taught me a ton. Learning to understand html code a bit more has been super helpful in also changing some things in my website. Learning how exactly to scrape data off a website was also a lot of fun, trying this out with a deep learning project in Python where I pull reviews and try sentiment analysis on it might be an interesting idea to explore. Cleaning the data\u0026hellip; between this and my work right now I\u0026rsquo;m not sure how much more data cleaning I\u0026rsquo;d want to do for a while, but it\u0026rsquo;s nice to see the end product so that makes it worth it. Exploring these GPUs has been a very interesting project, I didn\u0026rsquo;t realize how many different types of GPUs there are and all the different specs (memory type, clock speed, memory interface, etc.). I\u0026rsquo;ll definitely want to come back to this data eventually and see if some type of model can be built from it, but that will have to be for a later date.\n","date":"2021-06-29T00:00:00Z","image":"/p/gpuscraping/img/banner_huda43a43d3a89ed86863926de204a0975_2317027_120x120_fill_box_smart1_2.png","permalink":"/p/gpuscraping/","title":"Web Scraping GPU Information with Rvest"},{"content":" Original GitHub Repository\n Introduction Welcome! This is the first of many posts for the #tidytuesday data exploration project in which I explore a smaller data set with no goal of creating a model but strictly to become more familiar with R and packages such as dplyr, tidyverse, \u0026amp; ggplot. In this week, we\u0026rsquo;ll explore the IMDb data set that recorded information such as seasons, date, average rating, and shares for TV dramas from 1990 to 2018. The data originated from the Tidy Tuesday GitHub repository.\nInitial Exploration Reading in the data and displaying the first few rows, we can see:\n\r\ntv.data \u0026lt;- tv.data %\u0026gt;% select(-titleId) # Drop \u0026#39;titleId\u0026#39; Column length(unique(tv.data$title)) # 868 unique shows \r\nInitially I decided to drop the titleId column, as I didn’t see much use for it in the data exploration since we already have title. Of the 2266 observations within the data set, there are 868 unique shows. The number of seasons range from 1 to 44 (which we will need to explore this 44 season show to see if it is true). The average rating ranges from 2.704 to 9.682, as well as shares ranging from 0 to 55.65.\nav.plot \u0026lt;- ggplot(tv.data, aes(x=av_rating)) + geom_histogram(bins=40, fill=\u0026#34;#DA0A0A\u0026#34;) + xlab(\u0026#39;Rating\u0026#39;) + ylab(\u0026#39;\u0026#39;) + ggtitle(\u0026#39;Average Rating\u0026#39;) share.plot \u0026lt;- ggplot(tv.data, aes(x=share)) + geom_histogram(bins=40, fill=\u0026#34;#09C517\u0026#34;) + xlab(\u0026#39;Share\u0026#39;) + ylab(\u0026#39;\u0026#39;) + ggtitle(\u0026#39;Share\u0026#39;) season.plot \u0026lt;- ggplot(tv.data, aes(x=seasonNumber)) + geom_histogram(bins=40, fill=\u0026#34;#0A75DA\u0026#34;) + xlab(\u0026#39;Seasons\u0026#39;) + ylab(\u0026#39;\u0026#39;) + ggtitle(\u0026#39;Number of Seasons\u0026#39;) grid.arrange(av.plot, share.plot, season.plot, ncol=3) \r\nMonth, Day, \u0026amp; Year: Creation/Exploration tv.data \u0026lt;- tv.data %\u0026gt;% mutate(year = year(date), month = month(date), day = day(date)) %\u0026gt;% select(c(1,2,3,7,8,9,4,5,6)) \r\nI decided to split the date feature into 3 subset features: year, month, and day using the lubridate package. This will make exploration easier for me later on where I can group by year or month rather than date strings. I also reordered the columns to put these after the date feature instead of the end of the data frame cause my OCD didn’t like how it looked…\nYear ggplot(tv.data, aes(x=factor(year))) + geom_bar(fill= \u0026#34;#FF6666\u0026#34;) + ylab(\u0026#39;Year Occurance\u0026#39;) + xlab(\u0026#39;Year\u0026#39;) + ggtitle(\u0026#39;Year Occurance Distribution\u0026#39;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + geom_text(aes(y=..count.. + 8, label=..count..), stat=\u0026#39;count\u0026#39;, size = 3) \r\nIt seems that the occurrence for years is extremely right skewed, almost growing exponentially (the last date is 2018-10-10, so we are missing around 2.5 months of data for 2018). We\u0026rsquo;ll now want to explroe the average rating for each year to see if it increases/decreases as more shows seem to be coming out (maybe more isn’t always better).\ntv.data %\u0026gt;% select(year, av_rating) %\u0026gt;% group_by(year) %\u0026gt;% summarize(average_rating = mean(av_rating)) %\u0026gt;% {ggplot(., aes(x=factor(year), y=average_rating)) + geom_point(color= \u0026#34;#FF9333\u0026#34;, size=3) + geom_hline(yintercept=8, color=\u0026#39;red\u0026#39;, alpha=0.5) + geom_vline(xintercept=12, color=\u0026#39;blue\u0026#39;, alpha=0.5) + xlab(\u0026#39;Year\u0026#39;) + ylab(\u0026#39;Average Rating\u0026#39;) + ggtitle(\u0026#39;Average Yearly Rating\u0026#39;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + geom_text(aes(label=sprintf(\u0026#34;%.2f\u0026#34;, average_rating), y=average_rating + 0.03), size=2.8, vjust=0.3)} \r\nLooking at the average rating for each year, it seems the values range from 7.60 (in 2000) to 8.20 (in 2004). For the most part, there are no standout years in which extremely high/low ratings were present. For the most part, it seems that shows prior to 2001 had lower average ratings (all below 8) and almost all shows after 2001 had above an average rating of 8 (excluding 2007 which had a 7.93). If we think of the above graph as being split into the usual graph quadrants (I, II, III, IV), we can see that the majority of the data after 2001 is in quadrant I and all data prior to 2001 is in quadrant III.\nMonth ggplot(tv.data, aes(x=factor(month))) + geom_bar(fill= \u0026#34;#3396FF\u0026#34;) + ylab(\u0026#39;Month Occurance\u0026#39;) + xlab(\u0026#39;Month\u0026#39;) + ggtitle(\u0026#39;Monthly Occurance Distribution\u0026#39;) + geom_text(aes(y=..count.. + 20, label=..count..), stat=\u0026#39;count\u0026#39;, size = 4) \r\nAlthough this doesn’t tell us too much about the data, is seems that the majority of shows in our data set premiere in January, with most months holding somewhat steady in the 130-210 range (except for July).\nOne-Hot-Encoding for Genres genres.types \u0026lt;- unique(unlist(strsplit(unlist(unique(tv.data$genres)), \u0026#39;,\u0026#39;))) tv.data[genres.types] \u0026lt;- rep(NA, nrow(tv.data)) for (idx in 1:nrow(tv.data)) { tv.data[idx, genres.types] \u0026lt;- as.numeric(genres.types %in% unlist(strsplit(tv.data$genres[idx], \u0026#39;,\u0026#39;))) } kable(t(tv.data[1,]), col=\u0026#39;Observation #1\u0026#39;) \r\nThe original genres feature was a single string that listed all genres only separated by a comma. There probably is a fast way out there (or a package that could handle this for me) but my method still only took a few seconds to run so I’m not too worried about optimizing it. I initially got all unique genres strings from the data frame, which I then split on the commas, and finally only kept the unique values from this split (which was 22 total genres). I then created empty columns, looped over all rows for the given columns, and one hot encoded the features to have a 1 is the genre feature contained any of the given genres and a 0 if it does not. I was having an issue of the values being filled column wise, so looping across rows allowed me to fill by rows rather than columns (again, may not be the best way to do it but it still executed quickly so I’ll go with it).\nMost Common Genres data.frame(genres=all_of(genres.types), appearences=unname(tv.data %\u0026gt;% select(all_of(genres.types)) %\u0026gt;% colSums())) %\u0026gt;% slice_max(appearences, n=length(genres.types)) %\u0026gt;% {ggplot(., aes(x=factor(genres, levels= genres), y=appearences)) + geom_col(fill=\u0026#34;#48BF04\u0026#34;) + xlab(\u0026#39;Genre\u0026#39;) + ylab(\u0026#39;Appearences\u0026#39;) + ggtitle(\u0026#39;Number of Appearences Per Genre\u0026#39;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + geom_text(aes(label=sprintf(\u0026#34;%.0f\u0026#34;, appearences), y=appearences + 50), size=3, vjust=0.3)} \r\nFrom the above graph, we can see that “Drama” appears in every TV show genre description (which makes sense since the data set itself covers TV dramas from 1990 to 2018). Following this we can see that “Crime”, “Mystery”, and “Comedy” are the next most common sub-genres (since every show is also a Drama). An idea we may want to explore is whether we can condense these sub-genres down into more broad categories (i.e. combine genres like Action/Adventure or Thriller/Horror) and see how this effects the above graph.\nAverage Rating Per Genre data.frame(genres=all_of(genres.types), avg_ratings=(tv.data %\u0026gt;% select(all_of(genres.types)) %\u0026gt;% mutate_each(funs(.*tv.data$av_rating)) %\u0026gt;% colSums()) / (tv.data %\u0026gt;% select(all_of(genres.types)) %\u0026gt;% colSums())) %\u0026gt;% {ggplot(., aes(x=genres, y=avg_ratings)) + geom_point(color=\u0026#34;#8F0ADA\u0026#34;, size=3) + xlab(\u0026#39;Genre\u0026#39;) + ylab(\u0026#39;Avg Ratings\u0026#39;) + ggtitle(\u0026#39;Average Ratings Per Genre\u0026#39;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + geom_text(aes(label=sprintf(\u0026#34;%.2f\u0026#34;, avg_ratings),y=avg_ratings + 0.18), size=3, vjust=0.3)} \r\nI’m not gonna say I called this one but just look at “Reality-TV”… If you’ve ever seen an episode of Keeping up with the Kardashians or Real Housewives of any major city in America then you’d agree with that number. But besides that, it seems that most fall into the average range of 7.5 to 8.5 (a more reasonable number). It seems that surprisingly, TV shows relating to “War” are the highest rated, followed by “Sports” and “History”. Near the bottom, but not nearly as bad as Reality-TV, we have “Musical” and “Documentaries” coming in last place.\nShow Exploration Top 20 Highest Rated Shows tv.data %\u0026gt;% select(title, year, av_rating) %\u0026gt;% group_by(title) %\u0026gt;% summarize(avg_rating=mean(av_rating), year=floor(min(year))) %\u0026gt;% slice_max(avg_rating, n=20) %\u0026gt;% mutate(title=paste(title,\u0026#39;(\u0026#39;, year, \u0026#39;)\u0026#39;)) %\u0026gt;% {ggplot(., aes(x=fct_rev(factor(title, levels=title)), y=avg_rating)) + geom_col(fill=\u0026#34;#09C59D\u0026#34;) + xlab(\u0026#39;\u0026#39;) + ylab(\u0026#39;Average Rating\u0026#39;) + ggtitle(\u0026#39;Top 20 Shows Average Ratings\u0026#39;) + geom_text(aes(label=sprintf(\u0026#34;%.2f\u0026#34;, avg_rating), y=avg_rating + 0.3), size=3, vjust=0.3) + coord_flip()} \r\nLooking at the above graph, we can see that the 3 highest rated shows are all from the 1990’s (which is surprising given the earlier findings that shows prior to 2001 average ratings below 8.0). However, 16 of the 17 other top 20 shows are all post-2001, which validates our earlier findings. And even further, all of these shows are post-2010, which again makes sense since 2010+ had the highest average yearly ratings for TV shows from earlier findings.\nTop 20 Most Shared Shows tv.data %\u0026gt;% select(title, year, share) %\u0026gt;% group_by(title) %\u0026gt;% summarize(avg_sharing=mean(share), year=floor(min(year))) %\u0026gt;% slice_max(avg_sharing, n=20) %\u0026gt;% mutate(title=paste(title,\u0026#39;(\u0026#39;, year, \u0026#39;)\u0026#39;)) %\u0026gt;% {ggplot(., aes(x=fct_rev(factor(title, levels=title)), y=avg_sharing)) + geom_col(fill=\u0026#34;#C50981\u0026#34;) + xlab(\u0026#39;\u0026#39;) + ylab(\u0026#39;Average Sharing\u0026#39;) + ggtitle(\u0026#39;Top 20 Shared Shows\u0026#39;) + geom_text(aes(label=sprintf(\u0026#34;%.2f\u0026#34;, avg_sharing), y=avg_sharing + 1.2), size=3, vjust=0.3) + coord_flip()} \r\nLooking at the above graph, similar to the top 20 highest rated shows, the top 3 are all 1990’s era. On top of this, there seems to be an even distribution of 1990’s and 2000’s shows, which leads me to wonder if there are less shares for shows as years increases? There also does not seem to be much overlap with shows that have high rating and shows with high shares (maybe people already know about the high rated shows so they are shared less?).\nYearly Average Share tv.data %\u0026gt;% select(year, share) %\u0026gt;% group_by(year) %\u0026gt;% summarize(yearly_shares = mean(share)) %\u0026gt;% { ggplot(., aes(x=factor(year, levels=year), y=yearly_shares)) + geom_point(color=\u0026#34;#09C59D\u0026#34;, size=3) + xlab(\u0026#39;Year\u0026#39;) + ylab(\u0026#39;Average Yearly Share\u0026#39;) + ggtitle(\u0026#39;Average Share per Year \u0026#39;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + geom_text(aes(label=sprintf(\u0026#34;%.1f\u0026#34;, yearly_shares),y=yearly_shares + 0.4), size=3, vjust=0.3)} \r\nMy question from the previous section was correct: It does seem that as we progress through the years, people are sharing shows less. This is an interesting finding since it would seem today we have easier ways to share shows: social media, texting, etc. Maybe with all these new inventions there is less human interaction, but who knows.\nConclusion \r\nOverall, the findings from this project were very interesting. Seeing how more shows began to occur as the years increased was expected, with platforms such as YouTube, Netflix, Hulu, etc. And average rating increasing as years increased was also not a suprise, as I expected the more \u0026ldquo;modern\u0026rdquo; shows would have slightly better overall production. However, it was interesting to see that the top 3 shows with the highest average rating were all from the 1990\u0026rsquo;s as well as the top 4 shared shows being from the same era. With average shares decreasing as the years increased, could this be due to more shows being produced and therefore less average shares per show? Further exploration will be needed to answer this question, but our main question here is \u0026ldquo;Does TV have a golden age?\u0026rdquo;. From the data, if we used average yearly rating we could say yes; and it\u0026rsquo;s currently happening right now. However, if we used yearly shares and highest rated shows then we could also say yes; during the 1990\u0026rsquo;s shows were at their peak of being talked about among people and could be considered the Golden Age of television. This question is up to the individual to answer, and it all depends on what one would consider The Golden Age.\n","date":"2021-06-08T00:00:00Z","image":"/p/tvgoldenage/img/banner_hudecf68f3e37c839d885e2939bd226e89_152575_120x120_fill_q75_box_smart1.jpg","permalink":"/p/tvgoldenage/","title":"#tidytuesday: TV's Golden Age"},{"content":"\r\nIntroduction 6/1/2021 marks the first day of my Research Intership with the CHC (even though I\u0026rsquo;m writing this almost 2 weeks later) and I have some mixed emotions about how I feel about it. I\u0026rsquo;m not saying I don\u0026rsquo;t like the program (it\u0026rsquo;s quite the opposite), I\u0026rsquo;m beyond grateful for the opportunity. But I am nervous. Will I be able to keep up with everyone whose already been working on the project? What if I don\u0026rsquo;t deliver everything I need to at the end of the program? Am I going to be able to learn everything or will I fall behind? There\u0026rsquo;s a lot of questions I keep asking myself, the programs use of R and Data Analysis are new to me. I work on Deep Learning projects using Python (quick shameless plug: go check out some other posts on my website about these projects), and there isn\u0026rsquo;t much analysis for images or text data (some, but not to the extent of a survey with over 250 features). All these new methodologies have me feeling a little overwhelmed, but I\u0026rsquo;m also excited.\nI love learning (I\u0026rsquo;ve been in college for almost 6 years so I guess I better love it) and this is a summer of learning, but without the stress of grades. No midterms, no finals, no surprise quizzes on the one topic you decided to skip reading. I\u0026rsquo;m excited to see where the program takes me and how I will develop both as a person and a researcher.\nWhat exactly is the Internship? For the summer, I\u0026rsquo;ll be working on the second version of the Basic Needs Student Success Survey (BNS3) and helping to prepare the third. The first version was aimed towards Chico State students, the second was 3 CSU schools, and the third will be aimed towards getting it distributed across California. Pulled from the BNS Website:\n The purpose of this cross-sectional pilot study was to create a tool that identifies student perception of the impact of receiving CalFresh assistance on their health, nutrition, cooking confidence, time management and academic performance.\n My main purpose will be analyzing the findings from the second version of the survey and creating a website to present these findings (similar to the above current BNS Website). In no particular order I have three main personal goals for this program: become familiar with research environments/procedures that I can take with me to Grad School, master R and Data Analysis techniques, and finally use the tools learned during the program by applying them to mini projects.\nGoal #1: Learn Research Environments/Procedures It would be an understatement to say I\u0026rsquo;m not the most well-versed when it comes to the terminology of academia (I actually had to look up what well-versed means). English is my only language and I still barely speak that correctly, hence why I became a Math major. But my goal for this summer is to learn both the language and methods for the area of research. As I plan to go onto grad school and will most likely need to complete some type of final research project, this program is the perfect opportunity to learn the necessary tools I will need before I enter higher education.\nEven just from the training I had to, there were a lot more procedures than I previously thought were in a research environment. Publishing credits, data protection, data storage, ethical procedures, review boards, population selection. All these concepts that have to be considered while creating a research topic that I had never even thought about. My goal is to learn more about these, especially as I help prepare to get the next version of the survey ready for statewide distribution, and to carry forward the information I learn with me. I think the exposure that I\u0026rsquo;ll get throughout this summer will help prepare me to see what the next few years of my life will look like during grad school.\nGoal #2: Master R \u0026amp; Data Analysis When I first started on my machine learning journey there were two main languages that everyone talked about: R and Python. And just like everyone else who begins their machine learning journey, I kept asking \u0026ldquo;which one is the best to learn?\u0026rdquo;. Everyone said it was personal choice, and that Python seemed to be growing more popular with all the machine learning libraries it has. On top of this it seemed that industry jobs wanted Python over R experience (from what I found) and that R was mainly for academia, which at the time I never though I would go into. But now here I am, working the entire summer doing academic research using R and I guess I\u0026rsquo;ll finally get the answer to my initial question of which language is the best to learn.\nMy goal for this program is to master R and its libraries. Whether it be from helping update the data cleaning script, creating both this website and the final BNS pilot 2 website using RStudio, or from the Exploratory Data Analysis I\u0026rsquo;ll be doing on the survey data. I guess I do have a bit of a headstart since I\u0026rsquo;ve used R in two of my class (Sampling Methods \u0026amp; Applied Statistics II), but nothing to the extent of what I expect to be doing this summer. I\u0026rsquo;m excited to see how I can take the skills I\u0026rsquo;ve learned from Python and transfer them over the R. What I\u0026rsquo;m not excited about is accidently writing Python code inside of R and getting execution errors (and believe me it\u0026rsquo;s already happened a lot). I\u0026rsquo;m always up for a new challenge, and through both the research project and my outside personal mini-projects with R, I plan to acheive my goal of mastering R.\nGoal #3: Applying What I\u0026rsquo;ve Learned (#tidytuesday) Speaking of mini-projects, #tidytuesday is the main way I plan to apply what I\u0026rsquo;m learning throughout the summer to material outside of the research project. The entire purpose behind TidyTuesday is:\n A weekly data project aimed at the R ecosystem. As this project was borne out of the R4DS Online Learning Community and the R for Data Science textbook, an emphasis was placed on understanding how to summarize and arrange data to make meaningful charts with ggplot2, tidyr, dplyr, and other tools in the tidyverse ecosystem.\n No modeling, no data processing, no evaluation. Just pure Data Exploration. Given that one of the main purpose\u0026rsquo;s of my research project is EDA, this is the perfect opportunity for me to become familiar with R\u0026rsquo;s libraries for graphing and data transformation. I\u0026rsquo;ve already completed one project, discussing TV\u0026rsquo;s Golden Age and whether or not it was real, and I\u0026rsquo;ve already learned a ton from it. Using dpylr for filtering and feature engineering almost made me love it more than Python\u0026rsquo;s version of it.\nMy goal is to set aside around an hour per day for Wed/Thrs/Fri in order to work on one of these projects. I\u0026rsquo;m aiming to completed one every two weeks (so around 4-5 hours working on the actual project an around 1-2 hours creating/posting on this website). Obviously this is just extra work outside of the main project, so some weeks I may not work on TidyTuesday at all if I have too much research work to complete. But I see this as a great opportunity for me to expose myself to the area of EDA which I am not too familiar with.\n","date":"2021-06-01T00:00:00Z","image":"/p/researchgoals/img/banner_huc930a60f412a86b83b2164900be1f83b_448734_120x120_fill_q75_box_smart1.jpg","permalink":"/p/researchgoals/","title":"2021 Summer Undergraduate Research Goals"},{"content":" Original GitHub Repository\n Introduction MRI scans are one of the main tools used for analyzing tumors in the human brain. Huge amounts of image data are generated through these scans, which need to be examined by a radiologist, and can be susceptible to diagnosis error due to complex MRI scans. This is where the application of neural networks come in. Through Convolutional Neural Networks, we are able to process these scans in order to extract low level features that can help us correctly classifying and diagnose brain tumors. The purpose of this project is to deploy a deep learning model using Amazon SageMaker that can accurately classify MRI scans of brain tumors into four different categories:\n Glioma - a tumor made of astrocytes that occurs in the brain and spinal cord. Meningioma - a usually noncancerous tumor that arises from membranes surrounding the brain \u0026amp; spinal cord. None - no tumor present in brain. Pituitary - a tumor that forms in the pituitary gland near the brain that can change hormone levels.  Acquiring \u0026amp; Processing the data The first step in this project was acquiring the images necessary for training and testing, which originally came from Kaggle but was cloned from the original GitHub Repository hosted by Sartaj Bhuvaji. Initially, 2870 training images and 394 testing images were present in the data set. The training images were further split into training/validation sets, with an 80/20 split resulting in 2296 training and 574 validation images.\nNOTE: I now question whether randomly flipping up/down was appropriate for medical imaging (since most will be inputted the correct way), but it did not seem to negatively effect the results so I left it in. Going forward, possibly avoiding augmenting the images in a way that is very uncommon could be more beneficial.\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)) val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)) def cast(image, label): image = tf.convert_to_tensor(image, dtype=tf.uint8) # convert to tensor return image, label def train_augment(image, label): seed = np.random.randint(1, 150, size=1)[0] image = tf.convert_to_tensor(image, dtype=tf.uint8) # convert to tensor image = tf.image.random_flip_up_down(image, seed=seed) # random flip image = tf.image.random_flip_left_right(image, seed=seed) # random flip return image, label train_dataset = ( train_dataset .map(train_augment, num_parallel_calls=AUTOTUNE) .prefetch(AUTOTUNE) .shuffle(buffer_size=len(X_train)) ) val_dataset = ( val_dataset .map(cast, num_parallel_calls=AUTOTUNE) .prefetch(AUTOTUNE) ) \r\nAlthough these data sets came separated into subdirectories, which could have been uploaded directly to S3 and read using the ImageDataGenerator function in TensorFlow, I decided to convert them into TensorFlow Datasets and then into TFRecord Files. The training images were also augmented using rotations and initially, normalization. Little did I know how much of an issue this one step would become during training\u0026hellip;\nThe Struggle of Never Reading Side note, if there\u0026rsquo;s one thing this project taught me it was to read the documentation. I had chosen the EfficientNet architecture for the model, which I now know handles normalization within the architecture, but I had skipped reading this part. 2 days and countless failed training runs later, and I still couldn\u0026rsquo;t figure out why my images (which were currently being normalized) were resulting in 99% train accuracy and 10% validation accuracy. That is, until I opened the EfficientNet documentation and saw the fourth line:\n EfficientNet models expect their inputs to be float tensors of pixels with values in the [0-255] range.\n  \r\nI don\u0026rsquo;t know if anything else could describe the feeling when I saw that line. Hours of downloading and importing the project into Google Colab to step through the training process\u0026hellip; Hours of reformatting my TFRecord files thinking I was mislabeling my data\u0026hellip; Hours of reading Medium articles on SageMaker and TensorFlow training setups\u0026hellip; But it\u0026rsquo;s totally fine. At least my file size was reduced a lot from removing the normalization so hey, one win for me I guess.\nModeling Alright, enough with my rant. Now we can finally get to the good part, creating the TensorFlow model. As stated previously, the EfficientNet architecture was chosen for the model, more specifically the B0 architecture. Looking into the scripts/model.py directory we can see that the output was replaced with a new Dense layer to handle our 4 classes:\nfrom tensorflow.keras.applications import EfficientNetB0 from tensorflow.keras import layers, models from tensorflow.keras.optimizers import Adam IMG_SIZE = 224 def EfficientNetClassifier(): effnetb0 = EfficientNetB0(weights=\u0026#39;imagenet\u0026#39;, include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3)) model = layers.GlobalAveragePooling2D()(effnetb0.output) model = layers.Dropout(0.3)(model) model = layers.Dense(4, activation=\u0026#39;softmax\u0026#39;)(model) model = models.Model(inputs=effnetb0.input, outputs=model) model.compile(optimizer=\u0026#39;Adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model Class weights were also computed and uploaded to the S3 bucket since there is a slight difference in the no_tumor class compared to the other three and we need the model to learn equally from each class. They are as follows:\n   Class Weight     Glioma 0.86445783   Meningioma 0.85928144   No Tumor 1.81072555   Pituitary 0.88717156    Both Early Stopping and Learning Rate Reduction are implemented for the model, with learning rate being reduced 3 times and early stopping occurring at epoch 16 when validation loss plateaued.\nearly_stop = EarlyStopping(monitor=\u0026#39;val_loss\u0026#39;, mode=\u0026#39;min\u0026#39;, patience=5) lr_reduction = ReduceLROnPlateau(monitor=\u0026#39;val_loss\u0026#39;, patience = 2, verbose=1, factor=0.2, min_lr=0.000001) Training model.fit(x=train_dataset, epochs=epochs, validation_data=val_dataset, verbose=2, class_weight=class_weights, callbacks=[early_stop, lr_reduction]) On epoch 1, the initial training accuracy was 85.1% with a validation accuracy of 62.37%. After epoch 5, 13, and 15 the learning rate was reduced from an initial value of 0.001 to a final value of 0.000008. Early stopping ended our models training after epoch 16, where the validation loss plateaued around 0.044. The final results from training are:\n   Dataset Loss Accuracy     Training 0.0036 99.91%   Validation 0.0431 98.61%    With such a high training accuracy, I would be skeptical that the model is overfitting. But since our validation accuracy is within 1.5% of the training accuracy, it leads me to think that the model is performing well. This will be either confirmed or denied in the testing results section depending on the accuracy of the model of predicting with new data. Note: The final model is saved to the default S3 bucket, which will be loaded back into the main notebook and used for predicting.\nTesting Testing images/labels (394 total) were loaded and saved into numpy arrays, which were then flattened and sent to the endpoint for predicting. The maximum probability from the predicted array was then taken and converted back into a string label corresponding to the true label.\ndef process_image(image): image = tf.expand_dims(image, 0) input_data = {\u0026#39;instances\u0026#39;: np.asarray(image)} return input_data def predict_test_data(test_img): preds = [] probs = [] for img in tqdm.tqdm(test_img): input_data = process_image(img) y_pred = predictor.predict(input_data) probs.append(max(y_pred[\u0026#39;predictions\u0026#39;][0])) preds.append(string_labels[np.argmax(y_pred[\u0026#39;predictions\u0026#39;][0])]) return preds, probs With such a high training/validation accuracy, I expected the testing accuracy to have a similar result. (NOTE: refer to test_results directory to see the dataframe containing each test images predicted probability/label, true label, and if it was correct). However, the final results were:\n   Dataset Accuracy     Testing 73.86%    With these results, I\u0026rsquo;m skeptical that the model is indeed overfitting. With ~25% difference in training and testing accuracy, we may need to re-evaluate how we are augmenting images and ensure that there are no duplicates in the train/validation sets. However, further exploration and looking at other people\u0026rsquo;s projects, we can see the test accuracy we achieved is in on the mid-high end (which is somewhat reassuring I think?).\nFinal Results Due to the large difference in train/test accuracy, further exploration was needed on these incorrectly predicted observations. Looking at the testing accuracy for each class, as well as the probabilities for the incorrect predictions, we get the following:\n\r \r\nLooking at the accuracy per class, we can see that Meningioma and No Tumor are performing close to 100% accuracy. However, Glioma and Pituitary are not performing very well at all, with around 30% and 65% accuracy (respectively). If we looked the the probabilities associated with these incorrect predictions, we can see that most are extremely confident in their predictions. This leads me to wonder if something within the dataset is causing the issues, especially with everyone else having the same issues from what I can tell.\nFinally, looking at a confusion matrix, we can see where these mislabeled predictions are:\n\r\n Glioma - it seems the majority of Glioma tumors are being classified as Meningioma (56), followed by the true label Glioma (26), followed by No Tumor (17), and finally Pituitary (1). Pituitary - It seems the majority of Pituitary tumors are being classified as the true label Pituitary (48), followed by No Tumor (14), and finally Meningioma (12).  Conclusion From start to finish, this project was a learning experience that taught me more than I expected. As my first real project using TensorFlow, I feel there are areas that I could improve on (maybe going straight to TFRecord instead of the intermediate step of a TFDataset) but overall I\u0026rsquo;m happy with my results. Getting a training/validation accuracy of over 99% was very nice, and although testing accuracy was much lower I have suspicions that it may be an issues with the labeling/images rather than overfitting (given other people\u0026rsquo;s similar results). Overall, this project taught a lot of crucial skills that I will carry onto my future projects. But none more important than READ THE DOCUMENTATION.\nResources Below is a list of resources used throughout this project, including both documentation and articles that, help with ideas/formatting of my TensorFlow code and made the project possible:\n Train a TensorFlow Model in Amazon SageMaker - Jun M. TFRecord and Image Data Example - TensorFlow Documentation How to train an Image Classifier on TFRecord files - Karan Sindwani Working with TFRecords and tf.train.Example - Cihan Soylu Image classification via fine-tuning with EfficientNet - Keras API  ","date":"2021-05-10T00:00:00Z","image":"/p/braintumor/images/banner_hu9eb6ca4c1784edf324c9c2921bdfab7c_1375483_120x120_fill_q75_box_smart1.jpg","permalink":"/p/braintumor/","title":"Brain Tumor Detection using SageMaker \u0026 TensorFlow"},{"content":" Original GitHub Repository\n Alright, I\u0026rsquo;ll admit it\u0026hellip; It sounded a lot cooler in my head. \u0026ldquo;Battle of the Learning Methods: Statistical vs Machine Learning\u0026rdquo;. Man vs. Machine (well not really, but you get the point). A test of speed, strength, learning, and maybe a little bit of overfitting. Maybe it\u0026rsquo;s just me that\u0026rsquo;s excited about the project, but I finally get to implement what I learn in school against what I do in my free time. This term I took both Sampling Methods and Applied Statistics II, and this brought me to think if I could create a project that implemented the material from those classes and my knowledge of Machine Learning? This project was the answer.\nStroke is the third leading cause of death in the United States, with over 140,000 people dying annually. Each year approximately 795,000 people suffer from a stroke with nearly 75% of these occurring in people over the age of 65. Using medical record information (age, gender, BMI, smoking, diseases, etc.) we will create models that can accurately predict whether or not a patient has had a stroke. The goal for this project will be to explore the data and find any correlations between features and the response variable stroke that will allow us to engineer new features for the data. After doing this, we will make a comparison between Statistical Modeling and Ensemble Modeling to see which we are able to achieve better results with. Note: models will be evaluated by an F-Beta and Recall score since avoiding a missed diagnosis is the main focus.\nData Exploration \u0026amp; Feature Engineering The data originated from the Kaggle repository for stroke prediction. There are 11 features that were recorded for 5110 observations. Personally I like to perform feature engineering while doing my EDA so that I can create these new features while the ideas are fresh in my mind. From importing the data, our initial data set has the following features:\n\r\nHowever, the id feature is just a unique identifier, so it can be dropped from the data set.\nBMI (Body Mass Index) From the initial exploration, I found that the only feature that had missing values was bmi (with 201 na\u0026rsquo;s). Before filling the missing values, we have the following distribution for bmi: \r\nTwo important notes here: the data seems to be somewhat normally distributed and we seem to have extreme values for bmi. To handle the missing values, I decided to fill them with the median value +/- random noise between [1,4]. Do I have a logical explanation for why I did this? Of course not, but filling with the median only increased a single value being present, where adding the random noise allowed for a more \u0026ldquo;even\u0026rdquo; distribution around the median. On top of this, I filter the data to only include values up to the 99th quantile (approximately bmi = 53) to filter out the outliers. Doing this we used the following code and resulted in the following graph:\n# Fill missing with values with median +/- random noises between [1,4] error_term = np.round(np.sqrt(np.random.randint(1, 16, size=data.bmi.isna().sum())),2) bmi_fill = data.bmi.median() + error_term data.loc[data.bmi.isnull(), \u0026#39;bmi\u0026#39;] = bmi_fill # Remove any values above the 99th quantile (approx BMI = 53) data = data[data.bmi \u0026lt; np.quantile(data.bmi, 0.99)] \r\nOkay so a bit more normal right? Maybe a little right skew in there too? I\u0026rsquo;m happy with where that\u0026rsquo;s at. Now that we\u0026rsquo;ve handle the bmi feature it\u0026rsquo;s time to create our first new feature: weight classes based on patient bmi.\nWeight Class (Feature Engineering) I decided to create weight classes based on the National Heart, Lung, and Blood Institute BMI Scale which gives us the following categories:\n Underweight = Less than 18.5 Normal weight = 18.5 - 24.9 Overweight = 25 - 29.9 Obesity = Greater than 30  def weight(row): # see function.py file for function definition if row[\u0026#39;bmi\u0026#39;] \u0026gt;= 30: val = \u0026#39;obese\u0026#39; elif ((row[\u0026#39;bmi\u0026#39;] \u0026gt;= 25) \u0026amp; (row[\u0026#39;bmi\u0026#39;] \u0026lt; 30)): val = \u0026#39;over weight\u0026#39; elif ((row[\u0026#39;bmi\u0026#39;] \u0026gt;= 18.5) \u0026amp; (row[\u0026#39;bmi\u0026#39;] \u0026lt; 25)): val = \u0026#39;normal weight\u0026#39; else: val = \u0026#39;under weight\u0026#39; return val \r\nSo it seems that the majority of the data falls into the obese category, followed by over weight, normal weight, and finally under weight. The majority of strokes seem to occur in for patients that fall under either obese or over weight, some for normal weight, and only a single patient that was under weight. If we look further into this (not shown in this article, refer to notebook) we get the following mean ages for each weight group: normal - 33.76, obese - 49.96, over weight - 49.39, and under weight - 10.91. It seems that older patients, who are also more susceptible to strokes, fall into the obese and over weight classes.\nAge \r\nAge does not seem to have a certain type of distribution for all observations (closest to a uniform distribution). When looking strictly at stroke observations, we can see the majority of the data is above 35 and has an extreme left skew. The distribution for no stroke also seems to not have a certain distribution (but could be classified at closely uniform).\nAge Class (Feature Engineering) Using the Age Categories from the Canadian Statistics website, I created features for the life cycle groupings defined on their website:\n Children (0-14) Youth (15-24) Adults (25-64) Seniors (65+)  I also created a generic age_class feature to be used for graphing. Think of the above individual features as a one-hot-encoding of the age_class feature.\ndata[\u0026#39;child\u0026#39;] = np.where(data.age \u0026lt; 15, 1, 0) data[\u0026#39;youth\u0026#39;] = np.where(((data.age \u0026gt;= 15) \u0026amp; (data.age \u0026lt; 25)), 1, 0) data[\u0026#39;adult\u0026#39;] = np.where(((data.age \u0026gt;= 25) \u0026amp; (data.age \u0026lt; 65)), 1, 0) data[\u0026#39;senior\u0026#39;] = np.where(data.age \u0026gt; 65, 1, 0) def age(row): # see function.py file for function definition if row[\u0026#39;child\u0026#39;] == 1: val = \u0026#39;child\u0026#39; elif row[\u0026#39;youth\u0026#39;] == 1: val = \u0026#39;youth\u0026#39; elif row[\u0026#39;adult\u0026#39;] == 1: val = \u0026#39;adult\u0026#39; else: val = \u0026#39;senior\u0026#39; return val data[\u0026#39;age_class\u0026#39;] = data.apply(age, axis=1) \r\nIt seems that the majority of the people observed in the data were young adults (25-64). However, people categorized as seniors had the highest amount of strokes. It also looks like both children and youth have very low stroke rates, and looking into this further we can see only two female children (age 1 and 14) had strokes. An important note is that the two children who had strokes were both categorized as obese with a BMI of approximately 30 for both.\nRemaining Features Exploration The remaining features didn\u0026rsquo;t seem to have too many correlated findings or values that allowed for any feature engineering, so they will be generalized into this section. This will just be a brief overview of a few features, so refer the stroke_prediction.ipynb section Age \u0026amp; Other Numerical Features for a more in-depth exploration.\n\r\nWe can see that avg_glucose_level doesn\u0026rsquo;t have a certain distribution (could be considered bimodel), and for both stroke/no stroke we have the same findings (with stroke having a slightly higher second peak).\n\r\nLooking at the above graph, we don\u0026rsquo;t seem to have any clear indicator of Hypertension being a key feature for stroke detection. However, if we look at the ratio of stroke to no stroke within the Hypertension classes, we can find that people with Hypertension are about 4x more likely to have a stroke.\n\r\nAgain looking at the above graph, we don\u0026rsquo;t seem to have any clear indicator of Heart Disease being a key feature for stroke detection. However, if we look at the ratio of stroke to no stroke within the Heart Disease classes, we can find that people with Heart Disease are about 5x more likely to have a stroke.\n\r\nWe seem to have around 800 more females than males in our dataset, but the amount of strokes for each gender seem to be about equivalent. Similarly, their ratio in respect to their group total are about equivalent. There is no clear evidence that one gender is more susceptible to stroke then the other.\nFinal Data Processing Now that we have finished exploring the data, we will want to create a final data set that can be used in the modeling section. We will drop all unused features and one-hot-encode the remaining categorical features.\n# Drop features that wont be used data.drop([\u0026#39;work_type\u0026#39;, \u0026#39;Residence_type\u0026#39;, \u0026#39;age_class\u0026#39;, \u0026#39;gender\u0026#39;], axis=1, inplace=True) # Encode and create any new necessary features data[\u0026#39;ever_married\u0026#39;] = data[\u0026#39;ever_married\u0026#39;].replace([\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;], [0,1]) data[\u0026#39;age_over_45\u0026#39;] = np.where((data.age \u0026gt;= 45), 1, 0) data[\u0026#39;over_weight\u0026#39;] = np.where((data.bmi \u0026gt;= 25), 1, 0) data[\u0026#39;smokes\u0026#39;] = np.where((data.smoking_status == \u0026#39;smokes\u0026#39;), 1, 0) data[\u0026#39;never_smoked\u0026#39;] = np.where(((data.smoking_status == \u0026#39;never smoked\u0026#39;) | (data.smoking_status == \u0026#39;Unkown\u0026#39;)), 1, 0) data.drop([\u0026#39;smoking_status\u0026#39;, \u0026#39;child\u0026#39;, \u0026#39;youth\u0026#39;, \u0026#39;adult\u0026#39;, \u0026#39;senior\u0026#39;, \u0026#39;weight_class\u0026#39;], axis=1, inplace=True) \r\nLooking at the above matrix it seems that stroke has the highest correlation with the following features: age (0.25), age_over_45 (0.21), heart_disease (0.14), hypertension (0.13), and avg_glucose_level (0.13). There are other features that are correlated with stroke that we engineered, but are lower than the ones listed previously. Now that our data is processed and a subset of the features are kept, we can begin the modeling section.\nModeling Brief Overview of Data Formatting For the Statistical Modeling section, the data was reformatted in two ways to accommodate the large class imbalance (around 20x more observations of \u0026ldquo;No Stroke\u0026rdquo; compared to \u0026ldquo;Stroke\u0026rdquo;):\n  Training data was balanced by using SMOTE (Synthetic Minority Oversampling Technique) to increased of minority \u0026ldquo;stroke\u0026rdquo; class to a 3:4 ratio with the majority class \u0026ldquo;no stroke\u0026rdquo;. This resulted in around 3400 majority observations (0) and 2500 minority observations (1).\n  Testing data was balanced using the NearMiss algorithm, which undersampled the majority class to a 4:3 ratio with the minority class. This resulted in around 120 majority and 90 minority observations to be used for evaluation. Note: when evaluating based on oversampled data, I did not feel the results were as accurate since repeated observations were increasing the scores. I want the model to be prepared for real world data rather than higher metrics on repeated data.\n  For the Ensemble Modeling section, the data was reformatted in the following ways to accommodate the class imbalance:\n  Training data was left untouched since the ensemble algorithms we used are able to handle the imbalance within the model itself.\n  Testing data was resampled so that we would have a \u0026ldquo;Stroke\u0026rdquo; to \u0026ldquo;No Stroke\u0026rdquo; ratio of 2:3, resulting in around 50 minority and 75 majority observations (slightly smaller than the statistical modeling data).\n  An important note is that the extra observations from the majority class (after being undersampled) in the testing data were added back into the training data so that we had more data to train on. This was due to the algorithms being able to handle class imbalance (so more majority observations would not have a negative effect).\n  Statistical Modeling Using the StatsModels module, I created a Generalized Linear Model based on the Logistic (Binomial) family. Before the actual model can be built, we first need to set up the data splits and sampling methods described above. The following code took care of this:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) over_sampler = over_sampling.SMOTE(sampling_strategy=0.75, random_state=42) under_sampler = under_sampling.NearMiss(sampling_strategy=0.75, version=2) X_train, y_train = over_sampler.fit_resample(X_train, y_train) X_test, y_test = under_sampler.fit_resample(X_test, y_test) # Create data frame for GLM model (X_train \u0026amp; y_train in same data frame) data_glm = X_train.copy() data_glm[\u0026#39;stroke\u0026#39;] = y_train \r\nAs we can see, by oversampling we were able to bring up the minority class observations to approximately 2500, which gave us our 4:3 desired ratio (again, no real reason why I chose this but let\u0026rsquo;s just go with it). Testing data has also been reduced down to the desired 3:4 ratio. Now that we have our data formatted correctly, we can begin to build our GLM:\nresponse_var = \u0026#39;stroke ~ \u0026#39; # Fit with all variables explanatory_vars = \u0026#39; + \u0026#39;.join(X_train.columns.values) formula = response_var + explanatory_vars model = sm.GLM.from_formula(formula, family=sm.families.Binomial(), data=data_glm) result = model.fit() result.summary() \r\nFrom our initial model, we can see that almost all features are statistically significant (except for over_weight which was a feature I had previously created from bmi). We can now use the above model to predict on the test data, and we get the following initial (base model) results:\n\r\nOkay, so not bad. But I\u0026rsquo;m pretty confident that we can improve these results. How you may ask? Unlike machine learning models, we can use Grid or Random Search with a bunch of hyperparameters. Statistical models require a little more finesse. And by finesse I mean stepwise selection to find the best combination of these features. I\u0026rsquo;ll evaluate these models based on their BIC, Recall, and F-Beta score (beta=0.85 provided the best results).\n NOTE: All 3 stepwise methods resulted in the exact same model, with the same coefficient \u0026amp; p-values. So forward selection will explain in a bit more detail, and then exhaustive/backwards will just be a quick overview of the code and idea behind them.\n Foward Stepwise Feature Selection def fitModel(feature_subset): response_var = \u0026#39;stroke ~ \u0026#39; explanatory_vars = \u0026#39; + \u0026#39;.join(X_train[list(feature_subset)].columns.values) formula = response_var + explanatory_vars model = sm.GLM.from_formula(formula, family=sm.families.Binomial(), data=data_glm) result = model.fit() y_preds = round(result.predict(X_test[list(feature_subset)])) model_recall = recall_score(y_test, y_preds) model_fbeta = fbeta_score(y_test, y_preds, beta=0.85) return {\u0026#39;model\u0026#39;: result, \u0026#39;bic\u0026#39;: result.bic, \u0026#39;recall\u0026#39;: model_recall, \u0026#39;fbeta\u0026#39;: model_fbeta, \u0026#39;features\u0026#39; : X_train[list(feature_subset)].columns.values} def forwardSelection(predictors): start = time.time() remaining_predictors = [p for p in X.columns if p not in predictors] results = [] for p in remaining_predictors: results.append(fitModel(predictors+[p])) models = pd.DataFrame(results) best_model = models.sort_values(by=\u0026#39;fbeta\u0026#39;, ascending=False).iloc[0,:] return best_model.values Okay so a good amount of code just thrown at you there. The general idea for forward selection is that we iterate over all of the features in the training data set with each iteration adding the coefficient that improves our model the best. For each iteration, we try all features but only return a single model that had the highest F-Beta score, and append this to our list of final models. In total, we will have 10 models (given that we have 10 features) which we then plot and choose a final model from.\n\r\nFrom the above graphs, we can see that the model using 4 features resulted in the highest Recall \u0026amp; F-Beta score (BIC is in the middle, but this model gave the best results). To save space I\u0026rsquo;ll put the results from the model summary and confusion matrix for the test set in the final model section since all 3 stepwise methods resulted in the same model/scores.\nExhaustive Stepwise Feature Selection def exhaustiveSearch(k): start = time.time() results = [] for combo in itertools.combinations(X_train, k): results.append(fitModel(combo)) models = pd.DataFrame(results) best_model = models.sort_values(by=\u0026#39;fbeta\u0026#39;, ascending=False).iloc[0,:] return best_model.values The general idea for exhaustive selection is that we try every combinations of predictors given a certain length (in this case, 1 through 10) and return the best model. For example, in step 2 we try every combination of 2 features and so on. Note: forward search took around 1 second for all 10 steps, where exhaustive search took around 25 seconds - for a much larger model this would need to be considered before using.\nBackwards Stepwise Feature Selection def backward(predictors): start = time.time() results = [] for combo in itertools.combinations(predictors, len(predictors)-1): results.append(fitModel(combo)) models = pd.DataFrame(results) best_model = models.sort_values(by=\u0026#39;fbeta\u0026#39;, ascending=False).iloc[0,:] return best_model.values The final method is backwards selection, which will begin with all of our features in the data set, and each iteration it will drop the least significant coefficient. So we start with 10 features, and work our way down to 1 at the end.\nFinal Statistical Model models_bwd_flipped.loc[4][\u0026#39;Model\u0026#39;].summary() # final model \r\nOur final model! Although I\u0026rsquo;m showing the model from the Backward selection, both the forward/exhaustive models also had the same 4 features as their top performer (so just think of the above as the general winner for each method). All coefficients are now statistically significant, and to my surprise some of what I thought were key features have been dropped. Hypertension, heart disease, smoking, and average glucose level are key indicators for stroke but they were all dropped? Blows my mind, but our model seemed to perform much better without them and the results below show that:\n\r\nOkay, so what improvements do we have over the base model:\n True Negatives increased by 2, while False Positives decreased by 2 (less people classified as stroke that did not have a stroke). ⭐ False Negative decreased by 21, while True Positives increased by 21 (more people classified as stroke that actually had a stroke). ⭐ Precision increased from 57% to 66%. Recall increased from 58% to 82% (this was the most important evaluation metric to improve). Accuracy increased from 63% to 74%.  Note: the above came from the classification report, see either the README.md or stroke_prediction.ipynb.\nA huge improvement! The changes marked by the ⭐ were our main metrics to improve and we successfully did. There\u0026rsquo;s still room to improve (False Positives could be lowered a bit but this was a trade off to increase Recall) but this is a strong model. However, will it be strong enough to stay champion? Let\u0026rsquo;s move onto the ensemble modeling and see.\nEnsemble Modeling For this section I initially planned on using the Scikit-Learn library, but after some research I found another one that seemed to work a bit better for my problem: ImBalanced-Learn. This library is build on Scikit-Learn put is designed mainly for classification with imbalanced classes. 3 initial models will be fit, cross-validated on all of the data and had the following scores:\n\r\n BalancedRandomForestClassifier - A balanced random forest classifier that under-samples each bootstrap sample to balance our classes. RUSBoostClassifier - An AdaBoost algorithm that performs random under-sampling at each iteration to balance classes. BalancedBaggingClassifier - A bagging classifier that implements data balancing at fit time.  This may be a surprise, but the model chosen to continue with for hyperparameter tuning is the BalancedRandomForestClassifier. Although it had the lowest accuracy, precision, and F-Beta score it also had the highest recall (which is what we want). I believe with changes to our data set and model parameters, we will be able to improve these results.\nData Set Up Now that we have our initial model set up, we will want to create the training and testing splits. Unlike the previous section, we will want to keep the training set imbalanced but balance the testing set to have the same ratio used to evaluate the GLM, which is 4:3.\n NOTE: I now wonder if these results differed since we used different testing sets. The Ensemble Model testing set was smaller and had different observations, so this could be affecting the results.\n def sampled_data_split(data): stroke_obs = data[data.stroke == 1] no_stroke_obs = data[data.stroke == 0] sample_size = np.ceil(0.2 * len(stroke_obs)) stroke_sample = stroke_obs.sample(n=int(sample_size), random_state=42, axis=0) stroke_extra = pd.concat([stroke_obs, stroke_sample]).loc[stroke_obs.index.symmetric_difference(stroke_sample.index)] sample_size = np.ceil(len(stroke_sample)*1.5) no_stroke_sample = no_stroke_obs.sample(n=int(sample_size), random_state=42, axis=0) no_stroke_extra = pd.concat([no_stroke_obs, no_stroke_sample]).loc[no_stroke_obs.index.symmetric_difference(no_stroke_sample.index)] train_set = shuffle(pd.concat([stroke_extra, no_stroke_extra], axis=0)) test_set = shuffle(pd.concat([stroke_sample, no_stroke_sample], axis=0)) X_train, y_train = train_set.drop(\u0026#39;stroke\u0026#39;, axis=1), train_set.stroke X_test, y_test = test_set.drop(\u0026#39;stroke\u0026#39;, axis=1), test_set.stroke return X_train, X_test, y_train, y_test \r\nBase Model model = BalancedRandomForestClassifier(random_state=42, class_weight=\u0026#39;balanced\u0026#39;) model = model.fit(X_train, y_train) y_preds = model.predict(X_test) \r\n Accuracy: 81% Precision: 71% Recall: 88%  Okay, so already our model seems to be performing better than the GLM. This isn\u0026rsquo;t a huge surprise, I expected this to happen for the most part. But now that we have our base model, let\u0026rsquo;s see if we can increase these scores.\nHyperparameter Tuning SPOILER: The base model performed better than any of the tuned models below. Just in case you wanted to skip over the section (I don\u0026rsquo;t blame you, I probably wouldn\u0026rsquo;t make it through half this article with my attention span) the results seemed to slightly decrease with the tuned models. Two different methods were used in an attempt to improve the model: GridSearchCV and Exhaustive Feature Selection.\nGridSearchCV parameters = {\u0026#39;n_estimators\u0026#39;: [25, 50, 75, 100, 125], \u0026#39;criterion\u0026#39;: [\u0026#39;gini\u0026#39;, \u0026#39;entropy\u0026#39;], \u0026#39;max_features\u0026#39;: [\u0026#39;auto\u0026#39;, \u0026#39;log2\u0026#39;, None], \u0026#39;sampling_strategy\u0026#39;: [\u0026#39;auto\u0026#39;, \u0026#39;all\u0026#39;, 0.7], \u0026#39;class_weight\u0026#39;: [\u0026#39;balanced\u0026#39;, \u0026#39;balanced_subsample\u0026#39;]} scoring_params = {\u0026#39;precision\u0026#39;: make_scorer(precision_score), \u0026#39;recall\u0026#39;: make_scorer(recall_score), \u0026#39;fbeta\u0026#39;: make_scorer(fbeta_score, 1.25)} hpt_model = BalancedRandomForestClassifier(random_state=42) rs_cv = GridSearchCV(hpt_model, parameters, verbose=1, scoring=scoring_params, refit=\u0026#39;fbeta\u0026#39;, n_jobs=-1) start = time.time() search = rs_cv.fit(X_train, y_train) search.best_params_ # {\u0026#39;class_weight\u0026#39;: \u0026#39;balanced\u0026#39;,  # \u0026#39;criterion\u0026#39;: \u0026#39;gini\u0026#39;,  # \u0026#39;max_features\u0026#39;: \u0026#39;auto\u0026#39;,  # \u0026#39;n_estimators\u0026#39;: 25,  # \u0026#39;sampling_strategy\u0026#39;: \u0026#39;auto\u0026#39;} Using the above parameters, a new model was fit and used to predict on the data. To save room I won\u0026rsquo;t include the confusion matrix or classification report, but the main take away is that True Negatives decreased by 1 and False Positives increased by 1 (slightly worse than base model). However, the model increased it\u0026rsquo;s prediction speed by 2x which could be considered if that was the goal for the product (slightly worse results for much faster performance).\nExhaustive Feature Selection NOTE: Here we used the model from above to try and improve the results since it had similar results as the base model but faster performance).\nefs = EFS(search.best_estimator_, min_features=2, max_features=7, scoring=make_scorer(fbeta_score, beta=1.25), cv=5, n_jobs=-1) efs = efs.fit(X_train, y_train) list(X_train.iloc[:, list(efs.best_idx_)].columns) # [\u0026#39;age\u0026#39;, \u0026#39;hypertension\u0026#39;, \u0026#39;avg_glucose_level\u0026#39;, \u0026#39;bmi\u0026#39;, \u0026#39;smokes\u0026#39;, \u0026#39;never_smoked\u0026#39;] Again, a new model was fit using only the above features but this resulted in even worse performance. Our True Negatives decreased by 1 and False Positives increased by 1, as well as False Negatives increasing by 2 and True Positives decreasing by 2. From this, our base model seems to have the best performance and will be our final model.\nFinal Model Okay, so this was one of those cases where hyperparameter tuning didn\u0026rsquo;t seem to benefit our model very much (actually it seemed to hurt our model more than anything). Our base model seemed to perform the best out of all 3 models, and the features had the following importance:\n\r\nAn important note is that the age, bmi, and avg_glucose_levels were not normalized and all other features had discrete values in the range [0,1], but normalizing the inputs resulted in the same feature importance but with worse performance.\nConclusion Comparing the statistical and ensemble models, we can see that the ensemble model seems to be performing better (although not be a large margin). We have the following difference based on the final models for both types:\n Accuracy: Ensemble model has a 7% advantage. Precision: Ensemble model has a 5% advantage. Recall: Ensemble model has a 6% advantage.  An important note is that the two model types were trained and tested on different data. The statistical model had oversampling of the minority class for the training data in order to balance it since we could not handle the imbalance within the model itself (but the testing data was undersampled). The ensemble model had regular training data (not balanced) but the testing data was undersampled and slightly smaller (extra observations left over from the undersampling were also added back into training since imbalanced data was not a concern).\nOverall, the ensemble model seems to be slightly stronger in all evaluation aspects. The computational speeds were similar for both models. Places to improve upon for our models would to be to try and get the data splits to be more similar in order to have more validity to our statement of the stronger model.\n","date":"2021-03-21T00:00:00Z","image":"/p/strokeprediction/img/header_hucdbe1c766fbb076b014b35afd7dcfcdc_900878_120x120_fill_q75_box_smart1.jpg","permalink":"/p/strokeprediction/","title":"Stroke Prediction: Battle of the Learning Methods"},{"content":" Original GitHub Repository\nNote: An in-depth data exploration section was also created, but will not be explored in this post. I will mainly focus on the modeling section, please refer to the GitHub link above for the DataExploration.ipynb notebook.\n The More The Merrier\u0026hellip; Right? The current world population is nearly 7.8 billion people, and this number is estimated to rise to around 9.7 billion (or higher) in the year 2050. This means within the next 30 years, we will need to feed two billion more people without sacrificing the planet. This means we will need to increase our crop production in order to feed that growing population. Agriculture is one of the greatest contributors of global warming, with farming consuming immense amounts of our water supplies and leaving major pollutants as its byproduct from fertilizer runoff. This leads to the question, how do we supply the necessary amount of food for a growing world population without sacrificing the climate of our planet? This was the goal for my project, to use data from Production \u0026amp; Population from 1963 to 2013 to train a model that could then be used to predict the global food production necessary from 2014 to 2050, with an emphasis on the year 2050.\nThe Data Two different data sets were used for the training data: one coming from Kaggle that contains information on yearly production for 174 countries and the other coming from the Food and Agriculture Organization (FAO) website that contains the annual population for each country.\n Production: This data set contains the yearly food production of 115 food items for 174 countries, spanning from 1961 to 2013. It also is broken down into food and feed categories, which represent human and livestock consumables (respectively). The data will be summed across each year (column) in order to get a rough estimate of the global production for every year. This part of the data will be our dependent variable that will be predict from our population data. Population: This is an Annual Population Report obtained through FAO that contains the estimated population for 245 countries from 1961 to 2018. An important note is that this data set contains 71 more countries than the Production data set, but will be kept in for total population purposes. All years after 2013 will be dropped as well since the Production data stops there, and no further production data can be estimated and evaluated.  One more data set will be created for testing purposes, the estimated population from 2014 to 2050. Even though the current year is 2021, the Production data does not have any values after 2013 and so all further years will need to be predicted. This data comes from the Worldometer website which tracks live population counts, past population data, and future predictions. Data from this website is pulled from 2014-2020 (actual) and 2021-2050 (estimated) and put into a NumPy array.\nNote: Population is measured in \u0026ldquo;1000 persons\u0026rdquo; and Production is measured in \u0026ldquo;1000 tonnes\u0026rdquo;.\npop_est = [7210845.848 , 7295290.765, 7379797.139, 7464022.049, 7547858.925, 7631091.040, 7713468.100, 7794798.739, 7874965.825, 7953952.567, 8031800.429, 8108605.388, 8184437.460, 8259276.737, 8333078.316, 8405863.295, 8477660.693, 8548487.400, 8618349.489, 8687227.850, 8755083.431, 8821862.661, 8887524.213, 8952048.940, 9015437.653, 9077693.676, 9138828.468, 9198847.240, 9257745.535, 9315508.050, 9372118.186, 9427555.367, 9481803.274, 9534854.828, 9586707.986, 9637357.637, 9686800.357, 9735033.990] Predicting Production: They Grow Up So Fast \r\nAlthough the predictions for every year from 2014 to 2049 are just as important, the main focus is the 2050 prediction (marked as the purple point above). As we can see, from an initial plot it seems that the data is mostly linear. It does seem like the population growth may be slowing down as 2050 approaches, but again this is just estimated and not the true value. This brings me to ask; does this mean the population will peak in 2050 and begin to plateau? Or will it continue to grow and pass 10 billion?\nThe Simple Linear Model Okay, so population seems to be mostly linear. But does this idea stay true when we plot production against population?\n\r\nEh, not so much here. Yes it is somewhat linear, but I didn\u0026rsquo;t expect a completely straight line to be the best model (and it definitely wasn\u0026rsquo;t). So how bad was it really? Well I decided to evaluate the model\u0026rsquo;s based on RMSE, and the RMSE for the initial model was 405,017.06 and yes that seems large, but the numbers we are working with here are also extremely large (so maybe it isn\u0026rsquo;t so bad). The main purpose of this model was not to have a high accuracy right off the start, but rather to have a base value for the RMSE and ensure that our other models are improvements over this one. So how do we improve this model?\nThe Polynomial Model Ah yes, linear regression but with curved lines. The whole idea behind these models is that we can manipulate the x-values to better fit the data and estimate our target y-values. However, we also have to consider the curve after the training data finishes (2013). If we square the x value, it may fit better but growth past our last year may grow extremely fast and not properly model the data. If we cube the x value, our data may grow even faster and fit worse as the years increase. To help determine this, I will evaluate multiple models on their RMSE values to help chose a subset of values to modify the population data.\nDetermining the Degree rmse_scores = [] degree_range = np.arange(1,9) for deg in degree_range: poly_features = PolynomialFeatures(degree=deg) X_poly = poly_features.fit_transform(x_train) poly_model = LinearRegression().fit(X_poly, y) train_preds = poly_model.predict(X_poly) rmse = round(np.sqrt(mean_squared_error(y, train_preds)), 2) rmse_scores.append(rmse) \r\nWe can see a degree of 1 represents our base model, but degrees 2 to 5 seems to minimize the RMSE the best. We will want to train a polynomial model using those degrees, compare their RMSE as well as their prediction patterns for the future, and determine which model best suits our data. This may mean having a higher RMSE, but we are more focused on the predicted production rather than a minimal error in this case.\nThe Models for degree in deg_range: # [2-5] poly_feat = PolynomialFeatures(degree=degree) X_poly = poly_feat.fit_transform(x_train) # Manipulate x by given degree poly_model = LinearRegression().fit(X_poly, y) poly_2014_2050 = poly_feat.fit_transform(pop_) yhat = poly_model.predict(X_poly) # Predict 1963 to 2013 future_yhat = poly_model.predict(poly_2014_2050) # Predict 2014 to 2050 # Append values to be plotted rmse_scores.append(round(np.sqrt(mean_squared_error(y, yhat)), 2)) preds_2050.append(round(future_yhat[-1],3)) future_preds.append(future_yhat) poly_train_preds.append(yhat) \r\n   Degree RMSE 2050 Production Estimation     2 179427.41 22463820.97   3 182169.96 23084034.55   4 171969.60 17341906.93   5 169272.95 13500278.99    Just from a quick glance, we can see that we cut the original RMSE values by over 50%. It also seems that each model has a very close RMSE to one another (max difference of around 13,000). However, the 2050 estimate is where the models become extremely different with a maximum of 22.4 billion tonnes and a minimum of 13.5 billion tonnes (the above prediction are measured in 1000 tonnes and I multiplied them out to get these numbers). So just a slight difference, but how do we know which value is correct? The simple answer is that we don\u0026rsquo;t, but we can look to professional who study these concepts for some guidance on how much production we need to support the 2050 population.\nSo what exactly do we want to predict? This is where the goal of our model gets a little more complicated. What exactly is the necessary production needed to support the 2050 population? The answer to this depends on who you ask.\n  From the World Resources Institute, they predict that we needs an increase of around 56% in production from 2010 to 2050 in order to sustainably feed 10 billion people. The 2010 global production, from our data set, was 11,445,072 (in 1000 tonnes). Following this idea, that means we need to produce 17,854,312.32 (in 1000 tonnes) total for the estimated 2050 population. This seems to follow the degree 4 polynomial model, which estimated a production of 17,341,906.93 (in 1000 tonnes).\n  From the United Nations, they predict that global food production must double in order to support the global population. Given that the article was written in 2009, with a global production of 11,211,891 (in 1000 tonnes) for out data set, this means we need to produce 22,423,782 (in 1000 tonnes) for the year 2050. This idea follows the degree 2 model (almost exactly) with an estimated production of 22,463,820.97 (in 1000 tonnes).\n  A difference in 5 billion tonnes of production, the true total global production needed for 2050 is difficult to estimate. Despite not being the lowest RMSE for the models, these 2 most accurately depict what has been estimated by professionals. Choosing which model is correct will be more of a challenge, since there is no real way to determine what the actual growth will be in the future (global events, pandemics, food supply, etc.). However, these two models are good indicators of a possible future that we could see.\nConclusion In an attempt to keep this post short, I did skip over the majority of the code as well as another section on clustering countries based on their production. If you really want to know, the main producers are China, India, and the United States of America (and it\u0026rsquo;s not even a close tie between the remaining countries). But the main purpose of this post was to explore the production needed for the 2050 population. Although it\u0026rsquo;s difficult for me to make a strong conclusion about which model I think best fits the data, it\u0026rsquo;s always better to overprepare rather than being underprepared. The second degree model, although growth will only continue to increase after 2050 even faster given the graphs curve, is the one I would feel most confident backing up. This project taught me a lot, not only about where my food was coming from but also about where it is going to need to come from and how we all need to do our part to help sustainably increase production. I can\u0026rsquo;t emphasis enough checking out the DataExploration.ipynb on the original Github link, there are a ton of graphs on production and items that I had no idea about and really enjoyed exploring.\n","date":"2021-01-26T00:00:00Z","image":"/p/prodvspop/img/banner_hu2fba2ac787967c0d994b01819822a76b_1268585_120x120_fill_box_smart1_2.png","permalink":"/p/prodvspop/","title":"Global Food Production vs. Population: Predicting the Future"}]